[
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html",
    "href": "webppl vs memo/xiang2023-exp1-round3.html",
    "title": "",
    "section": "",
    "text": "code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R\nlibrary(tidyverse)\nlibrary(rwebppl)\n\njoint &lt;- data.frame(model = 'joint',\n                    agent = rep(c('A','B'), 18), \n                    scenario = c(rep('F,F;F,F',6),rep('F,F;F,L',6),rep('F,L;F,L',6),rep('F,F;L,L',6),rep('F,L;L,L',6),rep('L,L;L,L',6)),\n                    effort = 0, strength = 0,\n                    outcome = 0, prob = 0, round = rep(c(1,1,2,2,3,3), 6), \n                    reward = rep(c(10,10,20,20,20,20), 6))\njoint$prob[joint$round==1] = NaN\njoint$outcome = c(integer(6), integer(3),1,integer(2), 0,1,0,1,1,1, integer(2),integer(4)+1, 0,integer(5)+1, integer(6)+1)"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "title": "",
    "section": "1 - mcmc",
    "text": "1 - mcmc\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\" # footnote: For computational tractability, we constrained the effort space to discrete values, ranging from 0 to 1 with increments of 0.05.\n\n## R3 Probability\nmdl1 &lt;- \"\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'MCMC', kernel: 'MH', samples: 10000, burn: 1000, model() {\n  var sa = uniform(1,10)\n  var sb = uniform(1,10)\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      // display(tbl.jointU)\n      return tbl.jointU\n    },\n    efforts);\n};\n\nvar startingE = startingEffort(x2a,x2b)\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\n# mdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\n# a &lt;- rwebppl::webppl(mdl)\n# joint$prob[joint$round==3 & joint$scenario=='F,F;F,F'] &lt;- a$P*100\n# joint\n\n\noriginal_ffff &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5) |&gt; rwebppl::webppl()\noriginal_fffl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_flfl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"true\", mdl3, \"false\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_ffll &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_flll &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"true\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_llll &lt;- paste0(effort_space_joint, mdl1, \"true\", mdl2, \"true\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\n\n\n# plot\nbind_rows(\n  tibble(scenario='F,F;F,F', P=original_ffff$P),\n  tibble(scenario='F,F;F,L', P=original_fffl$P),\n  tibble(scenario='F,L;F,L', P=original_flfl$P),\n  tibble(scenario='F,F;L,L', P=original_ffll$P),\n  tibble(scenario='F,L;L,L', P=original_flll$P),\n  tibble(scenario='L,L;L,L', P=original_llll$P)\n) |&gt;\n  ggplot(aes(x=scenario, y=P*100)) +\n  geom_col() +\n  labs(title='Round 3 Success Probability by Scenario (MCMC, ORIGINAL CODE)', y='Probability (%)', x='Scenario') +\n  theme_minimal()"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "title": "",
    "section": "2 - enumeration",
    "text": "2 - enumeration\nnow trying it using enumeration (using a discretized strength space), as a step towards translating into memo:\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\"\n\n## R3 Probability\nmdl1 &lt;- \"\n// convert to a @jax.jit?\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\n// convert to a @jax.jit\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\n// convert to a @jax.jit\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\n// convert to a @jax.jit\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\n// convert to a @memo, and instead of pushing x2a and x2b to a global variable, bring in the computations which use them into the @memo ?\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'enumerate', model() {\n  var sa = (11+randomInteger(89))/10 // not in a general form, but computes values in (1, 10) instead of in [1, 10] to see if that fixes things\n  var sb = (11+randomInteger(89))/10\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\n// convert to a combination of @jax.jit and @memo ? so I'm not doing two-step nested optimization (like the difference between RSA in webppl and in memo)\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// convert to a @jax.jit\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      return tbl.jointU\n    },\n    efforts);\n};\n\n\nvar startingE = startingEffort(x2a,x2b)\n// display(jointUtility(startingE,x2a,x2b))\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\nmdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\nwebppl(mdl)\n\n\n\nnote that their model is assuming that 1 and 10 are lower probability, which is just a computational limitation! when enumerating, the resulting p is infinitesimally small but matches all others; when using MCMC, the bounds are underweighted.\n\n-&gt; using something like (11+randomInteger(89))/10 to simulate sampling from (1,10) rather than [1,10]; inspired by the observation that webppl’s uniform distribution is actually more like this, since there are only finite samples\nmaybe just need to exclude 1 from the range and not 10? -&gt; trying different versions of the discretized distribution"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "title": "",
    "section": "3 - comparison across scenarios",
    "text": "3 - comparison across scenarios\nNow moved into xiang2023-exp1-round3-with-debugging.wppl, which has input parameters and logs.\nWalking step by step and validate that the values (posteriors and equilibria across all 6 scenarios) match:\n\n\npaper results\n\n\nwebppl mcmc (below)\n\n\nwebppl mcmc discrete (below)\n\n\nwebppl enumerate (below)\n\n\ndetermine if need to use (1,10) (or another variant) instead of [1,10] for strength prior. I think we should be able to use [1,10] unless we see mismatch with original method\n\n[c] not sure why P jumps to 1 so fast after F,F;F,F!\n\nfigure out how the mcmc mode of my current debugging webppl version diverged from the original mcmc version (reproduced at top)\n\nIf want more reliable MCMC results: for MCMC versions, use 10 iterations to recreate paper’s method of avging across 10 model runs\n**seems like we’ll need to avoid one of the extremes to prevent startingE from being chosen as 1!\n\nOR we can see if increasing precision fixes things - Claude thinks that small error in strength estimates (due to discretization) could be amplified by the equilibrium-finding process, so we need greater precision.\n\nNOTE: if we use memo frames instead of game-theoretic nested optimization, then maybe this isn’t a problem anymore\n-&gt; deciding that [1,10] is sufficient since it’s not actually the source of discrepancy; the source is discretization itself… see explanation from Claude below. choosing to skip now to memo, where I can have very high precision more easily, to see if that matches the original (continuous) MCMC model’s startingE_table and final_table\n\n\n\n\n\n\n\nPosterior Mean Mismatch\n\nContinuous (MCMC): Posterior mean might be 2.347\nDiscrete (0.1 steps): Posterior can only represent 2.3 or 2.4, so mean ≈ 2.35\nSmall error in mean strength propagates forward\n\nCritical Thresholds\n\nJoint lifting condition: effort_a × strength_a + effort_b × strength_b ≥ 5\nNear threshold boundaries (e.g., strength 2.5-7), small discretization errors flip lift outcomes from success→fail or vice versa\nThis creates discontinuous jumps in expected utility\n\nCascade Through Equilibrium Finding\n\njointUtility averages lift2() over all posterior samples: listMean(map2(lift2, strength_a, strength_b))\nDiscrete approximation of this average differs from continuous\nThe nested best-response iteration (lines 163-210) magnifies this error\nDifferent expected lift probabilities → different optimal efforts → different equilibria\n\nAmplification\n\nSmall error in posterior mean (0.05 units)\n→ Different lift probabilities (5-10% difference)\n→ Different equilibrium efforts (0.1-0.2 difference)\n→ Large divergence in final joint utility and outcome probability\n\n\nWhy it matters most in mid-range (2.5-7): This is where the lift threshold creates maximum sensitivity—agents transition from “can only jointly lift” to “can lift solo,” making the utility function most nonlinear.\n\n\n\nTODO finish memo enumerate, with same parameters as webppl enumerate (xiang2023-exp1-round3-memo.qmd), but higher precision; and log all the stats (see stats) – and investigate discrepancies. e.g. why is Joint utility at optimum so high? why are the agents’ utilities different from each other?\n\n\nTODO modify memo joint effort model to implement solitary effort and compensatory effort models, and check that the P results match up with the P results from the paper (simple, no need to validate incrementally)\n\nmoonshot: memo enumerate, but the equilibrium-finding process uses memo frames in order to model agent’s uncertainty about each other (still accelerated by JAX, but now in the language of memo, and considering the uncertainty)\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nFAIL &lt;- FALSE\nLIFT &lt;- TRUE\n\n# Define the 6 scenarios\nscenarios &lt;- tribble(\n  ~name,      ~a_r1result, ~b_r1result, ~a_r2result, ~b_r2result,\n  \"ffff\",     FAIL,        FAIL,        FAIL,        FAIL,\n  \"fffl\",     FAIL,        FAIL,        FAIL,        LIFT,\n  \"flfl\",     FAIL,        LIFT,        FAIL,        LIFT,\n  \"ffll\",     FAIL,        FAIL,        LIFT,        LIFT,\n  \"flll\",     FAIL,        LIFT,        LIFT,        LIFT,\n  \"llll\",     LIFT,        LIFT,        LIFT,        LIFT\n)\n\n# Helper function to run a single scenario\nrun_scenario &lt;- function(scenario_name, method, discrete = FALSE,\n                        strengthPriorPrecision = NULL,\n                        strengthPriorVersion = NULL) {\n  scenario &lt;- scenarios %&gt;% filter(name == scenario_name)\n\n  params &lt;- data.frame(\n    method = method,\n    discrete = discrete,\n    a_r1result = scenario$a_r1result,\n    b_r1result = scenario$b_r1result,\n    a_r2result = scenario$a_r2result,\n    b_r2result = scenario$b_r2result\n  )\n\n  if (!is.null(strengthPriorPrecision)) {\n    params$strengthPriorPrecision &lt;- strengthPriorPrecision\n  }\n  if (!is.null(strengthPriorVersion)) {\n    params$strengthPriorVersion &lt;- strengthPriorVersion\n  }\n\n  suppressWarnings(rwebppl::kill_webppl())\n  if (interactive()) {\n    message(sprintf(\"Running %s for scenario %s\", method, scenario_name))\n  }\n  rwebppl::webppl(\n    program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\",\n    data = params,\n    data_var = \"params_df\",\n    random_seed = 1\n  )\n}\n\n# Helper function to run all scenarios for a given method configuration\nrun_all_scenarios &lt;- function(...) {\n  results &lt;- list()\n  for (scenario_name in scenarios$name) {\n    results[[scenario_name]] &lt;- run_scenario(scenario_name, ...)\n  }\n  results_tidy &lt;- results |&gt;\n    enframe() |&gt;\n    unnest_wider(value) |&gt;\n    unnest_wider(final_table) |&gt;\n    rename(scenario = name)\n  results_tidy\n}\n\ncompare_results &lt;- function(...) {\n  # Accepts data frames as inputs\n  results_list &lt;- list(...)\n  if (length(results_list) == 0) {\n    stop(\"At least one set of results must be provided\")\n  }\n\n  # Build summaries\n  results_combined &lt;- bind_rows(results_list, .id=\"method\")\n  P_results_summary &lt;- results_combined |&gt; select(method, scenario, P)\n  posteriors_summary &lt;- results_combined |&gt; select(method, scenario, aStrength_posterior, bStrength_posterior)\n\n  message(\"Now compare visually\")\n\n  show(P_results_summary)\n\n  # Make posteriors_summary longer for plotting (cols: method, agent, scenario, strength)\n  posteriors_long &lt;- posteriors_summary |&gt;\n    pivot_longer(cols = c(aStrength_posterior, bStrength_posterior),\n                  names_to = \"agent\",\n                  values_to = \"strength\") |&gt;\n    mutate(agent = ifelse(agent == \"aStrength_posterior\", \"a\", \"b\")) |&gt;\n    unnest_longer(strength)\n\n  # Plot density of posteriors for each scenario\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_wrap(~scenario, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  message(\"Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\")\n\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_grid(method~scenario, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_grid(method~scenario+agent, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  message(\"Compare all output values across methods\")\n\n  show(results_combined |&gt; select(-aStrength_posterior, -bStrength_posterior, -startingE_table))\n\n  message(\"Extract all final_table values into a comprehensive summary\")\n\n  # Plot comparisons for the rest of the variables\n  results_combined_vars_long &lt;- results_combined |&gt;\n    select(-where(is.list)) |&gt;\n    pivot_longer(-c(method, scenario), values_to=\"value\", names_to=\"variable\")\n  \n  show(\n    ggplot(results_combined_vars_long, aes(x = scenario, y = value, fill = method)) +\n      geom_col(position = \"dodge\") +\n      facet_wrap(~variable, scales = \"free_y\", ncol = 2) +\n      labs(\n        title = \"Output Variables Comparison Across Methods and Scenarios\",\n        x = \"Scenario\",\n        y = \"Value\",\n        fill = \"Method\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  )\n\n  message(\"Look at startingE details\")\n  results_combined_startingE_vars &lt;- results_combined |&gt;\n    select(method, scenario, startingE, startingE_table) |&gt;\n    unnest_wider(startingE_table)\n  show(results_combined_startingE_vars)\n\n  results_combined_startingE_vars_long &lt;- results_combined |&gt;\n    select(method, scenario, startingE_table) |&gt;\n    unnest_longer(startingE_table, indices_to = \"variable\", values_to = \"value\") |&gt;\n    unnest_longer(value)\n  \n  show(\n    ggplot(results_combined_startingE_vars_long, aes(x = scenario, y = value, fill = method)) +\n      geom_col(position = \"dodge\") +\n      facet_wrap(~variable, scales = \"free_y\", ncol = 2) +\n      labs(\n        title = \"startingE_table Variables Comparison Across Methods and Scenarios\",\n        x = \"Scenario\",\n        y = \"Value\",\n        fill = \"Method\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  )\n}\n\n\npaper results\nSee paper\n\n\nstrength prior [1,10]\n\nmcmc with continuous prior [1,10] (like paper)\n\nMCMC_continuous_results_all &lt;- run_all_scenarios(\n  \"MCMC\", discrete = FALSE\n)\n\nusing webppl version: main v0.9.15-0eb9bf5 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\n\nmcmc with discrete prior [1,10]\n\nMCMC_discrete_results_all &lt;- run_all_scenarios(\n  \"MCMC\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"[1,10]\"\n)\n\nusing webppl version: main v0.9.15-0eb9bf5 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\n\nenumerate with discrete prior [1,10]\n\nenumerate_results_all &lt;- run_all_scenarios(\n  \"enumerate\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"[1,10]\"\n)\n\n\n\nsummarize\n\ncompare_results(\n  MCMC_continuous = MCMC_continuous_results_all,\n  MCMC_discrete = MCMC_discrete_results_all,\n  enumerate = enumerate_results_all\n)\n\nNow compare visually\n\n\n# A tibble: 18 × 3\n   method          scenario     P\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 MCMC_continuous ffff     0.612\n 2 MCMC_continuous fffl     0.969\n 3 MCMC_continuous flfl     0.953\n 4 MCMC_continuous ffll     1    \n 5 MCMC_continuous flll     0.993\n 6 MCMC_continuous llll     1    \n 7 MCMC_discrete   ffff     0.637\n 8 MCMC_discrete   fffl     0.968\n 9 MCMC_discrete   flfl     0.949\n10 MCMC_discrete   ffll     1    \n11 MCMC_discrete   flll     0.991\n12 MCMC_discrete   llll     1    \n13 enumerate       ffff     0.714\n14 enumerate       fffl     0.964\n15 enumerate       flfl     0.970\n16 enumerate       ffll     1    \n17 enumerate       flll     0.989\n18 enumerate       llll     1    \n\n\n\n\n\n\n\n\n\nNow separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare all output values across methods\n\n\n# A tibble: 18 × 12\n   method   scenario     P      aU      bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;    \n 1 MCMC_co… ffff     0.612  0.0939  0.0939  0.9   0.9   0.188   0.612 &lt;dbl&gt;     \n 2 MCMC_co… fffl     0.969 12.1     5.36    0.35  0.85 17.5     0.969 &lt;dbl&gt;     \n 3 MCMC_co… flfl     0.953 12.0    11.4     0.5   0.55 23.4     0.953 &lt;dbl&gt;     \n 4 MCMC_co… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n 5 MCMC_co… flll     0.993 14.5    14.5     0.4   0.4  28.9     0.993 &lt;dbl&gt;     \n 6 MCMC_co… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n 7 MCMC_di… ffff     0.637 -0.244   0.431   0.95  0.9   0.187   0.637 &lt;dbl&gt;     \n 8 MCMC_di… fffl     0.968 12.1     5.33    0.35  0.85 17.4     0.968 &lt;dbl&gt;     \n 9 MCMC_di… flfl     0.949 11.9    11.3     0.5   0.55 23.2     0.949 &lt;dbl&gt;     \n10 MCMC_di… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n11 MCMC_di… flll     0.991 14.4    14.4     0.4   0.4  28.9     0.991 &lt;dbl&gt;     \n12 MCMC_di… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n13 enumera… ffff     0.714  0.781   0.781   1     1     1.56    0.714 &lt;dbl&gt;     \n14 enumera… fffl     0.964 12.0     5.26    0.35  0.85 17.3     0.964 &lt;dbl&gt;     \n15 enumera… flfl     0.970 13.1     9.68    0.35  0.6  22.7     0.970 &lt;dbl&gt;     \n16 enumera… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n17 enumera… flll     0.989 14.4    14.4     0.4   0.4  28.8     0.989 &lt;dbl&gt;     \n18 enumera… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n# ℹ 2 more variables: b_strength &lt;list&gt;, startingE &lt;dbl&gt;\n\n\nExtract all final_table values into a comprehensive summary\n\n\n\n\n\n\n\n\n\nLook at startingE details\n\n\n# A tibble: 18 × 11\n   method   scenario startingE    aU    bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;list&gt;    \n 1 MCMC_co… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 2 MCMC_co… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 3 MCMC_co… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n 4 MCMC_co… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 5 MCMC_co… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n 6 MCMC_co… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n 7 MCMC_di… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 8 MCMC_di… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 9 MCMC_di… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n10 MCMC_di… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n11 MCMC_di… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n12 MCMC_di… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n13 enumera… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n14 enumera… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n15 enumera… flfl          0.3   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n16 enumera… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n17 enumera… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n18 enumera… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n# ℹ 1 more variable: b_strength &lt;list&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nstrength prior (1,10)\n\nmcmc with discrete prior (1,10)\n\nMCMC_discrete_results_exclexcl &lt;- run_all_scenarios(\n  \"MCMC\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"(1,10)\"\n)\n\n\n\nenumerate with discrete prior (1,10)\n\nenumerate_results_exclexcl &lt;- run_all_scenarios(\n  \"enumerate\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"(1,10)\"\n)\n\n\n\nsummarize\n\ncompare_results(\n  MCMC_continuous = MCMC_continuous_results_all,\n  MCMC_discrete = MCMC_discrete_results_exclexcl,\n  enumerate = enumerate_results_exclexcl\n)\n\nNow compare visually\n\n\n# A tibble: 18 × 3\n   method          scenario     P\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 MCMC_continuous ffff     0.612\n 2 MCMC_continuous fffl     0.969\n 3 MCMC_continuous flfl     0.953\n 4 MCMC_continuous ffll     1    \n 5 MCMC_continuous flll     0.993\n 6 MCMC_continuous llll     1    \n 7 MCMC_discrete   ffff     0.733\n 8 MCMC_discrete   fffl     0.969\n 9 MCMC_discrete   flfl     0.950\n10 MCMC_discrete   ffll     1    \n11 MCMC_discrete   flll     0.990\n12 MCMC_discrete   llll     1    \n13 enumerate       ffff     0.726\n14 enumerate       fffl     0.967\n15 enumerate       flfl     0.972\n16 enumerate       ffll     1    \n17 enumerate       flll     0.989\n18 enumerate       llll     1    \n\n\n\n\n\n\n\n\n\nNow separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare all output values across methods\n\n\n# A tibble: 18 × 12\n   method   scenario     P      aU      bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;    \n 1 MCMC_co… ffff     0.612  0.0939  0.0939  0.9   0.9   0.188   0.612 &lt;dbl&gt;     \n 2 MCMC_co… fffl     0.969 12.1     5.36    0.35  0.85 17.5     0.969 &lt;dbl&gt;     \n 3 MCMC_co… flfl     0.953 12.0    11.4     0.5   0.55 23.4     0.953 &lt;dbl&gt;     \n 4 MCMC_co… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n 5 MCMC_co… flll     0.993 14.5    14.5     0.4   0.4  28.9     0.993 &lt;dbl&gt;     \n 6 MCMC_co… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n 7 MCMC_di… ffff     0.733  1.16    1.16    1     1     2.33    0.733 &lt;dbl&gt;     \n 8 MCMC_di… fffl     0.969 12.1     5.35    0.35  0.85 17.4     0.969 &lt;dbl&gt;     \n 9 MCMC_di… flfl     0.950 12.0    11.3     0.5   0.55 23.2     0.950 &lt;dbl&gt;     \n10 MCMC_di… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n11 MCMC_di… flll     0.990 14.4    14.4     0.4   0.4  28.8     0.990 &lt;dbl&gt;     \n12 MCMC_di… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n13 enumera… ffff     0.726  1.02    1.02    1     1     2.03    0.726 &lt;dbl&gt;     \n14 enumera… fffl     0.967 12.1     5.31    0.35  0.85 17.4     0.967 &lt;dbl&gt;     \n15 enumera… flfl     0.972 13.1     9.72    0.35  0.6  22.8     0.972 &lt;dbl&gt;     \n16 enumera… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n17 enumera… flll     0.989 14.4    14.4     0.4   0.4  28.7     0.989 &lt;dbl&gt;     \n18 enumera… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n# ℹ 2 more variables: b_strength &lt;list&gt;, startingE &lt;dbl&gt;\n\n\nExtract all final_table values into a comprehensive summary\n\n\n\n\n\n\n\n\n\nLook at startingE details\n\n\n# A tibble: 18 × 11\n   method   scenario startingE    aU    bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;list&gt;    \n 1 MCMC_co… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 2 MCMC_co… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 3 MCMC_co… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n 4 MCMC_co… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 5 MCMC_co… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n 6 MCMC_co… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n 7 MCMC_di… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n 8 MCMC_di… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 9 MCMC_di… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n10 MCMC_di… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n11 MCMC_di… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n12 MCMC_di… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n13 enumera… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n14 enumera… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n15 enumera… flfl          0.3   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n16 enumera… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n17 enumera… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n18 enumera… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n# ℹ 1 more variable: b_strength &lt;list&gt;"
  },
  {
    "objectID": "webppl vs memo/webppl.html",
    "href": "webppl vs memo/webppl.html",
    "title": "",
    "section": "",
    "text": "Cards\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nalice_chooses_card_model &lt;- '\nvar alice_chooses_card = function() {\n   var card = sample(Categorical({ vs: [1,2,3] }))\n   return card\n}\nvar dist = Infer(alice_chooses_card)\nexpectation(dist)\n'\nalice_chooses_card_E &lt;- rwebppl::webppl(alice_chooses_card_model)\n\nusing webppl version: main v0.9.15-0eb9bf5 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\nshow(alice_chooses_card_E)\n\n[1] 2\n\n\n\n\nBasic conditioning\n\n\nlibrary(tidyverse)\nbasic_inference_model &lt;- '\nvar model = function() {\n   // uniform prior of true value, discretized for parity with memo\n   var value = randomInteger(101) / 100; // uniform(0, 1)\n\n   // someone has observed that value is &gt;.5\n   condition(value &gt; .5)\n   \n   // return the estimated probability of each possible value\n   return value\n}\nvar dist = Infer(model)\ndist\n\n// alternatively, output the expected value:\n// expectation(dist)\n'\nposterior &lt;- rwebppl::webppl(basic_inference_model) |&gt;\n   arrange(support)\n\n# extra: could compute E of posterior manually like this\n# posteriorE &lt;- sum(as.numeric(posterior$prob) * as.numeric(posterior$support))\n# posteriorE\n\nposterior |&gt;\n   # fill in missing support (HACK: need to round support values to ensure compatibility when calling `complete`)\n   mutate(support = round(support, 3)) |&gt;\n   complete(support = seq(0, 1, by = 0.01) |&gt; round(3), fill = list(prob = 0)) |&gt;\n   ggplot() + geom_line(aes(x=support, y=prob)) + lims(x=c(0,1), y=c(0, NA))"
  },
  {
    "objectID": "webppl vs memo/index.html#memo",
    "href": "webppl vs memo/index.html#memo",
    "title": "",
    "section": "memo",
    "text": "memo"
  },
  {
    "objectID": "webppl vs memo/memo.html",
    "href": "webppl vs memo/memo.html",
    "title": "Basic conditioning",
    "section": "",
    "text": "Cards\n\n# adapted from https://github.com/kach/memo/blob/main/demo/Memonomicon.ipynb\nimport jax\nimport jax.numpy as np\nfrom memo import memo\n\ncards = np.array([1, 2, 3])\n\n@memo(save_comic=\"memo-comic-card\")\ndef alice_chooses_card_E():\n    alice: chooses(c in cards, wpp=1)\n    return E[alice.c]\n\n# Comic representation of the modeling: ./memo-comic-card.png\n\n\nprint(alice_chooses_card_E())\n\n2.0\n\n\n\nfrom memo import memo\nimport jax.numpy as np\n\npossible_values = np.array(range(1,101)) / 100\n\n@memo(save_comic=\"memo-comic-bc\")\ndef model[query_v: possible_values]():\n    # establish prior in the observer's frame: that a value generator has some value from .01 to 1\n    value_gen: given(v in possible_values, wpp=1)\n    \n    # push an observation into a valuator's frame:\n    # first, establish the prior in the valuator's frame: have the valuator model that the value generator chose a value uniformly at random\n    valuator: thinks[ value_gen: given(v in possible_values, wpp=1) ]\n    # then, now that the value is modeled by the valuator, condition that value\n    valuator: observes_that [value_gen.v &gt; .5]\n\n    # return the estimated probability of the generator's value for being each queried v, *estimated according to the valuator/from the valuator's frame*\n    valuator: knows(query_v) # first, push query_v into the frame of the valuator\n    return valuator[Pr[value_gen.v == query_v]]\n\n    # or return the expected value in the valuator's frame:\n    # return valuator[E[value_gen.v]]\n\n# Comic representation of the modeling: ./memo-comic-bc.png\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(possible_values, model())"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "title": "Helper Functions (JAX-compatible)",
    "section": "",
    "text": "Code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R\nNOTE 10/20/25: next time I run the cached cells, I’ll need to remove the “webppl vs memo/” directory prefix, and add a code block that sets the working directory appropriately for interactive execution"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Setup",
    "text": "Setup\n\nimport jax\nimport jax.numpy as np\nfrom memo import memo\nfrom matplotlib import pyplot as plt\n\n# Model parameters (from original WebPPL code)\nalpha = 13.5  # effort cost coefficient\nbeta = 24.5   # inequality aversion coefficient\nweight_box = 5  # weight of the box\nlow_reward = 10\nhigh_reward = 20\n\n# Discretized spaces (for computational tractability)\nefforts = np.array([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n                    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])\n\n# Discretized strength space (replacing continuous uniform(1,10))\n# (1,10) with digits=1\nstrength_values = np.linspace(1.1, 9.9, 89)\n# # [1,10] with digits=1\n# strength_values = np.linspace(1., 10., 91)\n\n\n@jax.jit\ndef lift(strength, effort):\n    \"\"\"Single agent lift: can they lift the box?\"\"\"\n    return (effort * strength &gt;= weight_box).astype(float)\n\n@jax.jit\ndef lift2(strength1, strength2, effort1, effort2):\n    \"\"\"Two-agent joint lift: can they lift the box together?\"\"\"\n    return (effort1 * strength1 + effort2 * strength2 &gt;= weight_box).astype(float)\n\n@jax.jit\ndef gini(effort1, effort2):\n    \"\"\"Gini coefficient for inequality aversion\"\"\"\n    return np.where(\n        effort1 == effort2,\n        0.0,\n        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)\n    )\n\n@jax.jit\ndef argmax_utility(strength, reward, effort_space=efforts):\n    \"\"\"Find effort that maximizes utility for single agent\"\"\"\n    utilities = reward * lift(strength, effort_space) - alpha * effort_space\n    return effort_space[np.argmax(utilities)]\n\n@jax.jit\ndef outcome(strength, reward):\n    \"\"\"Predicted outcome (success probability) for single agent at optimal effort\"\"\"\n    opt_effort = argmax_utility(strength, reward)\n    return lift(strength, opt_effort)\n\n# DEBUG: Test helper functions with sample values\nprint(\"=== HELPER FUNCTION TESTS ===\")\ntest_strength = 5.0\ntest_effort = 1.0\ntest_lift_result = lift(test_strength, test_effort)\nprint(f\"lift(strength=5.0, effort=1.0): {test_lift_result}\")\n\ntest_optE_low = argmax_utility(test_strength, low_reward)\nprint(f\"optE(strength=5.0, reward=10): {test_optE_low}\")\n\ntest_optE_high = argmax_utility(test_strength, high_reward)\nprint(f\"optE(strength=5.0, reward=20): {test_optE_high}\")\n\ntest_outcome_low = outcome(test_strength, low_reward)\nprint(f\"outcome(strength=5.0, reward=10): {test_outcome_low}\")\n\ntest_outcome_high = outcome(test_strength, high_reward)\nprint(f\"outcome(strength=5.0, reward=20): {test_outcome_high}\")\n\n=== HELPER FUNCTION TESTS ===\nlift(strength=5.0, effort=1.0): 1.0\noptE(strength=5.0, reward=10): 0.0\noptE(strength=5.0, reward=20): 1.0\noutcome(strength=5.0, reward=10): 0.0\noutcome(strength=5.0, reward=20): 1.0"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Bayesian Inference Over Agent Strengths",
    "text": "Bayesian Inference Over Agent Strengths\nThe original WebPPL code infers agent strengths from observed outcomes in rounds 1 and 2.\n\n# Helper function for the joint condition (needed because & operator not supported in memo)\n@jax.jit\ndef both_true(cond1, cond2):\n    \"\"\"Returns True if both conditions are True (for use in joint probability queries)\"\"\"\n    return cond1 & cond2\n\n@memo(save_comic=\"memo-comic-xiang-strength-inference\")\ndef infer_strengths[query_sa: strength_values, query_sb: strength_values](\n    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b\n):\n    \"\"\"\n    Infer posterior distribution over agent strengths given observed outcomes.\n\n    Observations:\n    - Round 1 (low reward = 10): A and B each attempt individual lifts\n    - Round 2 (high reward = 20): A and B each attempt individual lifts\n\n    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]\n    \"\"\"\n    # Prior: Agent A has some strength uniformly distributed in [1, 10]\n    agent_a: given(sa in strength_values, wpp=1)\n\n    # Prior: Agent B has some strength uniformly distributed in [1, 10]\n    agent_b: given(sb in strength_values, wpp=1)\n\n    # The participant models that the agents have some strength\n    participant: thinks[\n        agent_a: given(sa in strength_values, wpp=1),\n        agent_b: given(sb in strength_values, wpp=1)\n    ]\n\n    # Participant witnesses the outcomes and conditions on them\n    # Condition on round 1 outcomes (low reward)\n    participant: observes_that [outcome(agent_a.sa, {low_reward}) == r1_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {low_reward}) == r1_outcome_b]\n\n    # Condition on round 2 outcomes (high reward)\n    participant: observes_that [outcome(agent_a.sa, {high_reward}) == r2_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {high_reward}) == r2_outcome_b]\n\n    # Push query variables into participant frame \n    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths\n    participant: knows(query_sa)\n    participant: knows(query_sb)\n    return participant[Pr[both_true(agent_a.sa == query_sa, agent_b.sb == query_sb)]]\n    # (Using helper function for bitwise-and since & operator not supported in memo DSL)\n\n# Example: F,F;F,F scenario (both agents fail in both rounds)\n# This would be computationally expensive with many strength values\n# For testing, could use a coarser grid\nposterior_strengths_ffff = infer_strengths(0.0, 0.0, 0.0, 0.0)\n\n# DEBUG: Bayesian inference statistics\nprint(\"\\n=== BAYESIAN INFERENCE STATISTICS ===\")\nposterior_2d = posterior_strengths_ffff.reshape(len(strength_values), len(strength_values))\nprint(f\"Posterior shape: {posterior_2d.shape}\")\nprint(f\"Total probability mass: {np.sum(posterior_2d):.6f}\")\n\n# Compute marginals\nmarginal_a = np.sum(posterior_2d, axis=1)\nmarginal_b = np.sum(posterior_2d, axis=0)\n\n# # Compute statistics\n# mean_sa = np.sum(marginal_a * strength_values)\n# mean_sb = np.sum(marginal_b * strength_values)\n# print(f\"Agent A strength - mean: {mean_sa:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n# print(f\"Agent B strength - mean: {mean_sb:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n\n# # Find high-probability regions\n# high_prob_threshold = np.max(posterior_2d) * 0.1\n# high_prob_indices = np.where(posterior_2d &gt; high_prob_threshold)\n# print(f\"High probability region (top 10%): strength_a in [{strength_values[np.min(high_prob_indices[0])]:.2f}, {strength_values[np.max(high_prob_indices[0])]:.2f}]\")\n# print(f\"                                     strength_b in [{strength_values[np.min(high_prob_indices[1])]:.2f}, {strength_values[np.max(high_prob_indices[1])]:.2f}]\")\n\n# # For F,F;F,F scenario, agents fail at BOTH low_reward=10 and high_reward=20\n# # This means they cannot lift box even with high incentive\n# # Let's check what strengths are actually consistent with failing both rounds\n# print(\"\\n=== FIXME #1: Investigating strength bounds ===\")\n# print(\"For F,F;F,F: agents fail at reward=10 AND reward=20\")\n# print(f\"Box weight: {weight_box}\")\n# print(f\"Alpha (effort cost): {alpha}\")\n\n# # Check a few sample strengths to see if they would fail\n# test_strengths = [3.0, 4.0, 4.5, 4.9, 5.0, 5.5, 6.0, 7.0]\n# for s in test_strengths:\n#     outcome_low = outcome(s, low_reward)\n#     outcome_high = outcome(s, high_reward)\n#     opt_e_low = argmax_utility(s, low_reward)\n#     opt_e_high = argmax_utility(s, high_reward)\n#     print(f\"  strength={s:.1f}: outcome(R=10)={outcome_low:.0f} (effort={opt_e_low:.2f}), outcome(R=20)={outcome_high:.0f} (effort={opt_e_high:.2f})\")\n\n# Find the actual support of the posterior (non-zero probability mass)\nposterior_support = posterior_2d &gt; 1e-10\nsupport_indices = np.where(posterior_support)\nif len(support_indices[0]) &gt; 0:\n    print(f\"Non-negligible posterior support:\")\n    print(f\"  strength_a in [{strength_values[np.min(support_indices[0])]:.2f}, {strength_values[np.max(support_indices[0])]:.2f}]\")\n    print(f\"  strength_b in [{strength_values[np.min(support_indices[1])]:.2f}, {strength_values[np.max(support_indices[1])]:.2f}]\")\nelse:\n    print(\"\\nWARNING: Posterior has no support! Inference may have failed.\")\n\n\n=== BAYESIAN INFERENCE STATISTICS ===\nPosterior shape: (89, 89)\nTotal probability mass: 0.999996\nNon-negligible posterior support:\n  strength_a in [1.10, 4.90]\n  strength_b in [1.10, 4.90]"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-effort-model",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-effort-model",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Game-Theoretic Joint Effort Model",
    "text": "Game-Theoretic Joint Effort Model\nThis is the most complex part. The original WebPPL code models: 1. Two agents with uncertain strengths (posterior distributions from rounds 1-2) 2. Agents reason about each other’s efforts in round 3 (joint task) 3. Iterative best-response until Nash equilibrium 4. Optimization over initial effort to maximize joint utility\nKey insight: The game-theoretic reasoning operates on expected utilities over the posterior distribution of strengths. This is not purely probabilistic reasoning (like the inference above), but rather decision-theoretic optimization under uncertainty.\nProposed approach: - Keep the Bayesian inference in @memo (already done above) - Implement the game-theoretic equilibrium finding in pure JAX (deterministic optimization) - Combine them: compute equilibrium for each strength pair, weighted by posterior\n\n@jax.jit\ndef joint_utility_single_strength(strength_a, strength_b, effort_a, effort_b):\n    \"\"\"\n    Compute utilities for both agents given fixed strengths and efforts.\n\n    Returns: (utility_a, utility_b, success_prob)\n    \"\"\"\n    reward = high_reward  # Round 3 uses high reward\n    success = lift2(strength_a, strength_b, effort_a, effort_b)\n    gini_coef = gini(effort_a, effort_b)\n\n    utility_a = reward * success - alpha * effort_a - beta * gini_coef\n    utility_b = reward * success - alpha * effort_b - beta * gini_coef\n\n    return utility_a, utility_b, success\n\n@jax.jit\ndef best_response_a(strength_a, strength_b, effort_b):\n    \"\"\"\n    Find agent A's best response to agent B's effort.\n\n    Agent A optimizes: max_{e_a} E[utility_a(e_a, e_b) | s_a, s_b]\n    \"\"\"\n    # If strength_a and strength_b are distributions, we'd need to take expectations\n    # For now, assume they are point estimates\n    # TODO: I think if they are arrays right now, they're silently being operated on item-wise. is this sufficient, or do we need to take expectation? \n    utilities = jax.vmap(\n        lambda ea: joint_utility_single_strength(strength_a, strength_b, ea, effort_b)[0]\n    )(efforts)\n\n    return efforts[np.argmax(utilities)]\n\n@jax.jit\ndef best_response_b(strength_a, strength_b, effort_a):\n    \"\"\"\n    Find agent B's best response to agent A's effort.\n    \"\"\"\n    utilities = jax.vmap(\n        lambda eb: joint_utility_single_strength(strength_a, strength_b, effort_a, eb)[1]\n    )(efforts)\n\n    return efforts[np.argmax(utilities)]\n\ndef find_equilibrium(strength_a, strength_b, init_effort, max_depth=10, verbose=False):\n    \"\"\"\n    Find Nash equilibrium through iterated best responses.\n\n    Starting from init_effort for agent B, iterate:\n    - A responds to B\n    - B responds to A\n    Until convergence (or max_depth reached)\n\n    Note: depth starts at 1 to match WebPPL convention (not 0-indexed)\n    \"\"\"\n    effort_b = init_effort\n\n    # Start depth at 1 to match WebPPL convention\n    for depth in range(1, max_depth + 1):\n        effort_a = best_response_a(strength_a, strength_b, effort_b)\n        effort_b_new = best_response_b(strength_a, strength_b, effort_a)\n\n        # Check convergence\n        if np.abs(effort_b_new - effort_b) &lt; 0.06:\n            if verbose:\n                print(f\"  Converged at depth {depth}: effort_a={effort_a:.4f}, effort_b={effort_b_new:.4f}\")\n            return effort_a, effort_b_new, depth\n\n        effort_b = effort_b_new\n\n    print(f\"Warning: Equilibrium did not converge in {max_depth} iterations\")\n    return effort_a, effort_b, max_depth\n\n# To find the optimal initial effort, we'd search over init_effort values\ndef find_optimal_init_effort(posterior_sa_sb):\n    \"\"\"\n    Find initial effort that maximizes expected joint utility.\n\n    This matches the WebPPL startingEffort() function which optimizes jointU.\n    \"\"\"\n    joint_utilities = []\n    for init_effort in efforts:\n        stats = compute_expected_statistics(posterior_sa_sb, init_effort)\n        joint_utilities.append(stats['joint_utility'])  # Optimize joint utility (matches WebPPL)\n    \n    joint_utilities = np.array(joint_utilities)\n    return efforts[np.argmax(joint_utilities)]\n\ndef compute_expected_statistics(posterior_sa_sb, init_effort):\n    \"\"\"\n    Compute expected utilities and efforts over posterior distribution of strengths.\n\n    This matches the WebPPL jointUtility() function output, computing expectations over\n    the posterior distribution of agent strengths.\n\n    Args:\n        posterior_sa_sb: 2D array of posterior probabilities P(sa, sb | observations)\n                        Shape: (len(strength_values), len(strength_values))\n        init_effort: Initial effort for agent B to start equilibrium search\n\n    Returns:\n        dict with keys: joint_utility, agent_a_utility, agent_b_utility,\n                       agent_a_effort, agent_b_effort, outcome_prob\n    \"\"\"\n    expected_joint_utility = 0.0\n    expected_agent_a_utility = 0.0\n    expected_agent_b_utility = 0.0\n    expected_agent_a_effort = 0.0\n    expected_agent_b_effort = 0.0\n    expected_outcome = 0.0\n\n    # Iterate over all strength combinations\n    for i, sa in enumerate(strength_values):\n        for j, sb in enumerate(strength_values):\n            prob_sa_sb = posterior_sa_sb[i, j]\n\n            if prob_sa_sb &gt; 1e-10:  # Only compute for non-negligible probabilities\n                # Find equilibrium efforts for this strength pair\n                effort_a, effort_b, _ = find_equilibrium(sa, sb, init_effort)\n\n                # Compute utilities and outcome at equilibrium\n                utility_a, utility_b, success_prob = joint_utility_single_strength(sa, sb, effort_a, effort_b)\n\n                # Weight by posterior probability and accumulate\n                expected_agent_a_utility += prob_sa_sb * utility_a\n                expected_agent_b_utility += prob_sa_sb * utility_b\n                expected_joint_utility += prob_sa_sb * (utility_a + utility_b)\n                expected_agent_a_effort += prob_sa_sb * effort_a\n                expected_agent_b_effort += prob_sa_sb * effort_b\n                expected_outcome += prob_sa_sb * success_prob\n\n    return {\n        'joint_utility': expected_joint_utility,\n        'agent_a_utility': expected_agent_a_utility,\n        'agent_b_utility': expected_agent_b_utility,\n        'agent_a_effort': expected_agent_a_effort,\n        'agent_b_effort': expected_agent_b_effort,\n        'outcome_prob': expected_outcome\n    }"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-ffff-scenario",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-ffff-scenario",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Running the Model for F,F;F,F Scenario",
    "text": "Running the Model for F,F;F,F Scenario\n\n# Scenario: Both agents fail in both rounds (F,F;F,F)\n# This means weak agents who cannot lift the box individually even with high reward\n\n# Get posterior over strengths\nprint(\"\\nComputing posterior over agent strengths...\")\nposterior_ffff = posterior_strengths_ffff.reshape(len(strength_values), len(strength_values))\n\n# Test equilibrium finding with a sample strength pair\nprint(\"\\n=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===\")\n# Use the modal strength values as a test case\nmodal_idx_a = np.argmax(np.sum(posterior_ffff, axis=1))\nmodal_idx_b = np.argmax(np.sum(posterior_ffff, axis=0))\nsample_sa = strength_values[modal_idx_a]\nsample_sb = strength_values[modal_idx_b]\nprint(f\"Testing with strength_a={sample_sa:.2f}, strength_b={sample_sb:.2f}\")\neffort_a_test, effort_b_test, depth_test = find_equilibrium(sample_sa, sample_sb, 0.0, verbose=True)\nprint(f\"Convergence depth: {depth_test}\")\nprint(f\"Agent A equilibrium effort: {effort_a_test:.4f}\")\nprint(f\"Agent B equilibrium effort: {effort_b_test:.4f}\")\n\n# Find optimal initial effort (by searching over effort space)\nprint(\"\\n=== FINDING OPTIMAL INITIAL EFFORT ===\")\n# For now, just use a default initial effort\noptimal_init_effort = find_optimal_init_effort(posterior_ffff)\nprint(f\"Using initial effort: {optimal_init_effort}\")\n\n# Compute expected statistics at equilibrium (utilities, efforts, outcome)\nprint(\"\\nComputing expected statistics at equilibrium...\")\nstats = compute_expected_statistics(posterior_ffff, optimal_init_effort)\n\nprint(\"\\n=== FINAL RESULTS ===\")\nprint(f\"Optimal starting effort: {optimal_init_effort}\")\nprint(f\"Joint utility at optimum: {stats['joint_utility']:.6f}\")\nprint(f\"Agent A utility: {stats['agent_a_utility']:.6f}\")\nprint(f\"Agent B utility: {stats['agent_b_utility']:.6f}\")\nprint(f\"Agent A equilibrium effort: {stats['agent_a_effort']:.6f}\")\nprint(f\"Agent B equilibrium effort: {stats['agent_b_effort']:.6f}\")\nprint(f\"Expected outcome probability: {stats['outcome_prob']:.6f}\")\nprint(f\"\\n(Compare to original WebPPL output)\")\n\n# Sanity check: joint utility should equal sum of individual utilities\njoint_check = stats['agent_a_utility'] + stats['agent_b_utility']\nif abs(joint_check - stats['joint_utility']) &gt; 1e-5:\n    print(f\"WARNING: Joint utility mismatch! {stats['joint_utility']:.6f} != {joint_check:.6f}\")\n\n\nComputing posterior over agent strengths...\n\n=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===\nTesting with strength_a=1.10, strength_b=1.10\n  Converged at depth 1: effort_a=0.0000, effort_b=0.0000\nConvergence depth: 1\nAgent A equilibrium effort: 0.0000\nAgent B equilibrium effort: 0.0000\n\n=== FINDING OPTIMAL INITIAL EFFORT ===\nUsing initial effort: 0.8999999761581421\n\nComputing expected statistics at equilibrium...\n\n=== FINAL RESULTS ===\nOptimal starting effort: 0.8999999761581421\nJoint utility at optimum: 11.368189\nAgent A utility: 7.008577\nAgent B utility: 4.359619\nAgent A equilibrium effort: 0.399570\nAgent B equilibrium effort: 0.595792\nExpected outcome probability: 0.669291\n\n(Compare to original WebPPL output)\n\n\n\nVisualization\n\n# Visualize posterior over agent strengths\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Posterior heatmap\nim = axes[0].imshow(posterior_ffff, origin='lower', extent=[1, 10, 1, 10], aspect='auto')\naxes[0].set_xlabel('Agent B Strength')\naxes[0].set_ylabel('Agent A Strength')\naxes[0].set_title('Posterior P(strength_A, strength_B | F,F;F,F)')\nplt.colorbar(im, ax=axes[0], label='Probability')\n\n# Marginal distributions\nmarginal_a = np.sum(posterior_ffff, axis=1)\nmarginal_b = np.sum(posterior_ffff, axis=0)\n\naxes[1].plot(strength_values, marginal_a, label='Agent A', linewidth=2)\naxes[1].plot(strength_values, marginal_b, label='Agent B', linewidth=2, linestyle='--')\naxes[1].set_xlabel('Strength')\naxes[1].set_ylabel('Probability')\naxes[1].set_title('Marginal Distributions')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Notes on Conversion Strategy",
    "text": "Notes on Conversion Strategy\nThe conversion from WebPPL to memo revealed important distinctions:\n\n1. Probabilistic Inference vs. Decision Theory\nThe original WebPPL code mixes two types of reasoning: - Bayesian inference (rounds 1-2): Infer agent strengths from observed outcomes - Game-theoretic optimization (round 3): Find Nash equilibrium efforts\nIn the memo conversion: - Bayesian inference: Use @memo with given(), observes_that[], and thinks[] - Game-theoretic optimization: Use pure JAX functions (no probabilistic reasoning, just optimization)\n\n\n2. Key Conversion Decisions\nMutable arrays (x2a, x2b): - WebPPL: Pushes strength posterior samples to arrays, uses them later - memo: Query the posterior distribution directly (more principled)\nNested agent reasoning: - WebPPL: Nested JavaScript functions with recursive best-response - memo/JAX: Iterative best-response in pure JAX (not probabilistic, so no thinks[] needed here)\nExpected utilities: - WebPPL: Implicitly handles through sampling or enumeration - memo/JAX: Explicit loop over posterior, computing equilibrium for each strength pair\n\n\n3. Computational Considerations\nThe original uses either: - MCMC with 10,000 samples (slow but handles continuous distributions) - Enumeration with discrete values (exact but potentially more expensive)\n\n\n4. What Could Be Improved\nPotential optimizations (if using pure game-theoretic optimization): - Use coarser grid for exploration, then refine around high-probability regions - Vectorize the equilibrium finding across strength pairs (currently sequential) - Use automatic differentiation to find equilibrium (gradient-based instead of grid search)\nFurther extension with memo: Currently, the game-theoretic reasoning is outside of @memo. One could potentially: - Model agents’ beliefs about each other’s strengths using nested thinks[] - Model agents’ choices as chooses(e in efforts, wpp=utility(e)) - But this might be overkill and harder to reason about - However, this does incorporate uncertainty about the other agent into the model, which might add richness"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "title": "Helper Functions (JAX-compatible)",
    "section": "How to Compare Outputs",
    "text": "How to Compare Outputs\nTo verify that the WebPPL and memo implementations produce the same results:\nThese should match (up to numerical precision and discretization choices).\nCompare outcome probability (here, expected_outcome_ffff), strength posteriors, and joint utility results for r3 outcome and for starting effort.\n\nExpected matches:\n\nHelper functions (lift, optE, outcome) should return identical values for test inputs\nBayesian inference statistics should show similar:\n\nPosterior means (agent strengths)\nNon-neglible-probability regions\n\nEquilibrium finding should show similar:\n\nConvergence depth (may vary slightly due to discretization)\nEquilibrium efforts\n\nFinal outcome probability should be close\n\nKnown differences:\n\nSee above\nSmall numerical differences due to different underlying computation engines"
  }
]