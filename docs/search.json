[
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html",
    "href": "webppl vs memo/xiang2023-exp1-round3.html",
    "title": "",
    "section": "",
    "text": "code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R\nlibrary(tidyverse)\nlibrary(rwebppl)\n\njoint &lt;- data.frame(model = 'joint',\n                    agent = rep(c('A','B'), 18), \n                    scenario = c(rep('F,F;F,F',6),rep('F,F;F,L',6),rep('F,L;F,L',6),rep('F,F;L,L',6),rep('F,L;L,L',6),rep('L,L;L,L',6)),\n                    effort = 0, strength = 0,\n                    outcome = 0, prob = 0, round = rep(c(1,1,2,2,3,3), 6), \n                    reward = rep(c(10,10,20,20,20,20), 6))\njoint$prob[joint$round==1] = NaN\njoint$outcome = c(integer(6), integer(3),1,integer(2), 0,1,0,1,1,1, integer(2),integer(4)+1, 0,integer(5)+1, integer(6)+1)"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "title": "",
    "section": "1 - mcmc",
    "text": "1 - mcmc\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\" # footnote: For computational tractability, we constrained the effort space to discrete values, ranging from 0 to 1 with increments of 0.05.\n\n## R3 Probability\nmdl1 &lt;- \"\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'MCMC', kernel: 'MH', samples: 10000, burn: 1000, model() {\n  var sa = uniform(1,10)\n  var sb = uniform(1,10)\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      // display(tbl.jointU)\n      return tbl.jointU\n    },\n    efforts);\n};\n\nvar startingE = startingEffort(x2a,x2b)\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\nmdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\na &lt;- webppl(mdl)\njoint$prob[joint$round==3 & joint$scenario=='F,F;F,F'] &lt;- a$P*100\n\n\njoint"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "title": "",
    "section": "2 - enumeration",
    "text": "2 - enumeration\nnow trying it using enumeration (using a discretized strength space), as a step towards translating into memo:\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\"\n\n## R3 Probability\nmdl1 &lt;- \"\n// convert to a @jax.jit?\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\n// convert to a @jax.jit\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\n// convert to a @jax.jit\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\n// convert to a @jax.jit\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\n// convert to a @memo, and instead of pushing x2a and x2b to a global variable, bring in the computations which use them into the @memo ?\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'enumerate', model() {\n  var sa = (11+randomInteger(89))/10 // not in a general form, but computes values in (1, 10) instead of in [1, 10] to see if that fixes things\n  var sb = (11+randomInteger(89))/10\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\n// convert to a combination of @jax.jit and @memo ? so I'm not doing two-step nested optimization (like the difference between RSA in webppl and in memo)\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// convert to a @jax.jit\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      return tbl.jointU\n    },\n    efforts);\n};\n\n\nvar startingE = startingEffort(x2a,x2b)\n// display(jointUtility(startingE,x2a,x2b))\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\nmdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\nwebppl(mdl)\n\n\n\nnote that their model is assuming that 1 and 10 are lower probability, which is just a computational limitation! when enumerating, the resulting p is infinitesimally small but matches all others; when using MCMC, the bounds are underweighted.\n\n-&gt; using something like (11+randomInteger(89))/10 to simulate sampling from (1,10) rather than [1,10]; inspired by the observation that webppl’s uniform distribution is actually more like this, since there are only finite samples\nmaybe just need to exclude 1 from the range and not 10? -&gt; trying different versions of the discretized distribution"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "title": "",
    "section": "3 - comparison across scenarios",
    "text": "3 - comparison across scenarios\nNow moved into xiang2023-exp1-round3-with-debugging.wppl, which has input parameters and logs.\nWalking step by step and validate that the values (posteriors and equilibria across all 6 scenarios) match:\n\n\npaper results\n\n\nwebppl mcmc (below)\n\n\nwebppl mcmc discrete (below)\n\n\nwebppl enumerate (below)\n\n\nTODO (in-progress) determine if need to use (1,10) (or another variant) instead of [1,10] for strength prior\n\n\nTODO finish memo enumerate, with same parameters as webppl enumerate (xiang2023-exp1-round3-memo.qmd); and log all the stats (see stats) – and investigate discrepancies. e.g. why is Joint utility at optimum so high? why are the agents’ utilities different from each other?\n\n\nmoonshot: memo enumerate, but the equilibrium-finding process uses memo frames in order to model agent’s uncertainty about each other (still accelerated by JAX, but now in the language of memo, and considering the uncertainty)\n\n\nTODO: recreate compensatory effort and solitary effort models in memo; simple, no need to validate incrementally like doing for joint effort model\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nFAIL = FALSE\nLIFT = TRUE\n\n\npaper results\nSee paper\n\n\nstrength prior [1,10]\n\nmcmc with continuous prior [1,10] (like paper)\n\nsuppressWarnings(rwebppl::kill_webppl())\n\nusing webppl version: main v0.9.15-27823b2 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\nMCMC_continuous_ffff &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = FAIL\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_continuous_fffl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_continuous_flfl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_continuous_ffll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_continuous_flll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_continuous_llll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = FALSE,\n\n    a_r1result = LIFT, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\n\nmcmc with discrete prior [1,10]\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_ffff &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = FAIL\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_fffl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_flfl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_ffll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_flll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_llll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = LIFT, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\n\nenumerate with discrete prior [1,10]\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_ffff &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = FAIL\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_fffl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_flfl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_ffll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_flll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_llll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"[1,10]\",\n\n    a_r1result = LIFT, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\n\nsummarize\n\ncompare_results()\n\n[1] \"Now compare visually\"\n# A tibble: 6 × 4\n  scenario MCMC_continuous MCMC_discrete enumerate\n  &lt;chr&gt;              &lt;int&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 F,F;F,F                0         0.711     0.649\n2 F,F;F,L                1         1         1    \n3 F,L;F,L                1         1         1    \n4 F,F;L,L                1         1         1    \n5 F,L;L,L                1         1         1    \n6 L,L;L,L                1         1         1    \n\n\n\n\n\n\n\n\n\n[1] \"Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"Compare all output values across methods\"\n[1] \"Extract all final_table values into a comprehensive summary\"\n# A tibble: 18 × 9\n   scenario method          startingE    aE    bE     aU     bU jointU     P\n   &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 ffff     MCMC_continuous     0.188  0.15  0.15 -2.02  -2.02  -4.05  0    \n 2 fffl     MCMC_continuous    17.5    0     1    13.9    0.375 14.2   1    \n 3 flfl     MCMC_continuous    23.4    0.1   0.7  14.1    5.96  20.0   1    \n 4 ffll     MCMC_continuous    26.5    1     0     0.375 13.9   14.2   1    \n 5 flll     MCMC_continuous    28.9    1     0     0.375 13.9   14.2   1    \n 6 llll     MCMC_continuous    30.6    0.6   0.1   7.52  14.3   21.8   1    \n 7 ffff     MCMC_discrete       1.46   1     1     0.728  0.728  1.46  0.711\n 8 fffl     MCMC_discrete      17.1    0     1    13.9    0.375 14.2   1    \n 9 flfl     MCMC_discrete      22.9    0.1   0.7  14.1    5.96  20.0   1    \n10 ffll     MCMC_discrete      26.5    1     0     0.375 13.9   14.2   1    \n11 flll     MCMC_discrete      28.8    1     0     0.375 13.9   14.2   1    \n12 llll     MCMC_discrete      30.6    0.6   0.1   7.52  14.3   21.8   1    \n13 ffff     enumerate           1.38   0.95  0.95  0.163  0.163  0.325 0.649\n14 fffl     enumerate          17.1    0     1    13.9    0.375 14.2   1    \n15 flfl     enumerate          22.8    0.1   0.7  14.1    5.96  20.0   1    \n16 ffll     enumerate          26.5    1     0     0.375 13.9   14.2   1    \n17 flll     enumerate          28.8    1     0     0.375 13.9   14.2   1    \n18 llll     enumerate          30.6    0.6   0.1   7.52  14.3   21.8   1    \n# A tibble: 42 × 5\n   scenario variable  MCMC_continuous MCMC_discrete enumerate\n   &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 ffff     startingE           0.188         1.46      1.38 \n 2 ffff     aE                  0.15          1         0.95 \n 3 ffff     bE                  0.15          1         0.95 \n 4 ffff     aU                 -2.02          0.728     0.163\n 5 ffff     bU                 -2.02          0.728     0.163\n 6 ffff     jointU             -4.05          1.46      0.325\n 7 ffff     P                   0             0.711     0.649\n 8 fffl     startingE          17.5          17.1      17.1  \n 9 fffl     aE                  0             0         0    \n10 fffl     bE                  1             1         1    \n# ℹ 32 more rows\n\n\n\n\n\n\n\n\n\n# A tibble: 18 × 7\n   scenario method             aE    bE      aU      bU jointU\n   &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 ffff     MCMC_continuous  0.9   0.9   0.0939  0.0939  0.188\n 2 fffl     MCMC_continuous  0.35  0.85 12.1     5.36   17.5  \n 3 flfl     MCMC_continuous  0.5   0.55 12.0    11.4    23.4  \n 4 ffll     MCMC_continuous  0.5   0.5  13.2    13.2    26.5  \n 5 flll     MCMC_continuous  0.4   0.4  14.5    14.5    28.9  \n 6 llll     MCMC_continuous  0.35  0.35 15.3    15.3    30.6  \n 7 ffff     MCMC_discrete    1     1     0.728   0.728   1.46 \n 8 fffl     MCMC_discrete    0.35  0.85 11.9     5.18   17.1  \n 9 flfl     MCMC_discrete    0.35  0.6  13.2     9.78   22.9  \n10 ffll     MCMC_discrete    0.5   0.5  13.2    13.2    26.5  \n11 flll     MCMC_discrete    0.4   0.4  14.4    14.4    28.8  \n12 llll     MCMC_discrete    0.35  0.35 15.3    15.3    30.6  \n13 ffff     enumerate        1     1     0.688   0.688   1.38 \n14 fffl     enumerate        0.35  0.85 11.9     5.18   17.1  \n15 flfl     enumerate        0.35  0.6  13.1     9.72   22.8  \n16 ffll     enumerate        0.5   0.5  13.2    13.2    26.5  \n17 flll     enumerate        0.4   0.4  14.4    14.4    28.8  \n18 llll     enumerate        0.35  0.35 15.3    15.3    30.6  \n# A tibble: 30 × 5\n   scenario variable MCMC_continuous MCMC_discrete enumerate\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 ffff     aE                0.9            1         1    \n 2 ffff     bE                0.9            1         1    \n 3 ffff     aU                0.0939         0.728     0.688\n 4 ffff     bU                0.0939         0.728     0.688\n 5 ffff     jointU            0.188          1.46      1.38 \n 6 fffl     aE                0.35           0.35      0.35 \n 7 fffl     bE                0.85           0.85      0.85 \n 8 fffl     aU               12.1           11.9      11.9  \n 9 fffl     bU                5.36           5.18      5.18 \n10 fffl     jointU           17.5           17.1      17.1  \n# ℹ 20 more rows\n\n\n\n\n\n\n\n\n\n\n\n\nstrength prior (1,10)\n\nmcmc with discrete prior (1,10)\n\nsuppressWarnings(rwebppl::kill_webppl())\n\nusing webppl version: main v0.9.15-27823b2 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\nMCMC_discrete_ffff &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = FAIL\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_fffl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_flfl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_ffll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_flll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nMCMC_discrete_llll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"MCMC\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = LIFT, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\n\nenumerate with discrete prior (1,10)\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_ffff &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = FAIL\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_fffl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_flfl &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = FAIL, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_ffll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = FAIL,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_flll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = FAIL, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\nsuppressWarnings(rwebppl::kill_webppl())\nenumerate_llll &lt;- rwebppl::webppl(program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\", data=data.frame(\n    method = \"enumerate\",\n    discrete = TRUE,\n    strengthPriorPrecision = 1,\n    strengthPriorVersion = \"(1,10)\",\n\n    a_r1result = LIFT, b_r1result = LIFT,\n    a_r2result = LIFT, b_r2result = LIFT\n), data_var=\"params_df\", random_seed=1)\n\n\n\nsummarize\n\ncompare_results()\n\n[1] \"Now compare visually\"\n# A tibble: 6 × 4\n  scenario MCMC_continuous MCMC_discrete enumerate\n  &lt;chr&gt;              &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n1 F,F;F,F                0             0         0\n2 F,F;F,L                1             1         1\n3 F,L;F,L                1             1         1\n4 F,F;L,L                1             1         1\n5 F,L;L,L                1             1         1\n6 L,L;L,L                1             1         1\n\n\n\n\n\n\n\n\n\n[1] \"Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"Compare all output values across methods\"\n[1] \"Extract all final_table values into a comprehensive summary\"\n# A tibble: 18 × 9\n   scenario method          startingE    aE    bE     aU     bU jointU     P\n   &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1 ffff     MCMC_continuous     0.188  0.15  0.15 -2.02  -2.02   -4.05     0\n 2 fffl     MCMC_continuous    17.5    0     1    13.9    0.375  14.2      1\n 3 flfl     MCMC_continuous    23.4    0.1   0.7  14.1    5.96   20.0      1\n 4 ffll     MCMC_continuous    26.5    1     0     0.375 13.9    14.2      1\n 5 flll     MCMC_continuous    28.9    1     0     0.375 13.9    14.2      1\n 6 llll     MCMC_continuous    30.6    0.6   0.1   7.52  14.3    21.8      1\n 7 ffff     MCMC_discrete       2.54   0     0     0      0       0        0\n 8 fffl     MCMC_discrete      18.0    0     1    13.9    0.375  14.2      1\n 9 flfl     MCMC_discrete      23.5    0.1   0.7  14.1    5.96   20.0      1\n10 ffll     MCMC_discrete      26.5    1     0     0.375 13.9    14.2      1\n11 flll     MCMC_discrete      28.8    1     0     0.375 13.9    14.2      1\n12 llll     MCMC_discrete      30.6    0.6   0.1   7.52  14.3    21.8      1\n13 ffff     enumerate           2.32   0     0     0      0       0        0\n14 fffl     enumerate          17.3    0     1    13.9    0.375  14.2      1\n15 flfl     enumerate          23.0    0.1   0.7  14.1    5.96   20.0      1\n16 ffll     enumerate          26.5    1     0     0.375 13.9    14.2      1\n17 flll     enumerate          28.8    1     0     0.375 13.9    14.2      1\n18 llll     enumerate          30.6    0.6   0.1   7.52  14.3    21.8      1\n# A tibble: 42 × 5\n   scenario variable  MCMC_continuous MCMC_discrete enumerate\n   &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 ffff     startingE           0.188          2.54      2.32\n 2 ffff     aE                  0.15           0         0   \n 3 ffff     bE                  0.15           0         0   \n 4 ffff     aU                 -2.02           0         0   \n 5 ffff     bU                 -2.02           0         0   \n 6 ffff     jointU             -4.05           0         0   \n 7 ffff     P                   0              0         0   \n 8 fffl     startingE          17.5           18.0      17.3 \n 9 fffl     aE                  0              0         0   \n10 fffl     bE                  1              1         1   \n# ℹ 32 more rows\n\n\n\n\n\n\n\n\n\n# A tibble: 18 × 7\n   scenario method             aE    bE      aU      bU jointU\n   &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 ffff     MCMC_continuous  0.9   0.9   0.0939  0.0939  0.188\n 2 fffl     MCMC_continuous  0.35  0.85 12.1     5.36   17.5  \n 3 flfl     MCMC_continuous  0.5   0.55 12.0    11.4    23.4  \n 4 ffll     MCMC_continuous  0.5   0.5  13.2    13.2    26.5  \n 5 flll     MCMC_continuous  0.4   0.4  14.5    14.5    28.9  \n 6 llll     MCMC_continuous  0.35  0.35 15.3    15.3    30.6  \n 7 ffff     MCMC_discrete    1     1     1.27    1.27    2.54 \n 8 fffl     MCMC_discrete    0.55  0.75 10.3     7.64   18.0  \n 9 flfl     MCMC_discrete    0.5   0.55 12.1    11.4    23.5  \n10 ffll     MCMC_discrete    0.5   0.5  13.2    13.2    26.5  \n11 flll     MCMC_discrete    0.4   0.4  14.4    14.4    28.8  \n12 llll     MCMC_discrete    0.35  0.35 15.3    15.3    30.6  \n13 ffff     enumerate        1     1     1.16    1.16    2.32 \n14 fffl     enumerate        0.35  0.85 12.0     5.27   17.3  \n15 flfl     enumerate        0.35  0.6  13.2     9.79   23.0  \n16 ffll     enumerate        0.5   0.5  13.2    13.2    26.5  \n17 flll     enumerate        0.4   0.4  14.4    14.4    28.8  \n18 llll     enumerate        0.35  0.35 15.3    15.3    30.6  \n# A tibble: 30 × 5\n   scenario variable MCMC_continuous MCMC_discrete enumerate\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 ffff     aE                0.9             1         1   \n 2 ffff     bE                0.9             1         1   \n 3 ffff     aU                0.0939          1.27      1.16\n 4 ffff     bU                0.0939          1.27      1.16\n 5 ffff     jointU            0.188           2.54      2.32\n 6 fffl     aE                0.35            0.55      0.35\n 7 fffl     bE                0.85            0.75      0.85\n 8 fffl     aU               12.1            10.3      12.0 \n 9 fffl     bU                5.36            7.64      5.27\n10 fffl     jointU           17.5            18.0      17.3 \n# ℹ 20 more rows\n\n\n\n\n\n\n\n\n\nOkay not sure why startingE can be 1 sometimes (excluding 1 and 10 does not fix this for MCMC_discrete). Inspecting using startingE_table"
  },
  {
    "objectID": "webppl vs memo/webppl.html",
    "href": "webppl vs memo/webppl.html",
    "title": "",
    "section": "",
    "text": "Cards\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nalice_chooses_card_model &lt;- '\nvar alice_chooses_card = function() {\n   var card = sample(Categorical({ vs: [1,2,3] }))\n   return card\n}\nvar dist = Infer(alice_chooses_card)\nexpectation(dist)\n'\nalice_chooses_card_E &lt;- rwebppl::webppl(alice_chooses_card_model)\n\nusing webppl version: main v0.9.15-c0b0d00 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\nshow(alice_chooses_card_E)\n\n[1] 2\n\n\n\n\nBasic conditioning\n\n\nlibrary(tidyverse)\nbasic_inference_model &lt;- '\nvar model = function() {\n   // uniform prior of true value, discretized for parity with memo\n   var value = randomInteger(101) / 100; // uniform(0, 1)\n\n   // someone has observed that value is &gt;.5\n   condition(value &gt; .5)\n   \n   // return the estimated probability of each possible value\n   return value\n}\nvar dist = Infer(model)\ndist\n\n// alternatively, output the expected value:\n// expectation(dist)\n'\nposterior &lt;- rwebppl::webppl(basic_inference_model) |&gt;\n   arrange(support)\n\n# extra: could compute E of posterior manually like this\n# posteriorE &lt;- sum(as.numeric(posterior$prob) * as.numeric(posterior$support))\n# posteriorE\n\nposterior |&gt;\n   # fill in missing support (HACK: need to round support values to ensure compatibility when calling `complete`)\n   mutate(support = round(support, 3)) |&gt;\n   complete(support = seq(0, 1, by = 0.01) |&gt; round(3), fill = list(prob = 0)) |&gt;\n   ggplot() + geom_line(aes(x=support, y=prob)) + lims(x=c(0,1), y=c(0, NA))"
  },
  {
    "objectID": "webppl vs memo/index.html#memo",
    "href": "webppl vs memo/index.html#memo",
    "title": "",
    "section": "memo",
    "text": "memo"
  },
  {
    "objectID": "webppl vs memo/memo.html",
    "href": "webppl vs memo/memo.html",
    "title": "Basic conditioning",
    "section": "",
    "text": "Cards\n\n# adapted from https://github.com/kach/memo/blob/main/demo/Memonomicon.ipynb\nimport jax\nimport jax.numpy as np\nfrom memo import memo\n\ncards = np.array([1, 2, 3])\n\n@memo(save_comic=\"memo-comic-card\")\ndef alice_chooses_card_E():\n    alice: chooses(c in cards, wpp=1)\n    return E[alice.c]\n\n# Comic representation of the modeling: ./memo-comic-card.png\n\n\nprint(alice_chooses_card_E())\n\n2.0\n\n\n\nfrom memo import memo\nimport jax.numpy as np\n\npossible_values = np.array(range(1,101)) / 100\n\n@memo(save_comic=\"memo-comic-bc\")\ndef model[query_v: possible_values]():\n    # establish prior in the observer's frame: that a value generator has some value from .01 to 1\n    value_gen: given(v in possible_values, wpp=1)\n    \n    # push an observation into a valuator's frame:\n    # first, establish the prior in the valuator's frame: have the valuator model that the value generator chose a value uniformly at random\n    valuator: thinks[ value_gen: given(v in possible_values, wpp=1) ]\n    # then, now that the value is modeled by the valuator, condition that value\n    valuator: observes_that [value_gen.v &gt; .5]\n\n    # return the estimated probability of the generator's value for being each queried v, *estimated according to the valuator/from the valuator's frame*\n    valuator: knows(query_v) # first, push query_v into the frame of the valuator\n    return valuator[Pr[value_gen.v == query_v]]\n\n    # or return the expected value in the valuator's frame:\n    # return valuator[E[value_gen.v]]\n\n# Comic representation of the modeling: ./memo-comic-bc.png\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(possible_values, model())"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "title": "Helper Functions (JAX-compatible)",
    "section": "",
    "text": "Code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R\nNOTE 10/20/25: next time I run the cached cells, I’ll need to remove the “webppl vs memo/” directory prefix, and add a code block that sets the working directory appropriately for interactive execution"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Setup",
    "text": "Setup\n\nimport jax\nimport jax.numpy as np\nfrom memo import memo\nfrom matplotlib import pyplot as plt\n\n# Model parameters (from original WebPPL code)\nalpha = 13.5  # effort cost coefficient\nbeta = 24.5   # inequality aversion coefficient\nweight_box = 5  # weight of the box\nlow_reward = 10\nhigh_reward = 20\n\n# Discretized spaces (for computational tractability)\nefforts = np.array([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n                    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])\n\n# Discretized strength space (replacing continuous uniform(1,10))\n# (1,10) with digits=1\nstrength_values = np.linspace(1.1, 9.9, 89)\n# # [1,10] with digits=1\n# strength_values = np.linspace(1., 10., 91)\n\n\n@jax.jit\ndef lift(strength, effort):\n    \"\"\"Single agent lift: can they lift the box?\"\"\"\n    return (effort * strength &gt;= weight_box).astype(float)\n\n@jax.jit\ndef lift2(strength1, strength2, effort1, effort2):\n    \"\"\"Two-agent joint lift: can they lift the box together?\"\"\"\n    return (effort1 * strength1 + effort2 * strength2 &gt;= weight_box).astype(float)\n\n@jax.jit\ndef gini(effort1, effort2):\n    \"\"\"Gini coefficient for inequality aversion\"\"\"\n    return np.where(\n        effort1 == effort2,\n        0.0,\n        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)\n    )\n\n@jax.jit\ndef argmax_utility(strength, reward, effort_space=efforts):\n    \"\"\"Find effort that maximizes utility for single agent\"\"\"\n    utilities = reward * lift(strength, effort_space) - alpha * effort_space\n    return effort_space[np.argmax(utilities)]\n\n@jax.jit\ndef outcome(strength, reward):\n    \"\"\"Predicted outcome (success probability) for single agent at optimal effort\"\"\"\n    opt_effort = argmax_utility(strength, reward)\n    return lift(strength, opt_effort)\n\n# DEBUG: Test helper functions with sample values\nprint(\"=== HELPER FUNCTION TESTS ===\")\ntest_strength = 5.0\ntest_effort = 1.0\ntest_lift_result = lift(test_strength, test_effort)\nprint(f\"lift(strength=5.0, effort=1.0): {test_lift_result}\")\n\ntest_optE_low = argmax_utility(test_strength, low_reward)\nprint(f\"optE(strength=5.0, reward=10): {test_optE_low}\")\n\ntest_optE_high = argmax_utility(test_strength, high_reward)\nprint(f\"optE(strength=5.0, reward=20): {test_optE_high}\")\n\ntest_outcome_low = outcome(test_strength, low_reward)\nprint(f\"outcome(strength=5.0, reward=10): {test_outcome_low}\")\n\ntest_outcome_high = outcome(test_strength, high_reward)\nprint(f\"outcome(strength=5.0, reward=20): {test_outcome_high}\")\n\n=== HELPER FUNCTION TESTS ===\nlift(strength=5.0, effort=1.0): 1.0\noptE(strength=5.0, reward=10): 0.0\noptE(strength=5.0, reward=20): 1.0\noutcome(strength=5.0, reward=10): 0.0\noutcome(strength=5.0, reward=20): 1.0"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Bayesian Inference Over Agent Strengths",
    "text": "Bayesian Inference Over Agent Strengths\nThe original WebPPL code infers agent strengths from observed outcomes in rounds 1 and 2.\n\n# Helper function for the joint condition (needed because & operator not supported in memo)\n@jax.jit\ndef both_true(cond1, cond2):\n    \"\"\"Returns True if both conditions are True (for use in joint probability queries)\"\"\"\n    return cond1 & cond2\n\n@memo(save_comic=\"memo-comic-xiang-strength-inference\")\ndef infer_strengths[query_sa: strength_values, query_sb: strength_values](\n    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b\n):\n    \"\"\"\n    Infer posterior distribution over agent strengths given observed outcomes.\n\n    Observations:\n    - Round 1 (low reward = 10): A and B each attempt individual lifts\n    - Round 2 (high reward = 20): A and B each attempt individual lifts\n\n    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]\n    \"\"\"\n    # Prior: Agent A has some strength uniformly distributed in [1, 10]\n    agent_a: given(sa in strength_values, wpp=1)\n\n    # Prior: Agent B has some strength uniformly distributed in [1, 10]\n    agent_b: given(sb in strength_values, wpp=1)\n\n    # The participant models that the agents have some strength\n    participant: thinks[\n        agent_a: given(sa in strength_values, wpp=1),\n        agent_b: given(sb in strength_values, wpp=1)\n    ]\n\n    # Participant witnesses the outcomes and conditions on them\n    # Condition on round 1 outcomes (low reward)\n    participant: observes_that [outcome(agent_a.sa, {low_reward}) == r1_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {low_reward}) == r1_outcome_b]\n\n    # Condition on round 2 outcomes (high reward)\n    participant: observes_that [outcome(agent_a.sa, {high_reward}) == r2_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {high_reward}) == r2_outcome_b]\n\n    # Push query variables into participant frame \n    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths\n    participant: knows(query_sa)\n    participant: knows(query_sb)\n    return participant[Pr[both_true(agent_a.sa == query_sa, agent_b.sb == query_sb)]]\n    # (Using helper function for bitwise-and since & operator not supported in memo DSL)\n\n# Example: F,F;F,F scenario (both agents fail in both rounds)\n# This would be computationally expensive with many strength values\n# For testing, could use a coarser grid\nposterior_strengths_ffff = infer_strengths(0.0, 0.0, 0.0, 0.0)\n\n# DEBUG: Bayesian inference statistics\nprint(\"\\n=== BAYESIAN INFERENCE STATISTICS ===\")\nposterior_2d = posterior_strengths_ffff.reshape(len(strength_values), len(strength_values))\nprint(f\"Posterior shape: {posterior_2d.shape}\")\nprint(f\"Total probability mass: {np.sum(posterior_2d):.6f}\")\n\n# Compute marginals\nmarginal_a = np.sum(posterior_2d, axis=1)\nmarginal_b = np.sum(posterior_2d, axis=0)\n\n# # Compute statistics\n# mean_sa = np.sum(marginal_a * strength_values)\n# mean_sb = np.sum(marginal_b * strength_values)\n# print(f\"Agent A strength - mean: {mean_sa:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n# print(f\"Agent B strength - mean: {mean_sb:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n\n# # Find high-probability regions\n# high_prob_threshold = np.max(posterior_2d) * 0.1\n# high_prob_indices = np.where(posterior_2d &gt; high_prob_threshold)\n# print(f\"High probability region (top 10%): strength_a in [{strength_values[np.min(high_prob_indices[0])]:.2f}, {strength_values[np.max(high_prob_indices[0])]:.2f}]\")\n# print(f\"                                     strength_b in [{strength_values[np.min(high_prob_indices[1])]:.2f}, {strength_values[np.max(high_prob_indices[1])]:.2f}]\")\n\n# # For F,F;F,F scenario, agents fail at BOTH low_reward=10 and high_reward=20\n# # This means they cannot lift box even with high incentive\n# # Let's check what strengths are actually consistent with failing both rounds\n# print(\"\\n=== FIXME #1: Investigating strength bounds ===\")\n# print(\"For F,F;F,F: agents fail at reward=10 AND reward=20\")\n# print(f\"Box weight: {weight_box}\")\n# print(f\"Alpha (effort cost): {alpha}\")\n\n# # Check a few sample strengths to see if they would fail\n# test_strengths = [3.0, 4.0, 4.5, 4.9, 5.0, 5.5, 6.0, 7.0]\n# for s in test_strengths:\n#     outcome_low = outcome(s, low_reward)\n#     outcome_high = outcome(s, high_reward)\n#     opt_e_low = argmax_utility(s, low_reward)\n#     opt_e_high = argmax_utility(s, high_reward)\n#     print(f\"  strength={s:.1f}: outcome(R=10)={outcome_low:.0f} (effort={opt_e_low:.2f}), outcome(R=20)={outcome_high:.0f} (effort={opt_e_high:.2f})\")\n\n# Find the actual support of the posterior (non-zero probability mass)\nposterior_support = posterior_2d &gt; 1e-10\nsupport_indices = np.where(posterior_support)\nif len(support_indices[0]) &gt; 0:\n    print(f\"Non-negligible posterior support:\")\n    print(f\"  strength_a in [{strength_values[np.min(support_indices[0])]:.2f}, {strength_values[np.max(support_indices[0])]:.2f}]\")\n    print(f\"  strength_b in [{strength_values[np.min(support_indices[1])]:.2f}, {strength_values[np.max(support_indices[1])]:.2f}]\")\nelse:\n    print(\"\\nWARNING: Posterior has no support! Inference may have failed.\")\n\n\n=== BAYESIAN INFERENCE STATISTICS ===\nPosterior shape: (89, 89)\nTotal probability mass: 0.999996\nNon-negligible posterior support:\n  strength_a in [1.10, 4.90]\n  strength_b in [1.10, 4.90]"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-effort-model",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-effort-model",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Game-Theoretic Joint Effort Model",
    "text": "Game-Theoretic Joint Effort Model\nThis is the most complex part. The original WebPPL code models: 1. Two agents with uncertain strengths (posterior distributions from rounds 1-2) 2. Agents reason about each other’s efforts in round 3 (joint task) 3. Iterative best-response until Nash equilibrium 4. Optimization over initial effort to maximize joint utility\nKey insight: The game-theoretic reasoning operates on expected utilities over the posterior distribution of strengths. This is not purely probabilistic reasoning (like the inference above), but rather decision-theoretic optimization under uncertainty.\nProposed approach: - Keep the Bayesian inference in @memo (already done above) - Implement the game-theoretic equilibrium finding in pure JAX (deterministic optimization) - Combine them: compute equilibrium for each strength pair, weighted by posterior\n\n@jax.jit\ndef joint_utility_single_strength(strength_a, strength_b, effort_a, effort_b):\n    \"\"\"\n    Compute utilities for both agents given fixed strengths and efforts.\n\n    Returns: (utility_a, utility_b, success_prob)\n    \"\"\"\n    reward = high_reward  # Round 3 uses high reward\n    success = lift2(strength_a, strength_b, effort_a, effort_b)\n    gini_coef = gini(effort_a, effort_b)\n\n    utility_a = reward * success - alpha * effort_a - beta * gini_coef\n    utility_b = reward * success - alpha * effort_b - beta * gini_coef\n\n    return utility_a, utility_b, success\n\n@jax.jit\ndef best_response_a(strength_a, strength_b, effort_b):\n    \"\"\"\n    Find agent A's best response to agent B's effort.\n\n    Agent A optimizes: max_{e_a} E[utility_a(e_a, e_b) | s_a, s_b]\n    \"\"\"\n    # If strength_a and strength_b are distributions, we'd need to take expectations\n    # For now, assume they are point estimates\n    # TODO: I think if they are arrays right now, they're silently being operated on item-wise. is this sufficient, or do we need to take expectation? \n    utilities = jax.vmap(\n        lambda ea: joint_utility_single_strength(strength_a, strength_b, ea, effort_b)[0]\n    )(efforts)\n\n    return efforts[np.argmax(utilities)]\n\n@jax.jit\ndef best_response_b(strength_a, strength_b, effort_a):\n    \"\"\"\n    Find agent B's best response to agent A's effort.\n    \"\"\"\n    utilities = jax.vmap(\n        lambda eb: joint_utility_single_strength(strength_a, strength_b, effort_a, eb)[1]\n    )(efforts)\n\n    return efforts[np.argmax(utilities)]\n\ndef find_equilibrium(strength_a, strength_b, init_effort, max_depth=10, verbose=False):\n    \"\"\"\n    Find Nash equilibrium through iterated best responses.\n\n    Starting from init_effort for agent B, iterate:\n    - A responds to B\n    - B responds to A\n    Until convergence (or max_depth reached)\n\n    Note: depth starts at 1 to match WebPPL convention (not 0-indexed)\n    \"\"\"\n    effort_b = init_effort\n\n    # Start depth at 1 to match WebPPL convention\n    for depth in range(1, max_depth + 1):\n        effort_a = best_response_a(strength_a, strength_b, effort_b)\n        effort_b_new = best_response_b(strength_a, strength_b, effort_a)\n\n        # Check convergence\n        if np.abs(effort_b_new - effort_b) &lt; 0.06:\n            if verbose:\n                print(f\"  Converged at depth {depth}: effort_a={effort_a:.4f}, effort_b={effort_b_new:.4f}\")\n            return effort_a, effort_b_new, depth\n\n        effort_b = effort_b_new\n\n    print(f\"Warning: Equilibrium did not converge in {max_depth} iterations\")\n    return effort_a, effort_b, max_depth\n\n# To find the optimal initial effort, we'd search over init_effort values\ndef find_optimal_init_effort(posterior_sa_sb):\n    \"\"\"\n    Find initial effort that maximizes expected joint utility.\n\n    This matches the WebPPL startingEffort() function which optimizes jointU.\n    \"\"\"\n    joint_utilities = []\n    for init_effort in efforts:\n        stats = compute_expected_statistics(posterior_sa_sb, init_effort)\n        joint_utilities.append(stats['joint_utility'])  # Optimize joint utility (matches WebPPL)\n    \n    joint_utilities = np.array(joint_utilities)\n    return efforts[np.argmax(joint_utilities)]\n\ndef compute_expected_statistics(posterior_sa_sb, init_effort):\n    \"\"\"\n    Compute expected utilities and efforts over posterior distribution of strengths.\n\n    This matches the WebPPL jointUtility() function output, computing expectations over\n    the posterior distribution of agent strengths.\n\n    Args:\n        posterior_sa_sb: 2D array of posterior probabilities P(sa, sb | observations)\n                        Shape: (len(strength_values), len(strength_values))\n        init_effort: Initial effort for agent B to start equilibrium search\n\n    Returns:\n        dict with keys: joint_utility, agent_a_utility, agent_b_utility,\n                       agent_a_effort, agent_b_effort, outcome_prob\n    \"\"\"\n    expected_joint_utility = 0.0\n    expected_agent_a_utility = 0.0\n    expected_agent_b_utility = 0.0\n    expected_agent_a_effort = 0.0\n    expected_agent_b_effort = 0.0\n    expected_outcome = 0.0\n\n    # Iterate over all strength combinations\n    for i, sa in enumerate(strength_values):\n        for j, sb in enumerate(strength_values):\n            prob_sa_sb = posterior_sa_sb[i, j]\n\n            if prob_sa_sb &gt; 1e-10:  # Only compute for non-negligible probabilities\n                # Find equilibrium efforts for this strength pair\n                effort_a, effort_b, _ = find_equilibrium(sa, sb, init_effort)\n\n                # Compute utilities and outcome at equilibrium\n                utility_a, utility_b, success_prob = joint_utility_single_strength(sa, sb, effort_a, effort_b)\n\n                # Weight by posterior probability and accumulate\n                expected_agent_a_utility += prob_sa_sb * utility_a\n                expected_agent_b_utility += prob_sa_sb * utility_b\n                expected_joint_utility += prob_sa_sb * (utility_a + utility_b)\n                expected_agent_a_effort += prob_sa_sb * effort_a\n                expected_agent_b_effort += prob_sa_sb * effort_b\n                expected_outcome += prob_sa_sb * success_prob\n\n    return {\n        'joint_utility': expected_joint_utility,\n        'agent_a_utility': expected_agent_a_utility,\n        'agent_b_utility': expected_agent_b_utility,\n        'agent_a_effort': expected_agent_a_effort,\n        'agent_b_effort': expected_agent_b_effort,\n        'outcome_prob': expected_outcome\n    }"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-ffff-scenario",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-ffff-scenario",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Running the Model for F,F;F,F Scenario",
    "text": "Running the Model for F,F;F,F Scenario\n\n# Scenario: Both agents fail in both rounds (F,F;F,F)\n# This means weak agents who cannot lift the box individually even with high reward\n\n# Get posterior over strengths\nprint(\"\\nComputing posterior over agent strengths...\")\nposterior_ffff = posterior_strengths_ffff.reshape(len(strength_values), len(strength_values))\n\n# Test equilibrium finding with a sample strength pair\nprint(\"\\n=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===\")\n# Use the modal strength values as a test case\nmodal_idx_a = np.argmax(np.sum(posterior_ffff, axis=1))\nmodal_idx_b = np.argmax(np.sum(posterior_ffff, axis=0))\nsample_sa = strength_values[modal_idx_a]\nsample_sb = strength_values[modal_idx_b]\nprint(f\"Testing with strength_a={sample_sa:.2f}, strength_b={sample_sb:.2f}\")\neffort_a_test, effort_b_test, depth_test = find_equilibrium(sample_sa, sample_sb, 0.0, verbose=True)\nprint(f\"Convergence depth: {depth_test}\")\nprint(f\"Agent A equilibrium effort: {effort_a_test:.4f}\")\nprint(f\"Agent B equilibrium effort: {effort_b_test:.4f}\")\n\n# Find optimal initial effort (by searching over effort space)\nprint(\"\\n=== FINDING OPTIMAL INITIAL EFFORT ===\")\n# For now, just use a default initial effort\noptimal_init_effort = find_optimal_init_effort(posterior_ffff)\nprint(f\"Using initial effort: {optimal_init_effort}\")\n\n# Compute expected statistics at equilibrium (utilities, efforts, outcome)\nprint(\"\\nComputing expected statistics at equilibrium...\")\nstats = compute_expected_statistics(posterior_ffff, optimal_init_effort)\n\nprint(\"\\n=== FINAL RESULTS ===\")\nprint(f\"Optimal starting effort: {optimal_init_effort}\")\nprint(f\"Joint utility at optimum: {stats['joint_utility']:.6f}\")\nprint(f\"Agent A utility: {stats['agent_a_utility']:.6f}\")\nprint(f\"Agent B utility: {stats['agent_b_utility']:.6f}\")\nprint(f\"Agent A equilibrium effort: {stats['agent_a_effort']:.6f}\")\nprint(f\"Agent B equilibrium effort: {stats['agent_b_effort']:.6f}\")\nprint(f\"Expected outcome probability: {stats['outcome_prob']:.6f}\")\nprint(f\"\\n(Compare to original WebPPL output)\")\n\n# Sanity check: joint utility should equal sum of individual utilities\njoint_check = stats['agent_a_utility'] + stats['agent_b_utility']\nif abs(joint_check - stats['joint_utility']) &gt; 1e-5:\n    print(f\"WARNING: Joint utility mismatch! {stats['joint_utility']:.6f} != {joint_check:.6f}\")\n\n\nComputing posterior over agent strengths...\n\n=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===\nTesting with strength_a=1.10, strength_b=1.10\n  Converged at depth 1: effort_a=0.0000, effort_b=0.0000\nConvergence depth: 1\nAgent A equilibrium effort: 0.0000\nAgent B equilibrium effort: 0.0000\n\n=== FINDING OPTIMAL INITIAL EFFORT ===\nUsing initial effort: 0.8999999761581421\n\nComputing expected statistics at equilibrium...\n\n=== FINAL RESULTS ===\nOptimal starting effort: 0.8999999761581421\nJoint utility at optimum: 11.368189\nAgent A utility: 7.008577\nAgent B utility: 4.359619\nAgent A equilibrium effort: 0.399570\nAgent B equilibrium effort: 0.595792\nExpected outcome probability: 0.669291\n\n(Compare to original WebPPL output)\n\n\n\nVisualization\n\n# Visualize posterior over agent strengths\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Posterior heatmap\nim = axes[0].imshow(posterior_ffff, origin='lower', extent=[1, 10, 1, 10], aspect='auto')\naxes[0].set_xlabel('Agent B Strength')\naxes[0].set_ylabel('Agent A Strength')\naxes[0].set_title('Posterior P(strength_A, strength_B | F,F;F,F)')\nplt.colorbar(im, ax=axes[0], label='Probability')\n\n# Marginal distributions\nmarginal_a = np.sum(posterior_ffff, axis=1)\nmarginal_b = np.sum(posterior_ffff, axis=0)\n\naxes[1].plot(strength_values, marginal_a, label='Agent A', linewidth=2)\naxes[1].plot(strength_values, marginal_b, label='Agent B', linewidth=2, linestyle='--')\naxes[1].set_xlabel('Strength')\naxes[1].set_ylabel('Probability')\naxes[1].set_title('Marginal Distributions')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "title": "Helper Functions (JAX-compatible)",
    "section": "Notes on Conversion Strategy",
    "text": "Notes on Conversion Strategy\nThe conversion from WebPPL to memo revealed important distinctions:\n\n1. Probabilistic Inference vs. Decision Theory\nThe original WebPPL code mixes two types of reasoning: - Bayesian inference (rounds 1-2): Infer agent strengths from observed outcomes - Game-theoretic optimization (round 3): Find Nash equilibrium efforts\nIn the memo conversion: - Bayesian inference: Use @memo with given(), observes_that[], and thinks[] - Game-theoretic optimization: Use pure JAX functions (no probabilistic reasoning, just optimization)\n\n\n2. Key Conversion Decisions\nMutable arrays (x2a, x2b): - WebPPL: Pushes strength posterior samples to arrays, uses them later - memo: Query the posterior distribution directly (more principled)\nNested agent reasoning: - WebPPL: Nested JavaScript functions with recursive best-response - memo/JAX: Iterative best-response in pure JAX (not probabilistic, so no thinks[] needed here)\nExpected utilities: - WebPPL: Implicitly handles through sampling or enumeration - memo/JAX: Explicit loop over posterior, computing equilibrium for each strength pair\n\n\n3. Computational Considerations\nThe original uses either: - MCMC with 10,000 samples (slow but handles continuous distributions) - Enumeration with discrete values (exact but potentially more expensive)\n\n\n4. What Could Be Improved\nPotential optimizations (if using pure game-theoretic optimization): - Use coarser grid for exploration, then refine around high-probability regions - Vectorize the equilibrium finding across strength pairs (currently sequential) - Use automatic differentiation to find equilibrium (gradient-based instead of grid search)\nFurther extension with memo: Currently, the game-theoretic reasoning is outside of @memo. One could potentially: - Model agents’ beliefs about each other’s strengths using nested thinks[] - Model agents’ choices as chooses(e in efforts, wpp=utility(e)) - But this might be overkill and harder to reason about - However, this does incorporate uncertainty about the other agent into the model, which might add richness"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "title": "Helper Functions (JAX-compatible)",
    "section": "How to Compare Outputs",
    "text": "How to Compare Outputs\nTo verify that the WebPPL and memo implementations produce the same results:\nThese should match (up to numerical precision and discretization choices).\nCompare outcome probability (here, expected_outcome_ffff), strength posteriors, and joint utility results for r3 outcome and for starting effort.\n\nExpected matches:\n\nHelper functions (lift, optE, outcome) should return identical values for test inputs\nBayesian inference statistics should show similar:\n\nPosterior means (agent strengths)\nNon-neglible-probability regions\n\nEquilibrium finding should show similar:\n\nConvergence depth (may vary slightly due to discretization)\nEquilibrium efforts\n\nFinal outcome probability should be close\n\nKnown differences:\n\nSee above\nSmall numerical differences due to different underlying computation engines"
  }
]