[
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html",
    "href": "webppl vs memo/xiang2023-exp1-round3.html",
    "title": "Xiang2023 Exp1 Round3 Modeling in WebPPL",
    "section": "",
    "text": "code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R\nNOTE 10/20/25: next time I run the cached cells, I’ll need to remove the “webppl vs memo/” directory prefix, and add a code block that sets the working directory appropriately for interactive execution\nlibrary(tidyverse)\nlibrary(rwebppl)\n\njoint &lt;- data.frame(model = 'joint',\n                    agent = rep(c('A','B'), 18), \n                    scenario = c(rep('F,F;F,F',6),rep('F,F;F,L',6),rep('F,L;F,L',6),rep('F,F;L,L',6),rep('F,L;L,L',6),rep('L,L;L,L',6)),\n                    effort = 0, strength = 0,\n                    outcome = 0, prob = 0, round = rep(c(1,1,2,2,3,3), 6), \n                    reward = rep(c(10,10,20,20,20,20), 6))\njoint$prob[joint$round==1] = NaN\njoint$outcome = c(integer(6), integer(3),1,integer(2), 0,1,0,1,1,1, integer(2),integer(4)+1, 0,integer(5)+1, integer(6)+1)"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#mcmc",
    "title": "Xiang2023 Exp1 Round3 Modeling in WebPPL",
    "section": "1 - mcmc",
    "text": "1 - mcmc\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\" # footnote: For computational tractability, we constrained the effort space to discrete values, ranging from 0 to 1 with increments of 0.05.\n\n## R3 Probability\nmdl1 &lt;- \"\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'MCMC', kernel: 'MH', samples: 10000, burn: 1000, model() {\n  var sa = uniform(1,10)\n  var sb = uniform(1,10)\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      // display(tbl.jointU)\n      return tbl.jointU\n    },\n    efforts);\n};\n\nvar startingE = startingEffort(x2a,x2b)\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\n# mdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\n# a &lt;- rwebppl::webppl(mdl)\n# joint$prob[joint$round==3 & joint$scenario=='F,F;F,F'] &lt;- a$P*100\n# joint\n\n\noriginal_ffff &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5) |&gt; rwebppl::webppl()\noriginal_fffl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_flfl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"true\", mdl3, \"false\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_ffll &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_flll &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"true\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\noriginal_llll &lt;- paste0(effort_space_joint, mdl1, \"true\", mdl2, \"true\", mdl3, \"true\", mdl4, \"true\", mdl5) |&gt; rwebppl::webppl()\n\n\n# plot\nbind_rows(\n  tibble(scenario='F,F;F,F', P=original_ffff$P),\n  tibble(scenario='F,F;F,L', P=original_fffl$P),\n  tibble(scenario='F,L;F,L', P=original_flfl$P),\n  tibble(scenario='F,F;L,L', P=original_ffll$P),\n  tibble(scenario='F,L;L,L', P=original_flll$P),\n  tibble(scenario='L,L;L,L', P=original_llll$P)\n) |&gt;\n  ggplot(aes(x=scenario, y=P*100)) +\n  geom_col() +\n  labs(title='Round 3 Success Probability by Scenario (MCMC, ORIGINAL CODE)', y='Probability (%)', x='Scenario') +\n  theme_minimal()"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#enumeration",
    "title": "Xiang2023 Exp1 Round3 Modeling in WebPPL",
    "section": "2 - enumeration",
    "text": "2 - enumeration\nnow trying it using enumeration (using a discretized strength space), as a step towards translating into memo:\n\neffort_space_joint &lt;- \"var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\"\n\n## R3 Probability\nmdl1 &lt;- \"\n// convert to a @jax.jit?\nvar argMax = function(f, ar){\n  return maxWith(f, ar)[0]\n};\n\nvar alpha = 13.5, beta = 24.5\nvar weight = mem(function (box) {return 5})\nvar lowR = 10\nvar highR = 20\n\n// convert to a @jax.jit\nvar lift = function(strength,box,effort){\n  return (effort*strength &gt;= weight(box))\n}\n\n// convert to a @jax.jit\nvar optE = function(strength,box,reward) {\n  return argMax(\n    function(effort) {\n      if (strength.length &gt; 1)\n        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}\n        else \n          {return reward*lift(strength,box,effort) - alpha*effort}\n    },\n    efforts);\n};\n\n// convert to a @jax.jit\nvar outcome = function(strength,box,reward) {\n  if (strength.length &gt; 1)\n    { var opt_effort = optE(strength,box,reward)\n      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}\n  else \n  {return lift(strength,box,optE(strength,box,reward))}\n}\n\n// convert to a @memo, and instead of pushing x2a and x2b to a global variable, bring in the computations which use them into the @memo ?\nvar x2a = [], x2b = []\nvar samples2 = Infer({ method: 'enumerate', model() {\n  var sa = (11+randomInteger(89))/10 // not in a general form, but computes values in (1, 10) instead of in [1, 10] to see if that fixes things\n  var sb = (11+randomInteger(89))/10\n  condition(outcome(sa,'box',lowR) == \"\nmdl2 &lt;- \")\n  condition(outcome(sb,'box',lowR) == \"\nmdl3 &lt;- \")\n  condition(outcome(sa,'box',highR) == \"\nmdl4 &lt;- \")\n  condition(outcome(sb,'box',highR) == \"\nmdl5 &lt;- \")\n  x2a.push(sa)\n  x2b.push(sb)\n  return 0\n}})\n\n// convert to a combination of @jax.jit and @memo ? so I'm not doing two-step nested optimization (like the difference between RSA in webppl and in memo)\nvar jointUtility = function(init_effort,a_strength,b_strength){\n  var r3_reward = highR // round 3 reward\n  \n  var lift2 = function(strength,strength2,box,effort,effort2){\n    return (effort*strength + effort2*strength2) &gt;= weight(box)\n  }\n\n  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}\n  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.\n  \n  var a = function(depth,reward) {\n      var effort2 = b(depth - 1,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(a_strength,b_strength,'box',reward)\n  }\n  \n  var b = function(depth,reward) {\n      var effort2 = depth===0 ? init_effort : a(depth,reward)\n      var optEffort = function(strength,strength2,box,reward) {\n        return argMax(\n          function(effort) {\n            if (a_strength.length &gt; 1) {\n              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)\n            } else {\n              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)\n            }\n          },\n          efforts);\n      };\n    return optEffort(b_strength,a_strength,'box',reward)\n  }\n  \n  var findDepth = function(x) { // find the depth that is needed to converge\n     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) &lt; 0.06) {\n       return x;\n     } else {\n       return -1;\n     }\n   };\n\n  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.\n  var d = function() {\n     if (findDepth(ds[0]) &gt; 0) {\n       return ds[0]\n     } else if (findDepth(ds[1]) &gt; 0) {\n       return ds[1]\n     } else if (findDepth(ds[2]) &gt; 0) {\n       return ds[2]\n     } else if (findDepth(ds[3]) &gt; 0) {\n       return ds[3]\n     } else {\n       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')\n     }\n   };\n\n  var depth = d()\n  var aE = a(depth+1,r3_reward)\n  var bE = b(depth,r3_reward)\n  \n  var outcome2 = function(a_strength,b_strength,box) {\n    if (a_strength.length &gt; 1) { \n      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))\n    } else {\n      return lift2(a_strength,b_strength,box,aE,bE)\n    }\n  }\n    \n  // calculate agents' utility\n  if (a_strength.length &gt; 1) {\n    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  } else {\n    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)\n    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)\n    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};\n    return table\n  }\n}\n\n// convert to a @jax.jit\n// find the intial effort that maximizes the joint utility\nvar startingEffort = function(a_strength,b_strength) {\n  return argMax(\n    function(init_effort) {\n      var tbl = jointUtility(init_effort,a_strength,b_strength)\n      return tbl.jointU\n    },\n    efforts);\n};\n\n\nvar startingE = startingEffort(x2a,x2b)\n// display(jointUtility(startingE,x2a,x2b))\nvar output = {P: jointUtility(startingE,x2a,x2b).outcome}\noutput\n\"\n\n# F,F;F,F\nmdl &lt;- paste0(effort_space_joint, mdl1, \"false\", mdl2, \"false\", mdl3, \"false\", mdl4, \"false\", mdl5)\nwebppl(mdl)\n\n\n\nnote that their model is assuming that 1 and 10 are lower probability, which is just a computational limitation! when enumerating, the resulting p is infinitesimally small but matches all others; when using MCMC, the bounds are underweighted.\n\n-&gt; using something like (11+randomInteger(89))/10 to simulate sampling from (1,10) rather than [1,10]; inspired by the observation that webppl’s uniform distribution is actually more like this, since there are only finite samples\nmaybe just need to exclude 1 from the range and not 10? -&gt; trying different versions of the discretized distribution"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "href": "webppl vs memo/xiang2023-exp1-round3.html#comparison-across-scenarios",
    "title": "Xiang2023 Exp1 Round3 Modeling in WebPPL",
    "section": "3 - comparison across scenarios",
    "text": "3 - comparison across scenarios\nNow moved into xiang2023-exp1-round3-with-debugging.wppl, which has input parameters and logs.\nWalking step by step and validate that the values (posteriors and equilibria across all 6 scenarios) match:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nFAIL &lt;- FALSE\nLIFT &lt;- TRUE\n\n# Define the 6 scenarios\nscenarios &lt;- tribble(\n  ~name,      ~a_r1result, ~b_r1result, ~a_r2result, ~b_r2result,\n  \"ffff\",     FAIL,        FAIL,        FAIL,        FAIL,\n  \"fffl\",     FAIL,        FAIL,        FAIL,        LIFT,\n  \"flfl\",     FAIL,        LIFT,        FAIL,        LIFT,\n  \"ffll\",     FAIL,        FAIL,        LIFT,        LIFT,\n  \"flll\",     FAIL,        LIFT,        LIFT,        LIFT,\n  \"llll\",     LIFT,        LIFT,        LIFT,        LIFT\n)\n\n# Helper function to run a single scenario\nrun_scenario &lt;- function(scenario_name, method, discrete = FALSE,\n                        strengthPriorPrecision = NULL,\n                        strengthPriorVersion = NULL) {\n  scenario &lt;- scenarios %&gt;% filter(name == scenario_name)\n\n  params &lt;- data.frame(\n    method = method,\n    discrete = discrete,\n    a_r1result = scenario$a_r1result,\n    b_r1result = scenario$b_r1result,\n    a_r2result = scenario$a_r2result,\n    b_r2result = scenario$b_r2result\n  )\n\n  if (!is.null(strengthPriorPrecision)) {\n    params$strengthPriorPrecision &lt;- strengthPriorPrecision\n  }\n  if (!is.null(strengthPriorVersion)) {\n    params$strengthPriorVersion &lt;- strengthPriorVersion\n  }\n\n  suppressWarnings(rwebppl::kill_webppl())\n  if (interactive()) {\n    message(sprintf(\"Running %s for scenario %s\", method, scenario_name))\n  }\n  rwebppl::webppl(\n    program_file = \"webppl vs memo/xiang2023-exp1-round3-with-debugging.wppl\",\n    data = params,\n    data_var = \"params_df\",\n    random_seed = 1\n  )\n}\n\n# Helper function to run all scenarios for a given method configuration\nrun_all_scenarios &lt;- function(...) {\n  results &lt;- list()\n  for (scenario_name in scenarios$name) {\n    results[[scenario_name]] &lt;- run_scenario(scenario_name, ...)\n  }\n  results_tidy &lt;- results |&gt;\n    enframe() |&gt;\n    unnest_wider(value) |&gt;\n    unnest_wider(final_table) |&gt;\n    rename(scenario = name)\n  results_tidy\n}\n\ncompare_results &lt;- function(...) {\n  # Accepts data frames as inputs\n  results_list &lt;- list(...)\n  if (length(results_list) == 0) {\n    stop(\"At least one set of results must be provided\")\n  }\n\n  # Build summaries\n  results_combined &lt;- bind_rows(results_list, .id=\"method\")\n  P_results_summary &lt;- results_combined |&gt; select(method, scenario, P)\n  posteriors_summary &lt;- results_combined |&gt; select(method, scenario, aStrength_posterior, bStrength_posterior)\n\n  message(\"Now compare visually\")\n\n  show(P_results_summary)\n\n  # Make posteriors_summary longer for plotting (cols: method, agent, scenario, strength)\n  posteriors_long &lt;- posteriors_summary |&gt;\n    pivot_longer(cols = c(aStrength_posterior, bStrength_posterior),\n                  names_to = \"agent\",\n                  values_to = \"strength\") |&gt;\n    mutate(agent = ifelse(agent == \"aStrength_posterior\", \"a\", \"b\")) |&gt;\n    unnest_longer(strength)\n\n  # Plot density of posteriors for each scenario\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_wrap(~scenario, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  message(\"Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\")\n\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_grid(method~scenario, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  show(\n    ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +\n      geom_density() +\n      facet_grid(method~scenario+agent, scales = \"free_y\") +\n      labs(\n        title = \"Strength Posteriors Across Scenarios\",\n        x = \"Strength\",\n        y = \"Density\",\n        color = \"Method\",\n        linetype = \"Agent\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"bottom\")\n  )\n\n  message(\"Compare all output values across methods\")\n\n  show(results_combined |&gt; select(-aStrength_posterior, -bStrength_posterior, -startingE_table))\n\n  message(\"Extract all final_table values into a comprehensive summary\")\n\n  # Plot comparisons for the rest of the variables\n  results_combined_vars_long &lt;- results_combined |&gt;\n    select(-where(is.list)) |&gt;\n    pivot_longer(-c(method, scenario), values_to=\"value\", names_to=\"variable\")\n  \n  show(\n    ggplot(results_combined_vars_long, aes(x = scenario, y = value, fill = method)) +\n      geom_col(position = \"dodge\") +\n      facet_wrap(~variable, scales = \"free_y\", ncol = 2) +\n      labs(\n        title = \"Output Variables Comparison Across Methods and Scenarios\",\n        x = \"Scenario\",\n        y = \"Value\",\n        fill = \"Method\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  )\n\n  message(\"Look at startingE details\")\n  results_combined_startingE_vars &lt;- results_combined |&gt;\n    select(method, scenario, startingE, startingE_table) |&gt;\n    unnest_wider(startingE_table)\n  show(results_combined_startingE_vars)\n\n  results_combined_startingE_vars_long &lt;- results_combined |&gt;\n    select(method, scenario, startingE_table) |&gt;\n    unnest_longer(startingE_table, indices_to = \"variable\", values_to = \"value\") |&gt;\n    unnest_longer(value)\n  \n  show(\n    ggplot(results_combined_startingE_vars_long, aes(x = scenario, y = value, fill = method)) +\n      geom_col(position = \"dodge\") +\n      facet_wrap(~variable, scales = \"free_y\", ncol = 2) +\n      labs(\n        title = \"startingE_table Variables Comparison Across Methods and Scenarios\",\n        x = \"Scenario\",\n        y = \"Value\",\n        fill = \"Method\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  )\n}\n\n\npaper results\nSee paper\n\n\nstrength prior [1,10]\n\nmcmc with continuous prior [1,10] (like paper)\n\nMCMC_continuous_results_all &lt;- run_all_scenarios(\n  \"MCMC\", discrete = FALSE\n)\n\nusing webppl version: main v0.9.15-0eb9bf5 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\n\nmcmc with discrete prior [1,10]\n\nMCMC_discrete_results_all &lt;- run_all_scenarios(\n  \"MCMC\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"[1,10]\"\n)\n\nusing webppl version: main v0.9.15-0eb9bf5 /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\n\nenumerate with discrete prior [1,10]\n\nenumerate_results_all &lt;- run_all_scenarios(\n  \"enumerate\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"[1,10]\"\n)\n\n\n\nsummarize\n\ncompare_results(\n  MCMC_continuous = MCMC_continuous_results_all,\n  MCMC_discrete = MCMC_discrete_results_all,\n  enumerate = enumerate_results_all\n)\n\nNow compare visually\n\n\n# A tibble: 18 × 3\n   method          scenario     P\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 MCMC_continuous ffff     0.612\n 2 MCMC_continuous fffl     0.969\n 3 MCMC_continuous flfl     0.953\n 4 MCMC_continuous ffll     1    \n 5 MCMC_continuous flll     0.993\n 6 MCMC_continuous llll     1    \n 7 MCMC_discrete   ffff     0.637\n 8 MCMC_discrete   fffl     0.968\n 9 MCMC_discrete   flfl     0.949\n10 MCMC_discrete   ffll     1    \n11 MCMC_discrete   flll     0.991\n12 MCMC_discrete   llll     1    \n13 enumerate       ffff     0.714\n14 enumerate       fffl     0.964\n15 enumerate       flfl     0.970\n16 enumerate       ffll     1    \n17 enumerate       flll     0.989\n18 enumerate       llll     1    \n\n\n\n\n\n\n\n\n\nNow separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare all output values across methods\n\n\n# A tibble: 18 × 12\n   method   scenario     P      aU      bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;    \n 1 MCMC_co… ffff     0.612  0.0939  0.0939  0.9   0.9   0.188   0.612 &lt;dbl&gt;     \n 2 MCMC_co… fffl     0.969 12.1     5.36    0.35  0.85 17.5     0.969 &lt;dbl&gt;     \n 3 MCMC_co… flfl     0.953 12.0    11.4     0.5   0.55 23.4     0.953 &lt;dbl&gt;     \n 4 MCMC_co… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n 5 MCMC_co… flll     0.993 14.5    14.5     0.4   0.4  28.9     0.993 &lt;dbl&gt;     \n 6 MCMC_co… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n 7 MCMC_di… ffff     0.637 -0.244   0.431   0.95  0.9   0.187   0.637 &lt;dbl&gt;     \n 8 MCMC_di… fffl     0.968 12.1     5.33    0.35  0.85 17.4     0.968 &lt;dbl&gt;     \n 9 MCMC_di… flfl     0.949 11.9    11.3     0.5   0.55 23.2     0.949 &lt;dbl&gt;     \n10 MCMC_di… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n11 MCMC_di… flll     0.991 14.4    14.4     0.4   0.4  28.9     0.991 &lt;dbl&gt;     \n12 MCMC_di… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n13 enumera… ffff     0.714  0.781   0.781   1     1     1.56    0.714 &lt;dbl&gt;     \n14 enumera… fffl     0.964 12.0     5.26    0.35  0.85 17.3     0.964 &lt;dbl&gt;     \n15 enumera… flfl     0.970 13.1     9.68    0.35  0.6  22.7     0.970 &lt;dbl&gt;     \n16 enumera… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n17 enumera… flll     0.989 14.4    14.4     0.4   0.4  28.8     0.989 &lt;dbl&gt;     \n18 enumera… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n# ℹ 2 more variables: b_strength &lt;list&gt;, startingE &lt;dbl&gt;\n\n\nExtract all final_table values into a comprehensive summary\n\n\n\n\n\n\n\n\n\nLook at startingE details\n\n\n# A tibble: 18 × 11\n   method   scenario startingE    aU    bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;list&gt;    \n 1 MCMC_co… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 2 MCMC_co… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 3 MCMC_co… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n 4 MCMC_co… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 5 MCMC_co… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n 6 MCMC_co… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n 7 MCMC_di… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 8 MCMC_di… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 9 MCMC_di… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n10 MCMC_di… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n11 MCMC_di… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n12 MCMC_di… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n13 enumera… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n14 enumera… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n15 enumera… flfl          0.3   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n16 enumera… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n17 enumera… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n18 enumera… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n# ℹ 1 more variable: b_strength &lt;list&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nstrength prior (1,10)\n\nmcmc with discrete prior (1,10)\n\nMCMC_discrete_results_exclexcl &lt;- run_all_scenarios(\n  \"MCMC\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"(1,10)\"\n)\n\n\n\nenumerate with discrete prior (1,10)\n\nenumerate_results_exclexcl &lt;- run_all_scenarios(\n  \"enumerate\", discrete = TRUE,\n  strengthPriorPrecision = .05, strengthPriorVersion = \"(1,10)\"\n)\n\n\n\nsummarize\n\ncompare_results(\n  MCMC_continuous = MCMC_continuous_results_all,\n  MCMC_discrete = MCMC_discrete_results_exclexcl,\n  enumerate = enumerate_results_exclexcl\n)\n\nNow compare visually\n\n\n# A tibble: 18 × 3\n   method          scenario     P\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 MCMC_continuous ffff     0.612\n 2 MCMC_continuous fffl     0.969\n 3 MCMC_continuous flfl     0.953\n 4 MCMC_continuous ffll     1    \n 5 MCMC_continuous flll     0.993\n 6 MCMC_continuous llll     1    \n 7 MCMC_discrete   ffff     0.733\n 8 MCMC_discrete   fffl     0.969\n 9 MCMC_discrete   flfl     0.950\n10 MCMC_discrete   ffll     1    \n11 MCMC_discrete   flll     0.990\n12 MCMC_discrete   llll     1    \n13 enumerate       ffff     0.726\n14 enumerate       fffl     0.967\n15 enumerate       flfl     0.972\n16 enumerate       ffll     1    \n17 enumerate       flll     0.989\n18 enumerate       llll     1    \n\n\n\n\n\n\n\n\n\nNow separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare all output values across methods\n\n\n# A tibble: 18 × 12\n   method   scenario     P      aU      bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;    \n 1 MCMC_co… ffff     0.612  0.0939  0.0939  0.9   0.9   0.188   0.612 &lt;dbl&gt;     \n 2 MCMC_co… fffl     0.969 12.1     5.36    0.35  0.85 17.5     0.969 &lt;dbl&gt;     \n 3 MCMC_co… flfl     0.953 12.0    11.4     0.5   0.55 23.4     0.953 &lt;dbl&gt;     \n 4 MCMC_co… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n 5 MCMC_co… flll     0.993 14.5    14.5     0.4   0.4  28.9     0.993 &lt;dbl&gt;     \n 6 MCMC_co… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n 7 MCMC_di… ffff     0.733  1.16    1.16    1     1     2.33    0.733 &lt;dbl&gt;     \n 8 MCMC_di… fffl     0.969 12.1     5.35    0.35  0.85 17.4     0.969 &lt;dbl&gt;     \n 9 MCMC_di… flfl     0.950 12.0    11.3     0.5   0.55 23.2     0.950 &lt;dbl&gt;     \n10 MCMC_di… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n11 MCMC_di… flll     0.990 14.4    14.4     0.4   0.4  28.8     0.990 &lt;dbl&gt;     \n12 MCMC_di… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n13 enumera… ffff     0.726  1.02    1.02    1     1     2.03    0.726 &lt;dbl&gt;     \n14 enumera… fffl     0.967 12.1     5.31    0.35  0.85 17.4     0.967 &lt;dbl&gt;     \n15 enumera… flfl     0.972 13.1     9.72    0.35  0.6  22.8     0.972 &lt;dbl&gt;     \n16 enumera… ffll     1     13.2    13.2     0.5   0.5  26.5     1     &lt;dbl&gt;     \n17 enumera… flll     0.989 14.4    14.4     0.4   0.4  28.7     0.989 &lt;dbl&gt;     \n18 enumera… llll     1     15.3    15.3     0.35  0.35 30.6     1     &lt;dbl&gt;     \n# ℹ 2 more variables: b_strength &lt;list&gt;, startingE &lt;dbl&gt;\n\n\nExtract all final_table values into a comprehensive summary\n\n\n\n\n\n\n\n\n\nLook at startingE details\n\n\n# A tibble: 18 × 11\n   method   scenario startingE    aU    bU    aE    bE jointU outcome a_strength\n   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;list&gt;    \n 1 MCMC_co… ffff          0.65   0   0      0     0       0         0 &lt;dbl&gt;     \n 2 MCMC_co… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 3 MCMC_co… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n 4 MCMC_co… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 5 MCMC_co… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n 6 MCMC_co… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n 7 MCMC_di… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n 8 MCMC_di… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n 9 MCMC_di… flfl          0.5   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n10 MCMC_di… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n11 MCMC_di… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n12 MCMC_di… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n13 enumera… ffff          1      0   0      0     0       0         0 &lt;dbl&gt;     \n14 enumera… fffl          0.3   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n15 enumera… flfl          0.3   14.1 5.96   0.1   0.7    20.0       1 &lt;dbl&gt;     \n16 enumera… ffll          0.4   13.9 0.375  0     1      14.2       1 &lt;dbl&gt;     \n17 enumera… flll          0.4   14.2 6.73   0.1   0.65   20.9       1 &lt;dbl&gt;     \n18 enumera… llll          0.35  14.3 8.23   0.15  0.6    22.5       1 &lt;dbl&gt;     \n# ℹ 1 more variable: b_strength &lt;list&gt;\n\n\n\n\n\n\n\n\n\n\n\n\ntesting\nTEMP\n\ninvisible(\n  run_scenario(\n    'ffff', 'enumerate', discrete = T,\n    strengthPriorPrecision = .03,\n    strengthPriorVersion = \"[1,10]\"\n  )\n)\n\n# and compare to output from memo when using same precision (.05)"
  },
  {
    "objectID": "webppl vs memo/webppl.html",
    "href": "webppl vs memo/webppl.html",
    "title": "",
    "section": "",
    "text": "Cards\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nalice_chooses_card_model &lt;- '\nvar alice_chooses_card = function() {\n   var card = sample(Categorical({ vs: [1,2,3] }))\n   return card\n}\nvar dist = Infer(alice_chooses_card)\nexpectation(dist)\n'\nalice_chooses_card_E &lt;- rwebppl::webppl(alice_chooses_card_model)\n\nusing webppl version: main v0.9.15-cf87b2a /Users/jacobzimmerman/ucsd/fyp/memo-sandbox/.pixi/envs/default/lib/R/library/rwebppl/js/webppl\n\n\n\nshow(alice_chooses_card_E)\n\n[1] 2\n\n\n\n\nBasic conditioning\n\n\nlibrary(tidyverse)\nbasic_inference_model &lt;- '\nvar model = function() {\n   // uniform prior of true value, discretized for parity with memo\n   var value = randomInteger(101) / 100; // uniform(0, 1)\n\n   // someone has observed that value is &gt;.5\n   condition(value &gt; .5)\n   \n   // return the estimated probability of each possible value\n   return value\n}\nvar dist = Infer(model)\ndist\n\n// alternatively, output the expected value:\n// expectation(dist)\n'\nposterior &lt;- rwebppl::webppl(basic_inference_model) |&gt;\n   arrange(support)\n\n# extra: could compute E of posterior manually like this\n# posteriorE &lt;- sum(as.numeric(posterior$prob) * as.numeric(posterior$support))\n# posteriorE\n\nposterior |&gt;\n   # fill in missing support (HACK: need to round support values to ensure compatibility when calling `complete`)\n   mutate(support = round(support, 3)) |&gt;\n   complete(support = seq(0, 1, by = 0.01) |&gt; round(3), fill = list(prob = 0)) |&gt;\n   ggplot() + geom_line(aes(x=support, y=prob)) + lims(x=c(0,1), y=c(0, NA))"
  },
  {
    "objectID": "webppl vs memo/index.html#memo",
    "href": "webppl vs memo/index.html#memo",
    "title": "Basic comparison of WebPPL vs Memo",
    "section": "memo",
    "text": "memo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "memo-sandbox",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nModified\n\n\n\n\n\n\n\n\n \n\n\nBasic comparison of WebPPL vs Memo\n\n\n12/20/25, 3:38:45 PM\n\n\n\n\n\n\n \n\n\n \n\n\n12/15/25, 11:57:43 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#webppl-vs-memo",
    "href": "index.html#webppl-vs-memo",
    "title": "memo-sandbox",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nModified\n\n\n\n\n\n\n\n\n \n\n\nBasic comparison of WebPPL vs Memo\n\n\n12/20/25, 3:38:45 PM\n\n\n\n\n\n\n \n\n\n \n\n\n12/15/25, 11:57:43 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "webppl vs memo/memo.html",
    "href": "webppl vs memo/memo.html",
    "title": "Basic conditioning",
    "section": "",
    "text": "Cards\n\n# adapted from https://github.com/kach/memo/blob/main/demo/Memonomicon.ipynb\nimport jax\nimport jax.numpy as np\nfrom memo import memo\n\ncards = np.array([1, 2, 3])\n\n@memo(save_comic=\"memo-comic-card\")\ndef alice_chooses_card_E():\n    alice: chooses(c in cards, wpp=1)\n    return E[alice.c]\n\n# Comic representation of the modeling: ./memo-comic-card.png\n\n\nprint(alice_chooses_card_E())\n\n2.0\n\n\n\nfrom memo import memo\nimport jax.numpy as np\n\npossible_values = np.array(range(1,101)) / 100\n\n@memo(save_comic=\"memo-comic-bc\")\ndef model[query_v: possible_values]():\n    # establish prior in the observer's frame: that a value generator has some value from .01 to 1\n    value_gen: given(v in possible_values, wpp=1)\n    \n    # push an observation into a valuator's frame:\n    # first, establish the prior in the valuator's frame: have the valuator model that the value generator chose a value uniformly at random\n    valuator: thinks[ value_gen: given(v in possible_values, wpp=1) ]\n    # then, now that the value is modeled by the valuator, condition that value\n    valuator: observes_that [value_gen.v &gt; .5]\n\n    # return the estimated probability of the generator's value for being each queried v, *estimated according to the valuator/from the valuator's frame*\n    valuator: knows(query_v) # first, push query_v into the frame of the valuator\n    return valuator[Pr[value_gen.v == query_v]]\n\n    # or return the expected value in the valuator's frame:\n    # return valuator[E[value_gen.v]]\n\n# Comic representation of the modeling: ./memo-comic-bc.png\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(possible_values, model())"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "",
    "text": "Code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#setup",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Setup",
    "text": "Setup\n\nimport jax\nimport jax.numpy as np\nfrom memo import memo\nfrom matplotlib import pyplot as plt\nfrom functools import partial\n\n# Model parameters (from original WebPPL code)\nalpha = 13.5  # effort cost coefficient, from original paper; kept fixed for replication\nbeta = 24.5   # inequality aversion coefficient, from original paper; kept fixed for replication\nweight_box = 5  # weight of the box\nlow_reward = 10\nhigh_reward = 20\n\n# Discretized spaces (for computational tractability)\nefforts = np.array([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n                    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])\n\n# Discretized strength space (replacing continuous uniform(1,10))\nstrength_values = np.linspace(1., 10., 301) # [1,10] with step size of .03\n\nDEBUG = True"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#helper-functions-jax-compatible",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#helper-functions-jax-compatible",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Helper Functions (JAX-compatible)",
    "text": "Helper Functions (JAX-compatible)\n\n@jax.jit\ndef lift(strength, effort):\n    \"\"\"Single agent lift: can they lift the box?\"\"\"\n    return (effort * strength &gt;= weight_box).astype(float)\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef lift2(strength1, strength2, effort1, effort2, model_features):\n    \"\"\"Two-agent joint lift: can they lift the box together?\"\"\"\n    k = 3.5 # from original paper; kept fixed for replication\n    return jax.lax.cond(\n        ModelFeatures.SAFE in model_features,\n        lambda _: (effort1*strength1 + effort2*strength2) &gt;= (weight_box + ((1-effort1)+(1-effort2))*k),\n        lambda _: (effort1 * strength1 + effort2 * strength2 &gt;= weight_box),\n        operand=None\n    ).astype(float)\n\n@jax.jit\ndef gini(effort1, effort2):\n    \"\"\"Gini coefficient for inequality aversion\"\"\"\n    return np.where(\n        np.abs(effort1 - effort2) &lt; 1e-8, # instead of effort1 == effort2 (to bypass floating point error)\n        0.0,\n        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)\n    )"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#bayesian-inference-over-agent-strengths",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Bayesian Inference Over Agent Strengths",
    "text": "Bayesian Inference Over Agent Strengths\nThe original WebPPL code infers agent strengths from observed outcomes in rounds 1 and 2.\n\n@jax.jit\ndef argmax_utility(strength, reward, effort_space=efforts):\n    \"\"\"Find effort that maximizes utility for single agent\"\"\"\n    utilities = reward * lift(strength, effort_space) - alpha * effort_space\n    return effort_space[np.argmax(utilities)]\n\n@jax.jit\ndef outcome(strength, reward):\n    \"\"\"Predicted outcome (success probability) for single agent at optimal effort\"\"\"\n    opt_effort = argmax_utility(strength, reward)\n    return lift(strength, opt_effort)\n\n@memo(save_comic=\"memo-comic-xiang-strength-inference\", debug_trace=DEBUG)\ndef infer_strengths[query_sa: strength_values, query_sb: strength_values](\n    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b\n):\n    \"\"\"\n    Infer posterior distribution over agent strengths given observed outcomes.\n\n    Observations:\n    - Round 1 (low reward = 10): A and B each attempt individual lifts\n    - Round 2 (high reward = 20): A and B each attempt individual lifts\n\n    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]\n    \"\"\"\n\n    # The participant models that the agents have some strength in [1, 10]\n    participant: thinks[\n        agent_a: given(sa in strength_values, wpp=1),\n        agent_b: given(sb in strength_values, wpp=1)\n    ]\n\n    # Participant witnesses the outcomes and conditions on them\n    # Condition on round 1 outcomes (low reward)\n    participant: observes_that [outcome(agent_a.sa, {low_reward}) == r1_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {low_reward}) == r1_outcome_b]\n\n    # Condition on round 2 outcomes (high reward)\n    participant: observes_that [outcome(agent_a.sa, {high_reward}) == r2_outcome_a]\n    participant: observes_that [outcome(agent_b.sb, {high_reward}) == r2_outcome_b]\n\n    # Push query variables into participant frame \n    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths\n    participant: knows(query_sa)\n    participant: knows(query_sb)\n    return participant[Pr[agent_a.sa == query_sa and agent_b.sb == query_sb]]"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-optimization-of-effort",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#game-theoretic-joint-optimization-of-effort",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Game-Theoretic Joint Optimization of Effort",
    "text": "Game-Theoretic Joint Optimization of Effort\nThe original WebPPL code models:\n\nTwo agents with uncertain strengths (posterior distributions from rounds 1-2)\nAgents reason about each other’s efforts in round 3 (joint task)\nIterative best-response until Nash equilibrium\nOptimization over initial effort to maximize joint utility\n\nKey insight: The game-theoretic reasoning operates on expected utilities over the posterior distribution of strengths. This is not purely probabilistic reasoning (like the inference above), but rather decision-theoretic optimization under uncertainty.\nProposed approach:\n\nKeep the Bayesian inference in @memo (already done above)\nImplement the game-theoretic equilibrium finding in pure JAX (deterministic optimization)\nCombine them: compute equilibrium for each strength pair, weighted by posterior\n\n\nfrom enum import Flag, auto\nclass ModelFeatures(Flag):\n    NONE = 0\n    GINI = auto()\n    SAFE = auto()\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef lift2(strength1, strength2, effort1, effort2, model_features):\n    \"\"\"Two-agent joint lift: can they lift the box together?\"\"\"\n    k = 3.5 # from original paper; kept fixed for replication\n    return jax.lax.cond(\n        ModelFeatures.SAFE in model_features,\n        lambda _: (effort1*strength1 + effort2*strength2) &gt;= (weight_box + ((1-effort1)+(1-effort2))*k),\n        lambda _: (effort1 * strength1 + effort2 * strength2 &gt;= weight_box),\n        operand=None\n    ).astype(float)\n\n@jax.jit\ndef gini(effort1, effort2):\n    \"\"\"Gini coefficient for inequality aversion\"\"\"\n    return np.where(\n        np.abs(effort1 - effort2) &lt; 1e-8, # instead of effort1 == effort2 (to bypass floating point error)\n        0.0,\n        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)\n    )\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef outcome2(strength_a, strength_b, effort_a, effort_b, model_features):\n    \"\"\"Predicted outcome (success probability) for joint action\"\"\"\n    if np.ndim(strength_a) &gt; 0:\n        # translation of listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2))\n        # note that map2 maps over the input arrays *concurrently*\n        outcomes = jax.vmap(\n            lambda sa, sb: lift2(sa, sb, effort_a, effort_b, model_features)\n        )(strength_a, strength_b)\n        return np.mean(outcomes)\n    else:\n        return lift2(strength_a, strength_b, effort_a, effort_b, model_features)\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef joint_utility_multiple_strength(strength_a, strength_b, effort_a, effort_b, model_features):\n    \"\"\"\n    Compute utilities for both agents given multiple fixed strengths and efforts.\n    Returns: (utility_a, utility_b, success_prob)\n    \"\"\"\n    reward = high_reward  # Round 3 uses high reward\n    \n    success = outcome2(strength_a, strength_b, effort_a, effort_b, model_features=model_features)\n    gini_coef = jax.lax.cond(\n        ModelFeatures.GINI in model_features,\n        lambda _: gini(effort_a, effort_b),\n        lambda _: np.array(0.0),\n        operand=None\n    )\n\n    utility_a = reward * success - alpha * effort_a - beta * gini_coef\n    utility_b = reward * success - alpha * effort_b - beta * gini_coef\n\n    return utility_a, utility_b, success\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef best_response_a(strength_a, strength_b, effort_b, model_features):\n    \"\"\"\n    Find agent A's best response to agent B's effort.\n    Agent A optimizes: max_{e_a} E[utility_a(e_a, e_b) | s_a, s_b]\n    \"\"\"\n    utilities = jax.vmap(\n        lambda ea: joint_utility_multiple_strength(strength_a, strength_b, ea, effort_b, model_features=model_features)[0]\n    )(efforts)\n    return efforts[np.argmax(utilities)]\n\n@partial(jax.jit, static_argnames=['model_features'])\ndef best_response_b(strength_a, strength_b, effort_a, model_features):\n    \"\"\"\n    Find agent B's best response to agent A's effort.\n    \"\"\"\n    utilities = jax.vmap(\n        lambda eb: joint_utility_multiple_strength(strength_a, strength_b, effort_a, eb, model_features=model_features)[1]\n    )(efforts)\n    return efforts[np.argmax(utilities)]\n\ndef find_equilibrium(strength_a, strength_b, init_effort, model_features, max_depth=10, verbose=False, trace=False):\n    \"\"\"\n    Find Nash equilibrium through mutually recursive best responses.\n\n    This implementation EXACTLY matches the WebPPL version's recursive structure:\n    - a(depth, reward) calls b(depth-1, reward)\n    - b(depth, reward) calls a(depth, reward) if depth &gt; 0, else uses init_effort\n    - Convergence checked using findDepth() at specific depths [1, 2, 5, 10]\n\n    Args:\n        strength_a: Agent A's strength (scalar or array)\n        strength_b: Agent B's strength (scalar or array)\n        init_effort: Initial effort for agent B at depth=0\n        max_depth: Maximum recursion depth\n        verbose: Print convergence info\n        trace: Print detailed trace of recursive calls\n\n    Returns:\n        (effort_a, effort_b, converged_depth)\n    \"\"\"\n    reward = high_reward  # Round 3 reward (matches WebPPL r3_reward)\n\n    # Cache for memoizing recursive calls (mimics WebPPL's call stack behavior)\n    cache_a = {}\n    cache_b = {}\n\n    def a(depth):\n        \"\"\"Agent A's best response at given recursion depth.\"\"\"\n        if depth in cache_a:\n            return cache_a[depth]\n\n        # Get B's effort from one level down\n        effort_b = b(depth - 1)\n\n        if trace:\n            print(f\"  a(depth={depth}): B's effort from b({depth-1}) = {effort_b:.4f}\")\n\n        # A optimizes given B's effort\n        effort_a = best_response_a(strength_a, strength_b, effort_b, model_features)\n\n        if trace:\n            print(f\"  a(depth={depth}): A's best response = {effort_a:.4f}\")\n\n        cache_a[depth] = effort_a\n        return effort_a\n\n    def b(depth):\n        \"\"\"Agent B's best response at given recursion depth.\"\"\"\n        if depth in cache_b:\n            return cache_b[depth]\n\n        # Base case: depth 0 uses initial effort\n        if depth == 0:\n            if trace:\n                print(f\"  b(depth={depth}): Using init_effort = {init_effort:.4f}\")\n            cache_b[depth] = init_effort\n            return init_effort\n\n        # Get A's effort from same level\n        effort_a = a(depth)\n\n        if trace:\n            print(f\"  b(depth={depth}): A's effort from a({depth}) = {effort_a:.4f}\")\n\n        # B optimizes given A's effort\n        effort_b = best_response_b(strength_a, strength_b, effort_a, model_features)\n\n        if trace:\n            print(f\"  b(depth={depth}): B's best response = {effort_b:.4f}\")\n\n        cache_b[depth] = effort_b\n        return effort_b\n\n    def findDepth(x):\n        \"\"\"Check if convergence achieved at depth x (matches WebPPL logic).\"\"\"\n        b_at_x = b(x)\n        b_at_x_plus_1 = b(x + 1)\n        converged = np.abs(b_at_x - b_at_x_plus_1) &lt; 0.06\n\n        if trace:\n            print(f\"findDepth({x}): b({x})={b_at_x:.4f}, b({x+1})={b_at_x_plus_1:.4f}, diff={np.abs(b_at_x - b_at_x_plus_1):.4f}, converged={converged}\")\n\n        return x if converged else -1\n\n    # Try depths in order [1, 2, 5, 10] (matches WebPPL ds array)\n    candidate_depths = [1, 2, 5, 10]\n\n    if trace:\n        print(f\"\\n=== find_equilibrium(init_effort={init_effort:.4f}) ===\")\n\n    for depth_candidate in candidate_depths:\n        if depth_candidate &gt; max_depth:\n            break\n        result = findDepth(depth_candidate)\n        if result &gt; 0:\n            # Converged at this depth\n            # Return efforts from depth+1 for A, depth for B (matches WebPPL lines 209-210)\n            effort_a = a(result + 1)\n            effort_b = b(result)\n\n            if verbose:\n                print(f\"  Converged at depth {result}: effort_a={effort_a:.4f}, effort_b={effort_b:.4f}\")\n\n            return effort_a, effort_b, result\n\n    # Did not converge\n    print(f\"Warning: Effort could not converge in {candidate_depths[-1]} iterations\")\n    effort_a = a(candidate_depths[-1] + 1)\n    effort_b = b(candidate_depths[-1])\n    return effort_a, effort_b, candidate_depths[-1]\n\n# To find the optimal initial effort, we'd search over init_effort values\ndef starting_effort(posterior_sa, posterior_sb, model_features):\n    \"\"\"\n    Find initial effort that maximizes expected joint utility.\n\n    This matches the WebPPL startingEffort() function which optimizes jointU.\n\n    Args:\n        posterior_sa - sampled from posterior\n        posterior_sb - sampled from posterior\n\n    Returns:\n        Optimal initial effort value\n    \"\"\"\n    joint_utilities = []\n    for init_effort in efforts:\n        stats = expected_joint_utility(init_effort, posterior_sa, posterior_sb, model_features=model_features)\n        joint_utilities.append(stats['jointU'])  # Optimize joint utility (matches WebPPL)\n\n    joint_utilities = np.array(joint_utilities)\n    return efforts[np.argmax(joint_utilities)]\n\ndef expected_joint_utility(init_effort, posterior_sa, posterior_sb, model_features):\n    \"\"\"\n    Compute expected utilities and efforts over posterior distribution of strengths.\n\n    This matches the WebPPL jointUtility() function output, computing expectations over\n    the posterior distribution of agent strengths.\n\n    Args:\n        posterior_sa: sampled from posterior\n        posterior_sb: sampled from posterior\n        init_effort: Initial effort for agent B to start equilibrium search\n\n    Returns:\n        dict with keys: joint_utility, agent_a_utility, agent_b_utility,\n                       agent_a_effort, agent_b_effort, outcome_prob\n    \"\"\"\n    # Compute expectations by averaging over samples\n    effort_a, effort_b, depth = find_equilibrium(posterior_sa, posterior_sb, init_effort, model_features)\n\n    # print(f\"Strengths (A={posterior_sa:.2f}, B={posterior_sb:.2f}): Efforts (A={effort_a:.4f}, B={effort_b:.4f})\")\n\n    # Compute utilities and outcome at equilibrium\n    utility_a, utility_b, success_prob = joint_utility_multiple_strength(posterior_sa, posterior_sb, effort_a, effort_b, model_features=model_features)\n\n    # Test equilibrium finding with a sample strength pair\n    if (init_effort == 0.0 or not (ModelFeatures.GINI in model_features)) and DEBUG:\n        print(f\"\\n=== EQUILIBRIUM FINDING (init_effort={init_effort}, sample strength pair) ===\")\n        print(f\"Convergence depth: {depth}\")\n        print(f\"Agent A equilibrium effort: {effort_a:.4f}\")\n        print(f\"Agent B equilibrium effort: {effort_b:.4f}\")\n\n    return {\n        'jointU': utility_a + utility_b,\n        'aU': utility_a,\n        'bU': utility_b,\n        'aE': effort_a,\n        'bE': effort_b,\n        'P': outcome2(posterior_sa, posterior_sb, effort_a, effort_b, model_features=model_features)\n    }\n\n\n# for solitary/compensatory effort models\ndef independent_effort_optimization(others_effort, posterior_sa, posterior_sb):\n    \"\"\"\n    Compute efforts when each agent independently optimizes assuming \n    partner uses a fixed effort level (solitary=0, compensatory=1).\n    \n    This is NOT game-theoretic - no mutual best response.\n    Each agent simply optimizes: max_{e} E[U(e, others_effort)]\n    \"\"\"\n    \n    # Agent A optimizes assuming B uses others_effort\n    utilities_a = jax.vmap(\n        lambda ea: joint_utility_multiple_strength(\n            posterior_sa, posterior_sb, ea, others_effort, model_features=ModelFeatures.NONE\n        )[0]  # Get utility_a\n    )(efforts)\n    effort_a = efforts[np.argmax(utilities_a)]\n    \n    # Agent B optimizes assuming A uses others_effort  \n    utilities_b = jax.vmap(\n        lambda eb: joint_utility_multiple_strength(\n            posterior_sa, posterior_sb, others_effort, eb, model_features=ModelFeatures.NONE\n        )[1]  # Get utility_b\n    )(efforts)\n    effort_b = efforts[np.argmax(utilities_b)]\n    \n    # Compute final statistics with these independent efforts\n    utility_a, utility_b, success_prob = joint_utility_multiple_strength(\n        posterior_sa, posterior_sb, effort_a, effort_b, model_features=ModelFeatures.NONE\n    )\n    \n    return {\n        'jointU': utility_a + utility_b,\n        'aU': utility_a,\n        'bU': utility_b,\n        'aE': effort_a,\n        'bE': effort_b,\n        'P': success_prob\n    }"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-a-scenario",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#running-the-model-for-a-scenario",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Running the Model for a Scenario",
    "text": "Running the Model for a Scenario\n\nclass Model():\n    def __init__(self, r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b):\n        # 1. Get posterior over strengths\n\n        if DEBUG:\n            print(\"\\nComputing posterior over agent strengths...\")\n\n        posterior_strengths = infer_strengths(r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b)\n\n        if DEBUG:\n            # DEBUG: Bayesian inference statistics\n            print(\"\\n=== BAYESIAN INFERENCE STATISTICS ===\")\n            posterior_2d = posterior_strengths.reshape(len(strength_values), len(strength_values))\n            print(f\"Posterior shape: {posterior_2d.shape}\")\n            print(f\"Number of non-negligible posterior samples (compare to WebPPL): {np.sum(posterior_strengths &gt; 1e-10)}\")\n            print(f\"Total probability mass: {np.sum(posterior_2d):.6f}\")\n\n            # Compute marginals\n            marginal_a = np.sum(posterior_2d, axis=1)\n            marginal_b = np.sum(posterior_2d, axis=0)\n\n            # Compute statistics\n            mean_sa = np.sum(marginal_a * strength_values)\n            mean_sb = np.sum(marginal_b * strength_values)\n            print(f\"Agent A strength - mean: {mean_sa:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n            print(f\"Agent B strength - mean: {mean_sb:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}\")\n\n            # Find high-probability regions\n            high_prob_threshold = np.max(posterior_2d) * 0.1\n            high_prob_indices = np.where(posterior_2d &gt; high_prob_threshold)\n            print(f\"High probability region (top 10%): strength_a in [{strength_values[np.min(high_prob_indices[0])]:.2f}, {strength_values[np.max(high_prob_indices[0])]:.2f}]\")\n            print(f\"                                     strength_b in [{strength_values[np.min(high_prob_indices[1])]:.2f}, {strength_values[np.max(high_prob_indices[1])]:.2f}]\")\n\n            # Find the actual support of the posterior (non-zero probability mass)\n            posterior_support = posterior_2d &gt; 1e-10\n            support_indices = np.where(posterior_support)\n            if len(support_indices[0]) &gt; 0:\n                print(f\"Non-negligible posterior support:\")\n                print(f\"  strength_a in [{strength_values[np.min(support_indices[0])]:.2f}, {strength_values[np.max(support_indices[0])]:.2f}], with mean {np.sum(marginal_a * strength_values):.2f}\")\n                print(f\"  strength_b in [{strength_values[np.min(support_indices[1])]:.2f}, {strength_values[np.max(support_indices[1])]:.2f}], with mean {np.sum(marginal_b * strength_values):.2f}\")\n            else:\n                print(\"\\nWARNING: Posterior has no support! Inference may have failed.\")\n\n        self.posterior = posterior_strengths.reshape(len(strength_values), len(strength_values))\n\n        # assert that probabilities above 1e-10 all are the same\n        unique_probs = np.unique(self.posterior[self.posterior &gt; 1e-10])\n        if len(unique_probs) &gt; 1:\n            print(\"\\nERROR: Posterior probabilities vary! This WILL affect sampling accuracy.\")\n            print(f\"Unique non-negligible posterior probabilities: {unique_probs}\")\n        assert len(unique_probs) == 1, \"Posterior probabilities vary above threshold!\"\n        # if probs vary, would need to use a larger number of samples, to approximate the samples correctly, e.g.: \n        # num_samples = int(prob * 1000)\n        # posterior_sb.extend([sb] * num_samples)\n        # posterior_sa.extend([sa] * num_samples)\n        \n        # 2. Convert from posterior point probabilities to posterior samples\n        # (there should be sample counts proportional to the posterior probabilities)\n        posterior_sa = []\n        posterior_sb = []\n        for i, sa in enumerate(strength_values):\n            for j, sb in enumerate(strength_values):\n                prob = self.posterior[i, j]\n                if prob &gt; 1e-10:\n                    posterior_sa.extend([sa] * 1)\n                    posterior_sb.extend([sb] * 1)\n        self.posterior_sa = np.array(posterior_sa)\n        self.posterior_sb = np.array(posterior_sb)\n\n        if DEBUG:\n            print(f\"First 5 samples (sa, sb): {np.array(list(zip(list(reversed(self.posterior_sa))[:5], list(reversed(self.posterior_sb))[:5])))}\")\n            # ^reverse for the sake of matching the order by which webppl enumerates\n\n    def _check_stats(self, stats):\n        if DEBUG:\n            print(\"\\n=== FINAL RESULTS ===\")\n            if hasattr(self, 'optimal_init_effort'):\n                print(f\"Optimal starting effort: {self.optimal_init_effort}\")\n            print(f\"Joint utility at optimum: {stats['jointU']:.6f}\")\n            print(f\"Agent A utility: {stats['aU']:.6f}\")\n            print(f\"Agent B utility: {stats['bU']:.6f}\")\n            print(f\"Agent A equilibrium effort: {stats['aE']:.6f}\")\n            print(f\"Agent B equilibrium effort: {stats['bE']:.6f}\")\n            print(f\"Expected outcome probability: {stats['P']:.6f}\")\n            print(f\"\\n(Compare to original WebPPL output)\")\n\n        # Sanity check: joint utility should equal sum of individual utilities\n        joint_check = stats['aU'] + stats['bU']\n        if abs(joint_check - stats['jointU']) &gt; 1e-5:\n            print(f\"WARNING: Joint utility mismatch! {stats['jointU']:.6f} != {joint_check:.6f}\")\n\n    def joint_effort(self, model_features=ModelFeatures.GINI):\n        # Find optimal initial effort (by searching over effort space)\n        if DEBUG:\n            print(\"\\n=== FINDING OPTIMAL INITIAL EFFORT ===\")\n\n        self.optimal_init_effort = starting_effort(self.posterior_sa, self.posterior_sb, model_features=model_features)\n        if DEBUG:\n            print(f\"Using initial effort: {self.optimal_init_effort}\")\n\n        # Compute expected statistics at equilibrium (utilities, efforts, outcome)\n        if DEBUG:\n            print(\"\\nComputing expected statistics at equilibrium...\")\n        stats = expected_joint_utility(self.optimal_init_effort, self.posterior_sa, self.posterior_sb, model_features=model_features)\n\n        self._check_stats(stats)\n        return stats\n\n    def compensatory_effort(self):\n        stats = independent_effort_optimization(\n            others_effort=1.0,  # Assume partner uses maximum effort\n            posterior_sa=self.posterior_sa,\n            posterior_sb=self.posterior_sb\n        )\n        self._check_stats(stats)\n        return stats\n    \n    def solitary_effort(self):\n        stats = independent_effort_optimization(\n            others_effort=0.0,  # Assume partner contributes nothing\n            posterior_sa=self.posterior_sa,\n            posterior_sb=self.posterior_sb\n        )\n        self._check_stats(stats)\n        return stats\n\n\nModel(0.0, 0.0, 0.0, 0.0).joint_effort()\nModel(0.0, 0.0, 0.0, 1.0).solitary_effort()\nModel(0.0, 0.0, 0.0, 0.0).compensatory_effort()\n\n\nVisualization of inferred strength\n\nsample_model = Model(0.0, 0.0, 0.0, 0.0)\n\n# Visualize posterior over agent strengths\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Posterior heatmap\nim = axes[0].imshow(sample_model.posterior, origin='lower', extent=[1, 10, 1, 10], aspect='auto')\naxes[0].set_xlabel('Agent B Strength')\naxes[0].set_ylabel('Agent A Strength')\naxes[0].set_title('Posterior P(strength_A, strength_B | F,F;F,F)')\nplt.colorbar(im, ax=axes[0], label='Probability')\n\n# Marginal distributions\nmarginal_a = np.sum(sample_model.posterior, axis=1)\nmarginal_b = np.sum(sample_model.posterior, axis=0)\n\naxes[1].plot(strength_values, marginal_a, label='Agent A', linewidth=2)\naxes[1].plot(strength_values, marginal_b, label='Agent B', linewidth=2, linestyle='--')\naxes[1].set_xlabel('Strength')\naxes[1].set_ylabel('Probability')\naxes[1].set_title('Marginal Distributions')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nComputing posterior over agent strengths...\n --&gt; infer_strengths((0.0, 0.0, 0.0, 0.0))\n&lt;--  infer_strengths((0.0, 0.0, 0.0, 0.0)) has shape (301, 301)\n     time = 0.198271 sec\n\n=== BAYESIAN INFERENCE STATISTICS ===\nPosterior shape: (301, 301)\nNumber of non-negligible posterior samples (compare to WebPPL): 17956\nTotal probability mass: 1.000003\nAgent A strength - mean: 2.9950, min: 1.0000, max: 10.0000\nAgent B strength - mean: 2.9950, min: 1.0000, max: 10.0000\nHigh probability region (top 10%): strength_a in [1.00, 4.99]\n                                     strength_b in [1.00, 4.99]\nNon-negligible posterior support:\n  strength_a in [1.00, 4.99], with mean 2.99\n  strength_b in [1.00, 4.99], with mean 2.99\nFirst 5 samples (sa, sb): [[4.9900002 4.9900002]\n [4.9900002 4.96     ]\n [4.9900002 4.9300003]\n [4.9900002 4.9      ]\n [4.9900002 4.8700004]]"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#compute-predictions-for-all-scenarios",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#compute-predictions-for-all-scenarios",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Compute predictions for all scenarios",
    "text": "Compute predictions for all scenarios\n\nDEBUG = False\n\nmodels = {\n    \"F,F;F,F\": Model(0.0, 0.0, 0.0, 0.0),\n    \"F,F;F,L\": Model(0.0, 0.0, 0.0, 1.0),\n    \"F,F;L,L\": Model(0.0, 0.0, 1.0, 1.0),\n    \"F,L;F,L\": Model(0.0, 1.0, 0.0, 1.0),\n    \"F,L;L,L\": Model(0.0, 1.0, 1.0, 1.0),\n    \"L,L;L,L\": Model(1.0, 1.0, 1.0, 1.0)\n}\n\nmodel_fits = dict()\nmodel_fits['joint'] = {\n    scenario: model.joint_effort()\n    for scenario, model in models.items()\n}\nmodel_fits['compensatory'] = {\n    scenario: model.compensatory_effort()\n    for scenario, model in models.items()\n}\nmodel_fits['solitary'] = {\n    scenario: model.solitary_effort()\n    for scenario, model in models.items()\n}\n\n# to replicate competence_effort/Code/supplement/variations_of_joint_model/exp1_simulation.R\nmodel_fits['joint_wo_gini'] = {\n    scenario: model.joint_effort(ModelFeatures.NONE)\n    for scenario, model in models.items()\n}\nmodel_fits['safe_joint_w_gini'] = {\n    scenario: model.joint_effort(ModelFeatures.GINI | ModelFeatures.SAFE)\n    for scenario, model in models.items()\n}\n\n --&gt; infer_strengths((0.0, 0.0, 0.0, 0.0))\n&lt;--  infer_strengths((0.0, 0.0, 0.0, 0.0)) has shape (301, 301)\n     time = 0.000718 sec\n --&gt; infer_strengths((0.0, 0.0, 0.0, 1.0))\n&lt;--  infer_strengths((0.0, 0.0, 0.0, 1.0)) has shape (301, 301)\n     time = 0.000504 sec\n --&gt; infer_strengths((0.0, 0.0, 1.0, 1.0))\n&lt;--  infer_strengths((0.0, 0.0, 1.0, 1.0)) has shape (301, 301)\n     time = 0.000509 sec\n --&gt; infer_strengths((0.0, 1.0, 0.0, 1.0))\n&lt;--  infer_strengths((0.0, 1.0, 0.0, 1.0)) has shape (301, 301)\n     time = 0.000501 sec\n --&gt; infer_strengths((0.0, 1.0, 1.0, 1.0))\n&lt;--  infer_strengths((0.0, 1.0, 1.0, 1.0)) has shape (301, 301)\n     time = 0.000495 sec\n --&gt; infer_strengths((1.0, 1.0, 1.0, 1.0))\n&lt;--  infer_strengths((1.0, 1.0, 1.0, 1.0)) has shape (301, 301)\n     time = 0.000517 sec\n\n\n\nimport pandas as pd\n# Convert model fits to DataFrame for easier manipulation, converting key (e.g. \"joint\") to a column called \"model\"\nmodel_fits_df = pd.DataFrame.from_dict(\n    {(model, scenario): data \n     for model, scenarios in model_fits.items() \n     for scenario, data in scenarios.items()},\n    orient='index'\n).reset_index()\nmodel_fits_df.rename(columns={'level_0': 'model', 'level_1': 'scenario'}, inplace=True)\n\nmodel_fits_df\n\n\n\n\n\n\n\n\nmodel\nscenario\njointU\naU\nbU\naE\nbE\nP\n\n\n\n\n0\njoint\nF,F;F,F\n1.7502766\n0.8751383\n0.8751383\n1.0\n1.0\n0.7187569\n\n\n1\njoint\nF,F;F,L\n17.172792\n10.273896\n6.8988967\n0.5\n0.75\n0.9124448\n\n\n2\njoint\nF,F;L,L\n26.5\n13.25\n13.25\n0.5\n0.5\n1.0\n\n\n3\njoint\nF,L;F,L\n22.803928\n13.089464\n9.714463\n0.35\n0.6\n0.97131526\n\n\n4\njoint\nF,L;L,L\n28.812677\n14.406339\n14.406339\n0.4\n0.4\n0.9903169\n\n\n5\njoint\nL,L;L,L\n30.550003\n15.275002\n15.275002\n0.35\n0.35\n1.0\n\n\n6\ncompensatory\nF,F;F,F\n-1.918592\n-0.959296\n-0.959296\n0.85\n0.85\n0.52578527\n\n\n7\ncompensatory\nF,F;F,L\n-8.775\n0.0\n-8.775\n0.0\n0.65\n0.0\n\n\n8\ncompensatory\nF,F;L,L\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\ncompensatory\nF,L;F,L\n-6.333333\n0.20833334\n-6.5416665\n0.0\n0.5\n0.010416666\n\n\n10\ncompensatory\nF,L;L,L\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n11\ncompensatory\nL,L;L,L\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n12\nsolitary\nF,F;F,F\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n13\nsolitary\nF,F;F,L\n26.5\n20.0\n6.5\n0.0\n1.0\n0.99999994\n\n\n14\nsolitary\nF,F;L,L\n13.0\n6.5\n6.5\n1.0\n1.0\n1.0\n\n\n15\nsolitary\nF,L;F,L\n30.55\n20.0\n10.55\n0.0\n0.7\n1.0\n\n\n16\nsolitary\nF,L;L,L\n17.050001\n6.5000005\n10.550001\n1.0\n0.7\n1.0\n\n\n17\nsolitary\nL,L;L,L\n21.100002\n10.550001\n10.550001\n0.7\n0.7\n1.0\n\n\n18\njoint_wo_gini\nF,F;F,F\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n19\njoint_wo_gini\nF,F;F,L\n26.5\n20.0\n6.5\n0.0\n1.0\n0.99999994\n\n\n20\njoint_wo_gini\nF,F;L,L\n26.5\n6.5\n20.0\n1.0\n0.0\n1.0\n\n\n21\njoint_wo_gini\nF,L;F,L\n30.55\n20.0\n10.55\n0.0\n0.7\n1.0\n\n\n22\njoint_wo_gini\nF,L;L,L\n30.550001\n20.0\n10.550001\n0.0\n0.7\n1.0\n\n\n23\njoint_wo_gini\nL,L;L,L\n30.550003\n11.225001\n19.325\n0.65\n0.05\n1.0\n\n\n24\nsafe_joint_w_gini\nF,F;F,F\n1.7502766\n0.8751383\n0.8751383\n1.0\n1.0\n0.7187569\n\n\n25\nsafe_joint_w_gini\nF,F;F,L\n15.163114\n7.9190564\n7.244057\n0.85\n0.9\n0.9784528\n\n\n26\nsafe_joint_w_gini\nF,F;L,L\n21.02065\n10.510325\n10.510325\n0.7\n0.7\n0.99801624\n\n\n27\nsafe_joint_w_gini\nF,L;F,L\n18.397388\n9.198694\n9.198694\n0.75\n0.75\n0.9661847\n\n\n28\nsafe_joint_w_gini\nF,L;L,L\n22.635002\n11.655001\n10.980001\n0.6\n0.65\n1.0\n\n\n29\nsafe_joint_w_gini\nL,L;L,L\n24.407814\n12.203907\n12.203907\n0.55\n0.55\n0.9814453\n\n\n\n\n\n\n\n\n# save to csv\nmodel_fits_df.to_csv('xiang2023-exp1-round3-model_fits_results.csv', index=False)"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-conversion-strategy",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Notes on Conversion Strategy",
    "text": "Notes on Conversion Strategy\n\n\n\n\n\n\nWarning\n\n\n\nDisclaimer: Large portions of this section were written in collaboration with Claude Code and verified and edited for accuracy by the author. Of course, author takes responsibility for any inaccuracies and strongly welcomes corrections.\n\n\nThe conversion from WebPPL to memo revealed important distinctions:\n\n1. Probabilistic Inference vs. Decision Theory\nThe original WebPPL code mixes two types of reasoning:\n\nBayesian inference (rounds 1-2): Infer agent strengths from observed outcomes\nGame-theoretic optimization (round 3): Find Nash equilibrium efforts\n\nIn the memo conversion:\n\nBayesian inference: Use @memo with given(), observes_that[], and thinks[]\nGame-theoretic optimization: Use pure JAX functions (no probabilistic reasoning, just optimization)\n\n\n\n2. Key Conversion Decisions\nMutable arrays (x2a, x2b):\n\nWebPPL: Pushes strength posterior samples to arrays, uses them later\nmemo: Query the posterior distribution directly (more principled)\n\nNested agent reasoning:\n\nWebPPL: Nested JavaScript functions with recursive best-response\nJAX: Iterative best-response in pure JAX (not probabilistic, so no thinks[] needed here)\n\nExpected utilities:\n\nWebPPL: Implicitly handles through sampling or enumeration\nJAX: Explicit loop over posterior, computing equilibrium for each strength pair\n\n\n\n3. Computational Considerations\nThe original uses either:\n\nMCMC with 10,000 samples (slow but handles continuous distributions)\nEnumeration with discrete values (exact but potentially more expensive)\n\n\n\n4. What Could Be Improved\nFurther extension with memo:\nCurrently, the game-theoretic reasoning is outside of @memo. One could potentially:\n\nModel agents’ beliefs about each other’s strengths using nested thinks[]\nModel agents’ choices as chooses(e in efforts, wpp=utility(e))\nBut this might be overkill and harder to reason about\nHowever, this does incorporate uncertainty about the other agent into the model, which might add richness"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#how-to-compare-outputs",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "How to Compare Outputs",
    "text": "How to Compare Outputs\n(Also see TRACING_GUIDE.md)\nTo verify that the WebPPL and memo implementations produce the same results:\nThese should match (up to numerical precision and discretization choices).\nCompare outcome probability (here, expected_outcome), strength posteriors, and joint utility results for r3 outcome and for starting effort.\n\nExpected matches:\n\nHelper functions (lift, optE, outcome) should return identical values for test inputs\nBayesian inference statistics should show similar:\n\nPosterior means (agent strengths)\nNon-neglible-probability regions\n\nEquilibrium finding should show similar:\n\nConvergence depth (may vary slightly due to discretization)\nEquilibrium efforts\n\nFinal outcome probability should be close\n\nKnown differences:\n\nSee above\nSmall numerical differences due to different underlying computation engines"
  },
  {
    "objectID": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-the-alternative-models",
    "href": "webppl vs memo/xiang2023-exp1-round3-memo.html#notes-on-the-alternative-models",
    "title": "Xiang2023 Exp1 Round3 Modeling in Memo",
    "section": "Notes on the alternative models",
    "text": "Notes on the alternative models\nThe joint effort model and the solitary/compensatory models are fundamentally different in their computational structure:\nJoint Effort Model:\n\nUses game-theoretic Nash equilibrium finding\nAgents engage in mutually recursive best-response reasoning\nIncludes inequality aversion (Gini coefficient with β=24.5)\nOptimizes init_effort to maximize joint utility\n\nSolitary/Compensatory Models:\n\nUse independent optimization with fixed assumption of partner’s effort (0 or 1, resp.)\nEach agent independently maximizes their utility assuming partner uses fixed effort\nNo mutual recursion - direct calculation\nNo Gini coefficient - agents don’t care about effort matching"
  }
]