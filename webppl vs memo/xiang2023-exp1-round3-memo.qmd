---
format: html
---

Code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R

NOTE 10/20/25: next time I run the cached cells, I'll need to remove the "webppl vs memo/" directory prefix, and add a code block that sets the working directory appropriately for interactive execution

## Setup

```{python}
import jax
import jax.numpy as np
from memo import memo
from matplotlib import pyplot as plt

# Model parameters (from original WebPPL code)
alpha = 13.5  # effort cost coefficient
beta = 24.5   # inequality aversion coefficient
weight_box = 5  # weight of the box
low_reward = 10
high_reward = 20

# Discretized spaces (for computational tractability)
efforts = np.array([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,
                    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])

# Discretized strength space (replacing continuous uniform(1,10))
# # (1,10) with digits=1
# strength_values = np.linspace(1.1, 9.9, 89)
# # [1,10] with step size of .05
# strength_values = np.linspace(1., 10., 181)

# [1,10] with step size of .03
strength_values = np.linspace(1., 10., 301)

# # TEMP
# # Dummy inputs for debugging
# efforts = np.linspace(0., 1., 5)
# strength_values = np.linspace(1., 10., 10)
```

## Helper Functions (JAX-compatible)

```{python}
@jax.jit
def lift(strength, effort):
    """Single agent lift: can they lift the box?"""
    return (effort * strength >= weight_box).astype(float)

@jax.jit
def lift2(strength1, strength2, effort1, effort2):
    """Two-agent joint lift: can they lift the box together?"""
    return (effort1 * strength1 + effort2 * strength2 >= weight_box).astype(float)

@jax.jit
def gini(effort1, effort2):
    """Gini coefficient for inequality aversion"""
    return np.where(
        np.abs(effort1 - effort2) < 1e-8, # instead of effort1 == effort2
        0.0,
        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)
    )

@jax.jit
def argmax_utility(strength, reward, effort_space=efforts):
    """Find effort that maximizes utility for single agent"""
    utilities = reward * lift(strength, effort_space) - alpha * effort_space
    return effort_space[np.argmax(utilities)]

@jax.jit
def outcome(strength, reward):
    """Predicted outcome (success probability) for single agent at optimal effort"""
    opt_effort = argmax_utility(strength, reward)
    return lift(strength, opt_effort)

# DEBUG: Test helper functions with sample values
print("=== HELPER FUNCTION TESTS ===")
test_strength = 5.0
test_effort = .5
test_lift_result = lift(test_strength, test_effort)
print(f"lift(strength=5.0, effort=.5): {test_lift_result}")

test_optE_low = argmax_utility(test_strength, low_reward)
print(f"optE(strength=5.0, reward=10): {test_optE_low}")

test_optE_high = argmax_utility(test_strength, high_reward)
print(f"optE(strength=5.0, reward=20): {test_optE_high}")

test_outcome_low = outcome(test_strength, low_reward)
print(f"outcome(strength=5.0, reward=10): {test_outcome_low}")

test_outcome_high = outcome(test_strength, high_reward)
print(f"outcome(strength=5.0, reward=20): {test_outcome_high}")
```

## Bayesian Inference Over Agent Strengths

The original WebPPL code infers agent strengths from observed outcomes in rounds 1 and 2.

```{python}
# Helper function for the joint condition (needed because & operator not supported in memo)
@jax.jit
def both_true(cond1, cond2):
    """Returns True if both conditions are True (for use in joint probability queries)"""
    return cond1 & cond2

@memo(save_comic="memo-comic-xiang-strength-inference")
def infer_strengths[query_sa: strength_values, query_sb: strength_values](
    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b
):
    """
    Infer posterior distribution over agent strengths given observed outcomes.

    Observations:
    - Round 1 (low reward = 10): A and B each attempt individual lifts
    - Round 2 (high reward = 20): A and B each attempt individual lifts

    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]
    """
    # Prior: Agent A has some strength uniformly distributed in [1, 10]
    agent_a: given(sa in strength_values, wpp=1)

    # Prior: Agent B has some strength uniformly distributed in [1, 10]
    agent_b: given(sb in strength_values, wpp=1)

    # The participant models that the agents have some strength
    participant: thinks[
        agent_a: given(sa in strength_values, wpp=1),
        agent_b: given(sb in strength_values, wpp=1)
    ]

    # Participant witnesses the outcomes and conditions on them
    # Condition on round 1 outcomes (low reward)
    participant: observes_that [outcome(agent_a.sa, {low_reward}) == r1_outcome_a]
    participant: observes_that [outcome(agent_b.sb, {low_reward}) == r1_outcome_b]

    # Condition on round 2 outcomes (high reward)
    participant: observes_that [outcome(agent_a.sa, {high_reward}) == r2_outcome_a]
    participant: observes_that [outcome(agent_b.sb, {high_reward}) == r2_outcome_b]

    # Push query variables into participant frame 
    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths
    participant: knows(query_sa)
    participant: knows(query_sb)
    return participant[Pr[both_true(agent_a.sa == query_sa, agent_b.sb == query_sb)]]
    # (Using helper function for bitwise-and since & operator not supported in memo DSL)
```

## Game-Theoretic Joint Effort Model

This is the most complex part. The original WebPPL code models:
1. Two agents with uncertain strengths (posterior distributions from rounds 1-2)
2. Agents reason about each other's efforts in round 3 (joint task)
3. Iterative best-response until Nash equilibrium
4. Optimization over initial effort to maximize joint utility

**Key insight**: The game-theoretic reasoning operates on *expected utilities* over the posterior distribution of strengths. This is not purely probabilistic reasoning (like the inference above), but rather decision-theoretic optimization under uncertainty.

**Proposed approach**:
- Keep the Bayesian inference in `@memo` (already done above)
- Implement the game-theoretic equilibrium finding in pure JAX (deterministic optimization)
- Combine them: compute equilibrium for each strength pair, weighted by posterior

```{python}
@jax.jit
def outcome2(strength_a, strength_b, effort_a, effort_b):
    """Predicted outcome (success probability) for joint agents and efforts"""
    if np.ndim(strength_a) > 0 or np.ndim(strength_b) > 0:
        outcomes = jax.vmap(
            lambda sa, sb: lift2(sa, sb, effort_a, effort_b)
        )(strength_a, strength_b)
        return np.mean(outcomes)
    else:
        return lift2(strength_a, strength_b, effort_a, effort_b)

# @jax.jit
# def joint_utility_single_strength(strength_a, strength_b, effort_a, effort_b):
#     """
#     Compute utilities for both agents given fixed strengths and efforts.

#     Returns: (utility_a, utility_b, success_prob)
#     """
#     reward = high_reward  # Round 3 uses high reward
#     success = lift2(strength_a, strength_b, effort_a, effort_b)
#     gini_coef = gini(effort_a, effort_b)

#     utility_a = reward * success - alpha * effort_a - beta * gini_coef
#     utility_b = reward * success - alpha * effort_b - beta * gini_coef

#     return utility_a, utility_b, success

@jax.jit
def joint_utility_multiple_strength(strength_a, strength_b, effort_a, effort_b):
    """
    Compute utilities for both agents given multiple fixed strengths and efforts.

    Returns: (utility_a, utility_b, success_prob)
    """
    reward = high_reward  # Round 3 uses high reward

    # translation of listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2))
    # note that map2 maps over the input arrays *concurrently*
    success = np.mean(
        jax.vmap(
            lambda sa, sb: lift2(sa, sb, effort_a, effort_b)
        )(strength_a, strength_b)
    )
    gini_coef = gini(effort_a, effort_b)

    utility_a = reward * success - alpha * effort_a - beta * gini_coef
    utility_b = reward * success - alpha * effort_b - beta * gini_coef

    return utility_a, utility_b, success

@jax.jit
def best_response_a(strength_a, strength_b, effort_b):
    """
    Find agent A's best response to agent B's effort.

    Agent A optimizes: max_{e_a} E[utility_a(e_a, e_b) | s_a, s_b]
    """
    # if np.ndim(strength_a) > 0 or np.ndim(strength_b) > 0:
    # Multiple strength values
    utilities = jax.vmap(
        lambda ea: joint_utility_multiple_strength(strength_a, strength_b, ea, effort_b)[0]
    )(efforts)
    # else:
    #     # Single strength values
    #     utilities = jax.vmap(
    #         lambda ea: joint_utility_single_strength(strength_a, strength_b, ea, effort_b)[0]
    #     )(efforts)

    return efforts[np.argmax(utilities)]

@jax.jit
def best_response_b(strength_a, strength_b, effort_a):
    """
    Find agent B's best response to agent A's effort.
    """
    # if np.ndim(strength_a) > 0 or np.ndim(strength_b) > 0:
    # Multiple strength values
    utilities = jax.vmap(
        lambda eb: joint_utility_multiple_strength(strength_a, strength_b, effort_a, eb)[1]
    )(efforts)
    # else:
    #     # Single strength values
    #     utilities = jax.vmap(
    #         lambda eb: joint_utility_single_strength(strength_a, strength_b, effort_a, eb)[1]
    #     )(efforts)

    return efforts[np.argmax(utilities)]

# potentially more efficient version
def find_equilibrium_iterative(strength_a, strength_b, init_effort, max_depth=10, verbose=False):
    """
    Find Nash equilibrium through iterated best responses (iterative implementation).

    Starting from init_effort for agent B, iterate:
    - A responds to B
    - B responds to A
    Until convergence (or max_depth reached)

    Note: depth starts at 1 to match WebPPL convention (not 0-indexed)
    """
    effort_b = init_effort

    # Start depth at 1 to match WebPPL convention
    for depth in range(1, max_depth + 1):
        effort_a = best_response_a(strength_a, strength_b, effort_b)
        effort_b_new = best_response_b(strength_a, strength_b, effort_a)

        # Check convergence
        if np.abs(effort_b_new - effort_b) < 0.06:
            if verbose:
                print(f"  Converged at depth {depth}: effort_a={effort_a:.4f}, effort_b={effort_b_new:.4f}")
            return effort_a, effort_b_new, depth

        effort_b = effort_b_new

    print(f"Warning: Equilibrium did not converge in {max_depth} iterations")
    return effort_a, effort_b, max_depth

# matches exactly webppl's version
def find_equilibrium(strength_a, strength_b, init_effort, max_depth=10, verbose=False, trace=False):
    """
    Find Nash equilibrium through mutually recursive best responses.

    This implementation EXACTLY matches the WebPPL version's recursive structure:
    - a(depth, reward) calls b(depth-1, reward)
    - b(depth, reward) calls a(depth, reward) if depth > 0, else uses init_effort
    - Convergence checked using findDepth() at specific depths [1, 2, 5, 10]

    Args:
        strength_a: Agent A's strength (scalar or array)
        strength_b: Agent B's strength (scalar or array)
        init_effort: Initial effort for agent B at depth=0
        max_depth: Maximum recursion depth
        verbose: Print convergence info
        trace: Print detailed trace of recursive calls

    Returns:
        (effort_a, effort_b, converged_depth)
    """
    reward = high_reward  # Round 3 reward (matches WebPPL r3_reward)

    # Cache for memoizing recursive calls (mimics WebPPL's call stack behavior)
    cache_a = {}
    cache_b = {}

    def a(depth):
        """Agent A's best response at given recursion depth."""
        if depth in cache_a:
            return cache_a[depth]

        # Get B's effort from one level down
        effort_b = b(depth - 1)

        if trace:
            print(f"  a(depth={depth}): B's effort from b({depth-1}) = {effort_b:.4f}")

        # A optimizes given B's effort
        effort_a = best_response_a(strength_a, strength_b, effort_b)

        if trace:
            print(f"  a(depth={depth}): A's best response = {effort_a:.4f}")

        cache_a[depth] = effort_a
        return effort_a

    def b(depth):
        """Agent B's best response at given recursion depth."""
        if depth in cache_b:
            return cache_b[depth]

        # Base case: depth 0 uses initial effort
        if depth == 0:
            if trace:
                print(f"  b(depth={depth}): Using init_effort = {init_effort:.4f}")
            cache_b[depth] = init_effort
            return init_effort

        # Get A's effort from same level
        effort_a = a(depth)

        if trace:
            print(f"  b(depth={depth}): A's effort from a({depth}) = {effort_a:.4f}")

        # B optimizes given A's effort
        effort_b = best_response_b(strength_a, strength_b, effort_a)

        if trace:
            print(f"  b(depth={depth}): B's best response = {effort_b:.4f}")

        cache_b[depth] = effort_b
        return effort_b

    def findDepth(x):
        """Check if convergence achieved at depth x (matches WebPPL logic)."""
        b_at_x = b(x)
        b_at_x_plus_1 = b(x + 1)
        converged = np.abs(b_at_x - b_at_x_plus_1) < 0.06

        if trace:
            print(f"findDepth({x}): b({x})={b_at_x:.4f}, b({x+1})={b_at_x_plus_1:.4f}, diff={np.abs(b_at_x - b_at_x_plus_1):.4f}, converged={converged}")

        return x if converged else -1

    # Try depths in order [1, 2, 5, 10] (matches WebPPL ds array)
    candidate_depths = [1, 2, 5, 10]

    if trace:
        print(f"\n=== find_equilibrium(init_effort={init_effort:.4f}) ===")

    for depth_candidate in candidate_depths:
        if depth_candidate > max_depth:
            break
        result = findDepth(depth_candidate)
        if result > 0:
            # Converged at this depth
            # Return efforts from depth+1 for A, depth for B (matches WebPPL lines 209-210)
            effort_a = a(result + 1)
            effort_b = b(result)

            if verbose:
                print(f"  Converged at depth {result}: effort_a={effort_a:.4f}, effort_b={effort_b:.4f}")

            return effort_a, effort_b, result

    # Did not converge
    print(f"Warning: Effort could not converge in {candidate_depths[-1]} iterations")
    effort_a = a(candidate_depths[-1] + 1)
    effort_b = b(candidate_depths[-1])
    return effort_a, effort_b, candidate_depths[-1]

# To find the optimal initial effort, we'd search over init_effort values
def starting_effort(posterior_sa, posterior_sb):
    """
    Find initial effort that maximizes expected joint utility.

    This matches the WebPPL startingEffort() function which optimizes jointU.

    Args:
        posterior_sa - sampled from posterior
        posterior_sb - sampled from posterior

    Returns:
        Optimal initial effort value
    """
    joint_utilities = []
    for init_effort in efforts:
        stats = expected_joint_utility(init_effort, posterior_sa, posterior_sb)
        joint_utilities.append(stats['jointU'])  # Optimize joint utility (matches WebPPL)

    joint_utilities = np.array(joint_utilities)
    return efforts[np.argmax(joint_utilities)]

def expected_joint_utility(init_effort, posterior_sa, posterior_sb):
    """
    Compute expected utilities and efforts over posterior distribution of strengths.

    This matches the WebPPL jointUtility() function output, computing expectations over
    the posterior distribution of agent strengths.

    Args:
        posterior_sa: sampled from posterior
        posterior_sb: sampled from posterior
        init_effort: Initial effort for agent B to start equilibrium search

    Returns:
        dict with keys: joint_utility, agent_a_utility, agent_b_utility,
                       agent_a_effort, agent_b_effort, outcome_prob
    """
    # Compute expectations by averaging over samples
    sa = np.array(posterior_sa)
    sb = np.array(posterior_sb)

    effort_a, effort_b, depth = find_equilibrium_iterative(sa, sb, init_effort)

    # print(f"Strengths (A={sa:.2f}, B={sb:.2f}): Efforts (A={effort_a:.4f}, B={effort_b:.4f})")

    # Compute utilities and outcome at equilibrium
    utility_a, utility_b, success_prob = joint_utility_multiple_strength(sa, sb, effort_a, effort_b)

    # Test equilibrium finding with a sample strength pair
    if (init_effort == 0.0):
        print("\n=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===")
        print(f"Convergence depth: {depth}")
        print(f"Agent A equilibrium effort: {effort_a:.4f}")
        print(f"Agent B equilibrium effort: {effort_b:.4f}")

    return {
        'jointU': utility_a + utility_b,
        'aU': utility_a,
        'bU': utility_b,
        'aE': effort_a,
        'bE': effort_b,
        'outcome': outcome2(sa, sb, effort_a, effort_b)
    }
```

## Running the Model for a Scenario

```{python}
# Get posterior over strengths
print("\nComputing posterior over agent strengths...")

posterior_strengths = infer_strengths(0.0, 0.0, 0.0, 0.0)

# DEBUG: Bayesian inference statistics
print("\n=== BAYESIAN INFERENCE STATISTICS ===")
posterior_2d = posterior_strengths.reshape(len(strength_values), len(strength_values))
print(f"Posterior shape: {posterior_2d.shape}")
print(f"Number of non-negligible posterior samples (compare to WebPPL): {np.sum(posterior_strengths > 1e-10)}")
print(f"Total probability mass: {np.sum(posterior_2d):.6f}")

# Compute marginals
marginal_a = np.sum(posterior_2d, axis=1)
marginal_b = np.sum(posterior_2d, axis=0)

# # Compute statistics
# mean_sa = np.sum(marginal_a * strength_values)
# mean_sb = np.sum(marginal_b * strength_values)
# print(f"Agent A strength - mean: {mean_sa:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}")
# print(f"Agent B strength - mean: {mean_sb:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}")

# # Find high-probability regions
# high_prob_threshold = np.max(posterior_2d) * 0.1
# high_prob_indices = np.where(posterior_2d > high_prob_threshold)
# print(f"High probability region (top 10%): strength_a in [{strength_values[np.min(high_prob_indices[0])]:.2f}, {strength_values[np.max(high_prob_indices[0])]:.2f}]")
# print(f"                                     strength_b in [{strength_values[np.min(high_prob_indices[1])]:.2f}, {strength_values[np.max(high_prob_indices[1])]:.2f}]")

# # For F,F;F,F scenario, agents fail at BOTH low_reward=10 and high_reward=20
# # This means they cannot lift box even with high incentive
# # Let's check what strengths are actually consistent with failing both rounds
# print("\n=== FIXME #1: Investigating strength bounds ===")
# print("For F,F;F,F: agents fail at reward=10 AND reward=20")
# print(f"Box weight: {weight_box}")
# print(f"Alpha (effort cost): {alpha}")

# # Check a few sample strengths to see if they would fail
# test_strengths = [3.0, 4.0, 4.5, 4.9, 5.0, 5.5, 6.0, 7.0]
# for s in test_strengths:
#     outcome_low = outcome(s, low_reward)
#     outcome_high = outcome(s, high_reward)
#     opt_e_low = argmax_utility(s, low_reward)
#     opt_e_high = argmax_utility(s, high_reward)
#     print(f"  strength={s:.1f}: outcome(R=10)={outcome_low:.0f} (effort={opt_e_low:.2f}), outcome(R=20)={outcome_high:.0f} (effort={opt_e_high:.2f})")

# Find the actual support of the posterior (non-zero probability mass)
posterior_support = posterior_2d > 1e-10
support_indices = np.where(posterior_support)
if len(support_indices[0]) > 0:
    print(f"Non-negligible posterior support:")
    print(f"  strength_a in [{strength_values[np.min(support_indices[0])]:.2f}, {strength_values[np.max(support_indices[0])]:.2f}], with mean {np.sum(marginal_a * strength_values):.2f}")
    print(f"  strength_b in [{strength_values[np.min(support_indices[1])]:.2f}, {strength_values[np.max(support_indices[1])]:.2f}], with mean {np.sum(marginal_b * strength_values):.2f}")
else:
    print("\nWARNING: Posterior has no support! Inference may have failed.")

posterior = posterior_strengths.reshape(len(strength_values), len(strength_values))

# Find optimal initial effort (by searching over effort space)
print("\n=== FINDING OPTIMAL INITIAL EFFORT ===")

# convert from point probabilities to samples; there should be sample counts proportional to the posterior probabilities
posterior_sa = []
posterior_sb = []
for i, sa in enumerate(strength_values):
    for j, sb in enumerate(strength_values):
        prob = posterior[i, j]
        if prob > 1e-10:
            # TODO: if probs vary, should use a larger number of samples, e.g.: 
            # num_samples = int(prob * 1000)
            # posterior_sa.extend([sa] * num_samples)
            # posterior_sb.extend([sb] * num_samples)
            posterior_sa.extend([sa] * 1)
            posterior_sb.extend([sb] * 1)
posterior_sa = np.array(posterior_sa)
posterior_sb = np.array(posterior_sb)

print(f"First 5 samples (sa, sb): {np.array(list(zip(list(reversed(posterior_sa))[:5], list(reversed(posterior_sb))[:5])))}")
# ^reverse for the sake of matching the order by which webppl enumerates

optimal_init_effort = starting_effort(posterior_sa, posterior_sb)
print(f"Using initial effort: {optimal_init_effort}")


# Compute expected statistics at equilibrium (utilities, efforts, outcome)
print("\nComputing expected statistics at equilibrium...")
stats = expected_joint_utility(optimal_init_effort, posterior_sa, posterior_sb)

print("\n=== FINAL RESULTS ===")
print(f"Optimal starting effort: {optimal_init_effort}")
print(f"Joint utility at optimum: {stats['jointU']:.6f}")
print(f"Agent A utility: {stats['aU']:.6f}")
print(f"Agent B utility: {stats['bU']:.6f}")
print(f"Agent A equilibrium effort: {stats['aE']:.6f}")
print(f"Agent B equilibrium effort: {stats['bE']:.6f}")
print(f"Expected outcome probability: {stats['outcome']:.6f}")
print(f"\n(Compare to original WebPPL output)")

# Sanity check: joint utility should equal sum of individual utilities
joint_check = stats['aU'] + stats['bU']
if abs(joint_check - stats['jointU']) > 1e-5:
    print(f"WARNING: Joint utility mismatch! {stats['jointU']:.6f} != {joint_check:.6f}")
```

### Visualization

```{python}
#| eval: false
# Visualize posterior over agent strengths
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Posterior heatmap
im = axes[0].imshow(posterior, origin='lower', extent=[1, 10, 1, 10], aspect='auto')
axes[0].set_xlabel('Agent B Strength')
axes[0].set_ylabel('Agent A Strength')
axes[0].set_title('Posterior P(strength_A, strength_B | F,F;F,F)')
plt.colorbar(im, ax=axes[0], label='Probability')

# Marginal distributions
marginal_a = np.sum(posterior, axis=1)
marginal_b = np.sum(posterior, axis=0)

axes[1].plot(strength_values, marginal_a, label='Agent A', linewidth=2)
axes[1].plot(strength_values, marginal_b, label='Agent B', linewidth=2, linestyle='--')
axes[1].set_xlabel('Strength')
axes[1].set_ylabel('Probability')
axes[1].set_title('Marginal Distributions')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
# Visualize outcome variables like did for webppl

# compare_results <- function(...) {
#   # Accepts data frames as inputs
#   results_list <- list(...)
#   if (length(results_list) == 0) {
#     stop("At least one set of results must be provided")
#   }

#   # Build summaries
#   results_combined <- bind_rows(results_list, .id="method")
#   P_results_summary <- results_combined |> select(method, scenario, P)
#   posteriors_summary <- results_combined |> select(method, scenario, aStrength_posterior, bStrength_posterior)

#   message("Now compare visually")

#   show(P_results_summary)

#   # Make posteriors_summary longer for plotting (cols: method, agent, scenario, strength)
#   posteriors_long <- posteriors_summary |>
#     pivot_longer(cols = c(aStrength_posterior, bStrength_posterior),
#                   names_to = "agent",
#                   values_to = "strength") |>
#     mutate(agent = ifelse(agent == "aStrength_posterior", "a", "b")) |>
#     unnest_longer(strength)

#   # Plot density of posteriors for each scenario
#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_wrap(~scenario, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   message("Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors")

#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_grid(method~scenario, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_grid(method~scenario+agent, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   message("Compare all output values across methods")

#   show(results_combined |> select(-aStrength_posterior, -bStrength_posterior, -startingE_table))

#   message("Extract all final_table values into a comprehensive summary")

#   # Plot comparisons for the rest of the variables
#   results_combined_vars_long <- results_combined |>
#     select(-where(is.list)) |>
#     pivot_longer(-c(method, scenario), values_to="value", names_to="variable")
  
#   show(
#     ggplot(results_combined_vars_long, aes(x = scenario, y = value, fill = method)) +
#       geom_col(position = "dodge") +
#       facet_wrap(~variable, scales = "free_y", ncol = 2) +
#       labs(
#         title = "Output Variables Comparison Across Methods and Scenarios",
#         x = "Scenario",
#         y = "Value",
#         fill = "Method"
#       ) +
#       theme_minimal() +
#       theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   )

#   message("Look at startingE details")
#   results_combined_startingE_vars <- results_combined |>
#     select(method, scenario, startingE, startingE_table) |>
#     unnest_wider(startingE_table)
#   show(results_combined_startingE_vars)

#   results_combined_startingE_vars_long <- results_combined |>
#     select(method, scenario, startingE_table) |>
#     unnest_longer(startingE_table, indices_to = "variable", values_to = "value") |>
#     unnest_longer(value)
  
#   show(
#     ggplot(results_combined_startingE_vars_long, aes(x = scenario, y = value, fill = method)) +
#       geom_col(position = "dodge") +
#       facet_wrap(~variable, scales = "free_y", ncol = 2) +
#       labs(
#         title = "startingE_table Variables Comparison Across Methods and Scenarios",
#         x = "Scenario",
#         y = "Value",
#         fill = "Method"
#       ) +
#       theme_minimal() +
#       theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   )
# }

```

## Notes on Conversion Strategy

The conversion from WebPPL to memo revealed important distinctions:

### 1. Probabilistic Inference vs. Decision Theory

The original WebPPL code mixes two types of reasoning:
- **Bayesian inference** (rounds 1-2): Infer agent strengths from observed outcomes
- **Game-theoretic optimization** (round 3): Find Nash equilibrium efforts

In the memo conversion:
- **Bayesian inference**: Use `@memo` with `given()`, `observes_that[]`, and `thinks[]`
- **Game-theoretic optimization**: Use pure JAX functions (no probabilistic reasoning, just optimization)

### 2. Key Conversion Decisions

**Mutable arrays (x2a, x2b)**:
- WebPPL: Pushes strength posterior samples to arrays, uses them later
- memo: Query the posterior distribution directly (more principled)

**Nested agent reasoning**:
- WebPPL: Nested JavaScript functions with recursive best-response
- memo/JAX: Iterative best-response in pure JAX (not probabilistic, so no `thinks[]` needed here)

**Expected utilities**:
- WebPPL: Implicitly handles through sampling or enumeration
- memo/JAX: Explicit loop over posterior, computing equilibrium for each strength pair

### 3. Computational Considerations

The original uses either:
- MCMC with 10,000 samples (slow but handles continuous distributions)
- Enumeration with discrete values (exact but potentially more expensive)

### 4. What Could Be Improved

**Potential optimizations (if using pure game-theoretic optimization)**:
- Use coarser grid for exploration, then refine around high-probability regions
- Vectorize the equilibrium finding across strength pairs (currently sequential)
- Use automatic differentiation to find equilibrium (gradient-based instead of grid search)

**Further extension with memo**:
Currently, the game-theoretic reasoning is outside of `@memo`. One could potentially:
- Model agents' beliefs about each other's strengths using nested `thinks[]`
- Model agents' choices as `chooses(e in efforts, wpp=utility(e))`
- But this might be overkill and harder to reason about
- However, this does incorporate uncertainty about the other agent into the model, which might add richness

## How to Compare Outputs

To verify that the WebPPL and memo implementations produce the same results:

These should match (up to numerical precision and discretization choices).

Compare outcome probability (here, `expected_outcome`), strength posteriors, and joint utility results for r3 outcome and for starting effort.

1. **Expected matches:**
   - Helper functions (lift, optE, outcome) should return identical values for test inputs
   - Bayesian inference statistics should show similar:
     - Posterior means (agent strengths)
     - Non-neglible-probability regions
   - Equilibrium finding should show similar:
     - Convergence depth (may vary slightly due to discretization)
     - Equilibrium efforts
   - Final outcome probability should be close

2. **Known differences:**
   - See above
   - Small numerical differences due to different underlying computation engines
