---
format: html
---

Code adapted from https://github.com/jczimm/competence_effort/blob/main/Code/main_text/exp1_simulation.R

## Setup

```{python}
import jax
import jax.numpy as np
from memo import memo
from matplotlib import pyplot as plt
from functools import partial

# Model parameters (from original WebPPL code)
alpha = 13.5  # effort cost coefficient, from original paper; kept fixed for replication
beta = 24.5   # inequality aversion coefficient, from original paper; kept fixed for replication
gamma = 1. # strength of belief that agent will achieve optimal effort # TEMP
weight_box = 5  # weight of the box
low_reward = 10
high_reward = 20

# Discretized spaces (for computational tractability)
efforts = np.array([0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,
                    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])

# Discretized strength space (replacing continuous uniform(1,10))
# # (1,10) with digits=1
# strength_values = np.linspace(1.1, 9.9, 89)
# # [1,10] with step size of .05
# strength_values = np.linspace(1., 10., 181)

# [1,10] with step size of .9
strength_values = np.linspace(1., 10., 11) #TEMP
# strength_values = np.linspace(1., 10., 301)

DEBUG = True
```

## Helper Functions (JAX-compatible)

```{python}
@jax.jit
def lift(strength, effort):
    """Single agent lift: can they lift the box?"""
    return (effort * strength >= weight_box).astype(float)

@partial(jax.jit, static_argnames=['model_features'])
def lift2(strength1, strength2, effort1, effort2, model_features):
    """Two-agent joint lift: can they lift the box together?"""
    k = 3.5 # from original paper; kept fixed for replication
    return jax.lax.cond(
        ModelFeatures.SAFE in model_features,
        lambda _: (effort1*strength1 + effort2*strength2) >= (weight_box + ((1-effort1)+(1-effort2))*k),
        lambda _: (effort1 * strength1 + effort2 * strength2 >= weight_box),
        operand=None
    ).astype(float)

@jax.jit
def gini(effort1, effort2):
    """Gini coefficient for inequality aversion"""
    return np.where(
        np.abs(effort1 - effort2) < 1e-8, # instead of effort1 == effort2 (to bypass floating point error)
        0.0,
        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)
    )

```

## Bayesian Inference Over Agent Strengths

The original WebPPL code infers agent strengths from observed outcomes in rounds 1 and 2.

```{python}
@jax.jit
def argmax_utility(strength, reward, effort_space=efforts):
    """Find effort that maximizes utility for single agent"""
    utilities = reward * lift(strength, effort_space) - alpha * effort_space
    return effort_space[np.argmax(utilities)]

@jax.jit
def outcome(strength, reward):
    """Predicted outcome (success probability) for single agent at optimal effort"""
    opt_effort = argmax_utility(strength, reward)
    return lift(strength, opt_effort)

# # DEBUG: Test helper functions with sample values
# print("=== HELPER FUNCTION TESTS ===")
# test_strength = 5.0
# test_effort = .5
# test_lift_result = lift(test_strength, test_effort)
# print(f"lift(strength=5.0, effort=.5): {test_lift_result}")

# test_optE_low = argmax_utility(test_strength, low_reward)
# print(f"optE(strength=5.0, reward=10): {test_optE_low}")

# test_optE_high = argmax_utility(test_strength, high_reward)
# print(f"optE(strength=5.0, reward=20): {test_optE_high}")

# test_outcome_low = outcome(test_strength, low_reward)
# print(f"outcome(strength=5.0, reward=10): {test_outcome_low}")

# test_outcome_high = outcome(test_strength, high_reward)
# print(f"outcome(strength=5.0, reward=20): {test_outcome_high}")

@memo(save_comic="memo-comic-xiang-strength-inference", debug_trace=DEBUG)
def infer_strengths[query_sa: strength_values, query_sb: strength_values](
    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b
):
    """
    Infer posterior distribution over agent strengths given observed outcomes.

    Observations:
    - Round 1 (low reward = 10): A and B each attempt individual lifts
    - Round 2 (high reward = 20): A and B each attempt individual lifts

    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]
    """

    # The participant models that the agents have some strength in [1, 10]
    participant: thinks[
        agent_a: given(sa in strength_values, wpp=1),
        agent_b: given(sb in strength_values, wpp=1)
    ]

    # Participant witnesses the outcomes and conditions on them
    # Condition on round 1 outcomes (low reward)
    participant: observes_that [outcome(agent_a.sa, {low_reward}) == r1_outcome_a]
    participant: observes_that [outcome(agent_b.sb, {low_reward}) == r1_outcome_b]

    # Condition on round 2 outcomes (high reward)
    participant: observes_that [outcome(agent_a.sa, {high_reward}) == r2_outcome_a]
    participant: observes_that [outcome(agent_b.sb, {high_reward}) == r2_outcome_b]

    # Push query variables into participant frame 
    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths
    participant: knows(query_sa)
    participant: knows(query_sb)
    return participant[Pr[agent_a.sa == query_sa and agent_b.sb == query_sb]]
```

TEMP
```{python}
@jax.jit
def utility(effort, strength, reward):
    return reward * lift(strength, effort) - alpha * effort

@jax.jit
def normal(mu, sigma, x):
    return jax.scipy.stats.norm.pdf(x, loc=mu, scale=sigma)

@memo(save_comic="memo-comic-xiang-strength-inference_add_perceived_effort", debug_trace=DEBUG)
def infer_strengths_w_perceived_effort[query_sa: strength_values, query_sb: strength_values](
    r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b,
    r1_perceived_effort_a, r1_perceived_effort_b, r2_perceived_effort_a, r2_perceived_effort_b
):
    """
    Same as infer_strengths but tries to pull everything closer together (and maybe avoid model implementation errors) by using memo features for maximization. Also exposes target effort as a parameter that could be separated from actual effort. Downside is that since we are now modeling uncertainty about not only strength but also about effort, the model takes a lot longer to run. Probably necessary for a model with nested uncertainty, but apparently not for the Xiang2023 model.

    Infer posterior distribution over agent strengths given observed outcomes.

    Observations:
    - Round 1 (low reward = 10): A and B each attempt individual lifts
    - Round 2 (high reward = 20): A and B each attempt individual lifts

    Returns: Pr[strength_a == query_sa AND strength_b == query_sb | observations]
    """

    # The participant models that the agents have some strength in [1,10], and that each chooses target effort to optimize utility
    participant: thinks[
        agent_a: given(sa in strength_values, wpp=1),
        agent_b: given(sb in strength_values, wpp=1)
    ]
    
    # Participant witnesses the outcomes and conditions on them

    participant: thinks[
        agent_a: chooses(r1_opt_effort in efforts,
            to_maximize=utility(r1_opt_effort, sa, {low_reward})),
        
        # MVP of way to sample actual effort based on optimal effort, just for agent a for round 1 for this demo. gamma is a free parameter that should be fit to the data (this free parameter is strength of optimal effort as prior, i.e. strength of belief that agent will achieve optimal effort).
        agent_a: given(r1_actual_effort in efforts,
            wpp=
            # = P(r1_actual_effort|observation) = P(observation|r1_actual_effort)P(r1_actual_effort)/P(observation). P(observation) is constant across values of r1_actual_effort so can omit, since using wpp (proportional probability)
            # so:
            # P(r1_actual_effort|observation) is prop to P(r1_actual_effort)P(observation|r1_actual_effort)
            normal(r1_opt_effort, {gamma}, r1_actual_effort) *
            normal(r1_actual_effort, {gamma}, r1_perceived_effort_a)
            
            # MVP ^this conditioning instead would be done using a world model's expected value of effort. here, my goal is to compute the posterior, which is approximately: P(r1_actual_effort | sample(r1_actual_effort) == r1_perceived_effort_a) given the prior P(r1_actual_effort) ~ N(r1_opt_effort, gamma). I want to make sure that I'm conditioning only r1_actual_effort and not the entire trace (which I think observes_event would be doing)
        )
    ]
    participant: observes_that [lift(agent_a.sa, agent_a.r1_actual_effort) == r1_outcome_a]

    participant: thinks[
        agent_b: chooses(r1_opt_effort in efforts,
            to_maximize=utility(r1_opt_effort, sb, {low_reward})),
        agent_b: given(r1_actual_effort in efforts,
            wpp=
                normal(r1_opt_effort, {gamma}, r1_actual_effort) *
                normal(r1_actual_effort, {gamma}, r1_perceived_effort_b)
        )
    ]
    participant: observes_that [lift(agent_b.sb, agent_b.r1_actual_effort) == r1_outcome_b]

    # Round 2

    participant: thinks[
        agent_a: chooses(r2_opt_effort in efforts,
            to_maximize=utility(r2_opt_effort, sa, {high_reward})),
        agent_a: given(r2_actual_effort in efforts,
            wpp=
                normal(r2_opt_effort, {gamma}, r2_actual_effort) *
                normal(r2_actual_effort, {gamma}, r2_perceived_effort_a)
        )
    ]
    participant: observes_that [lift(agent_a.sa, agent_a.r2_actual_effort) == r2_outcome_a]
    
    participant: thinks[
        agent_b: chooses(r2_opt_effort in efforts,
            to_maximize=utility(r2_opt_effort, sb, {high_reward})),
        agent_b: given(r2_actual_effort in efforts,
            wpp=
                normal(r2_opt_effort, {gamma}, r2_actual_effort) *
                normal(r2_actual_effort, {gamma}, r2_perceived_effort_b)
        )
    ]
    participant: observes_that [lift(agent_b.sb, agent_b.r2_actual_effort) == r2_outcome_b]

    # Push query variables into participant frame 
    # and return participant's estimate of the posterior probability for the agents having these given (query) strengths
    participant: knows(query_sa)
    participant: knows(query_sb)
    return participant[Pr[agent_a.sa == query_sa and agent_b.sb == query_sb]]
```

TEMP
```{python}
sample_strengths = infer_strengths_w_perceived_effort(1.0, 0.0, 1.0, 0.0, 7.5, 0., 7.5, 0.)
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Posterior heatmap
im = axes[0].imshow(sample_strengths, origin='lower', extent=[1, 10, 1, 10], aspect='auto')
axes[0].set_xlabel('Agent B Strength')
axes[0].set_ylabel('Agent A Strength')
axes[0].set_title('Posterior P(strength_A, strength_B | outcomes, perceived efforts)')
plt.colorbar(im, ax=axes[0], label='Probability')

plt.tight_layout()
plt.show()
```

## Game-Theoretic Joint Optimization of Effort

This is the most complex part. The original WebPPL code models:
1. Two agents with uncertain strengths (posterior distributions from rounds 1-2)
2. Agents reason about each other's efforts in round 3 (joint task)
3. Iterative best-response until Nash equilibrium
4. Optimization over initial effort to maximize joint utility

**Key insight**: The game-theoretic reasoning operates on *expected utilities* over the posterior distribution of strengths. This is not purely probabilistic reasoning (like the inference above), but rather decision-theoretic optimization under uncertainty.

**Proposed approach**:
- Keep the Bayesian inference in `@memo` (already done above)
- Implement the game-theoretic equilibrium finding in pure JAX (deterministic optimization)
- Combine them: compute equilibrium for each strength pair, weighted by posterior

```{python}
from enum import Flag, auto
class ModelFeatures(Flag):
    NONE = 0
    GINI = auto()
    SAFE = auto()

@partial(jax.jit, static_argnames=['model_features'])
def lift2(strength1, strength2, effort1, effort2, model_features):
    """Two-agent joint lift: can they lift the box together?"""
    k = 3.5 # from original paper; kept fixed for replication
    return jax.lax.cond(
        ModelFeatures.SAFE in model_features,
        lambda _: (effort1*strength1 + effort2*strength2) >= (weight_box + ((1-effort1)+(1-effort2))*k),
        lambda _: (effort1 * strength1 + effort2 * strength2 >= weight_box),
        operand=None
    ).astype(float)

@jax.jit
def gini(effort1, effort2):
    """Gini coefficient for inequality aversion"""
    return np.where(
        np.abs(effort1 - effort2) < 1e-8, # instead of effort1 == effort2 (to bypass floating point error)
        0.0,
        np.abs(effort1 - effort2) / 4 / (effort1 + effort2)
    )

@partial(jax.jit, static_argnames=['model_features'])
def outcome2(strength_a, strength_b, effort_a, effort_b, model_features):
    """Predicted outcome (success probability) for joint action"""
    if np.ndim(strength_a) > 0:
        outcomes = jax.vmap(
            lambda sa, sb: lift2(sa, sb, effort_a, effort_b, model_features)
        )(strength_a, strength_b)
        return np.mean(outcomes)
    else:
        return lift2(strength_a, strength_b, effort_a, effort_b, model_features)

# @jax.jit
# def joint_utility_single_strength(strength_a, strength_b, effort_a, effort_b):
#     """
#     Compute utilities for both agents given fixed strengths and efforts.

#     Returns: (utility_a, utility_b, success_prob)
#     """
#     reward = high_reward  # Round 3 uses high reward
#     success = lift2(strength_a, strength_b, effort_a, effort_b)
#     gini_coef = gini(effort_a, effort_b)

#     utility_a = reward * success - alpha * effort_a - beta * gini_coef
#     utility_b = reward * success - alpha * effort_b - beta * gini_coef

#     return utility_a, utility_b, success

@partial(jax.jit, static_argnames=['model_features'])
def joint_utility_multiple_strength(strength_a, strength_b, effort_a, effort_b, model_features):
    """
    Compute utilities for both agents given multiple fixed strengths and efforts.

    Returns: (utility_a, utility_b, success_prob)
    """
    reward = high_reward  # Round 3 uses high reward

    # translation of listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2))
    # note that map2 maps over the input arrays *concurrently*
    # success = np.mean(
    #     jax.vmap(
    #         lambda sa, sb: lift2(sa, sb, effort_a, effort_b)
    #     )(strength_a, strength_b)
    # )
    
    success = outcome2(strength_a, strength_b, effort_a, effort_b, model_features=model_features)
    gini_coef = jax.lax.cond(
        ModelFeatures.GINI in model_features,
        lambda _: gini(effort_a, effort_b),
        lambda _: np.array(0.0),
        operand=None
    )

    utility_a = reward * success - alpha * effort_a - beta * gini_coef
    utility_b = reward * success - alpha * effort_b - beta * gini_coef

    return utility_a, utility_b, success

@partial(jax.jit, static_argnames=['model_features'])
def best_response_a(strength_a, strength_b, effort_b, model_features):
    """
    Find agent A's best response to agent B's effort.

    Agent A optimizes: max_{e_a} E[utility_a(e_a, e_b) | s_a, s_b]
    """
    # if np.ndim(strength_a) > 0 or np.ndim(strength_b) > 0:
    # Multiple strength values
    utilities = jax.vmap(
        lambda ea: joint_utility_multiple_strength(strength_a, strength_b, ea, effort_b, model_features=model_features)[0]
    )(efforts)
    # else:
    #     # Single strength values
    #     utilities = jax.vmap(
    #         lambda ea: joint_utility_single_strength(strength_a, strength_b, ea, effort_b)[0]
    #     )(efforts)

    return efforts[np.argmax(utilities)]

@partial(jax.jit, static_argnames=['model_features'])
def best_response_b(strength_a, strength_b, effort_a, model_features):
    """
    Find agent B's best response to agent A's effort.
    """
    # if np.ndim(strength_a) > 0 or np.ndim(strength_b) > 0:
    # Multiple strength values
    utilities = jax.vmap(
        lambda eb: joint_utility_multiple_strength(strength_a, strength_b, effort_a, eb, model_features=model_features)[1]
    )(efforts)
    # else:
    #     # Single strength values
    #     utilities = jax.vmap(
    #         lambda eb: joint_utility_single_strength(strength_a, strength_b, effort_a, eb)[1]
    #     )(efforts)

    return efforts[np.argmax(utilities)]

def find_equilibrium(strength_a, strength_b, init_effort, model_features, max_depth=10, verbose=False, trace=False):
    """
    Find Nash equilibrium through mutually recursive best responses.

    This implementation EXACTLY matches the WebPPL version's recursive structure:
    - a(depth, reward) calls b(depth-1, reward)
    - b(depth, reward) calls a(depth, reward) if depth > 0, else uses init_effort
    - Convergence checked using findDepth() at specific depths [1, 2, 5, 10]

    Args:
        strength_a: Agent A's strength (scalar or array)
        strength_b: Agent B's strength (scalar or array)
        init_effort: Initial effort for agent B at depth=0
        max_depth: Maximum recursion depth
        verbose: Print convergence info
        trace: Print detailed trace of recursive calls

    Returns:
        (effort_a, effort_b, converged_depth)
    """
    reward = high_reward  # Round 3 reward (matches WebPPL r3_reward)

    # Cache for memoizing recursive calls (mimics WebPPL's call stack behavior)
    cache_a = {}
    cache_b = {}

    def a(depth):
        """Agent A's best response at given recursion depth."""
        if depth in cache_a:
            return cache_a[depth]

        # Get B's effort from one level down
        effort_b = b(depth - 1)

        if trace:
            print(f"  a(depth={depth}): B's effort from b({depth-1}) = {effort_b:.4f}")

        # A optimizes given B's effort
        effort_a = best_response_a(strength_a, strength_b, effort_b, model_features)

        if trace:
            print(f"  a(depth={depth}): A's best response = {effort_a:.4f}")

        cache_a[depth] = effort_a
        return effort_a

    def b(depth):
        """Agent B's best response at given recursion depth."""
        if depth in cache_b:
            return cache_b[depth]

        # Base case: depth 0 uses initial effort
        if depth == 0:
            if trace:
                print(f"  b(depth={depth}): Using init_effort = {init_effort:.4f}")
            cache_b[depth] = init_effort
            return init_effort

        # Get A's effort from same level
        effort_a = a(depth)

        if trace:
            print(f"  b(depth={depth}): A's effort from a({depth}) = {effort_a:.4f}")

        # B optimizes given A's effort
        effort_b = best_response_b(strength_a, strength_b, effort_a, model_features)

        if trace:
            print(f"  b(depth={depth}): B's best response = {effort_b:.4f}")

        cache_b[depth] = effort_b
        return effort_b

    def findDepth(x):
        """Check if convergence achieved at depth x (matches WebPPL logic)."""
        b_at_x = b(x)
        b_at_x_plus_1 = b(x + 1)
        converged = np.abs(b_at_x - b_at_x_plus_1) < 0.06

        if trace:
            print(f"findDepth({x}): b({x})={b_at_x:.4f}, b({x+1})={b_at_x_plus_1:.4f}, diff={np.abs(b_at_x - b_at_x_plus_1):.4f}, converged={converged}")

        return x if converged else -1

    # Try depths in order [1, 2, 5, 10] (matches WebPPL ds array)
    candidate_depths = [1, 2, 5, 10]

    if trace:
        print(f"\n=== find_equilibrium(init_effort={init_effort:.4f}) ===")

    for depth_candidate in candidate_depths:
        if depth_candidate > max_depth:
            break
        result = findDepth(depth_candidate)
        if result > 0:
            # Converged at this depth
            # Return efforts from depth+1 for A, depth for B (matches WebPPL lines 209-210)
            effort_a = a(result + 1)
            effort_b = b(result)

            if verbose:
                print(f"  Converged at depth {result}: effort_a={effort_a:.4f}, effort_b={effort_b:.4f}")

            return effort_a, effort_b, result

    # Did not converge
    print(f"Warning: Effort could not converge in {candidate_depths[-1]} iterations")
    effort_a = a(candidate_depths[-1] + 1)
    effort_b = b(candidate_depths[-1])
    return effort_a, effort_b, candidate_depths[-1]

# To find the optimal initial effort, we'd search over init_effort values
def starting_effort(posterior_sa, posterior_sb, model_features):
    """
    Find initial effort that maximizes expected joint utility.

    This matches the WebPPL startingEffort() function which optimizes jointU.

    Args:
        posterior_sa - sampled from posterior
        posterior_sb - sampled from posterior

    Returns:
        Optimal initial effort value
    """
    joint_utilities = []
    for init_effort in efforts:
        stats = expected_joint_utility(init_effort, posterior_sa, posterior_sb, model_features=model_features)
        joint_utilities.append(stats['jointU'])  # Optimize joint utility (matches WebPPL)

    joint_utilities = np.array(joint_utilities)
    return efforts[np.argmax(joint_utilities)]

def expected_joint_utility(init_effort, posterior_sa, posterior_sb, model_features):
    """
    Compute expected utilities and efforts over posterior distribution of strengths.

    This matches the WebPPL jointUtility() function output, computing expectations over
    the posterior distribution of agent strengths.

    Args:
        posterior_sa: sampled from posterior
        posterior_sb: sampled from posterior
        init_effort: Initial effort for agent B to start equilibrium search

    Returns:
        dict with keys: joint_utility, agent_a_utility, agent_b_utility,
                       agent_a_effort, agent_b_effort, outcome_prob
    """
    # Compute expectations by averaging over samples
    effort_a, effort_b, depth = find_equilibrium(posterior_sa, posterior_sb, init_effort, model_features)

    # print(f"Strengths (A={posterior_sa:.2f}, B={posterior_sb:.2f}): Efforts (A={effort_a:.4f}, B={effort_b:.4f})")

    # Compute utilities and outcome at equilibrium
    utility_a, utility_b, success_prob = joint_utility_multiple_strength(posterior_sa, posterior_sb, effort_a, effort_b, model_features=model_features)

    # Test equilibrium finding with a sample strength pair
    if (init_effort == 0.0 or not (ModelFeatures.GINI in model_features)) and DEBUG:
        print(f"\n=== EQUILIBRIUM FINDING (init_effort={init_effort}, sample strength pair) ===")
        print(f"Convergence depth: {depth}")
        print(f"Agent A equilibrium effort: {effort_a:.4f}")
        print(f"Agent B equilibrium effort: {effort_b:.4f}")

    return {
        'jointU': utility_a + utility_b,
        'aU': utility_a,
        'bU': utility_b,
        'aE': effort_a,
        'bE': effort_b,
        'P': outcome2(posterior_sa, posterior_sb, effort_a, effort_b, model_features=model_features)
    }


# for solitary/compensatory effort models
def independent_effort_optimization(others_effort, posterior_sa, posterior_sb):
    """
    Compute efforts when each agent independently optimizes assuming 
    partner uses a fixed effort level (solitary=0, compensatory=1).
    
    This is NOT game-theoretic - no mutual best response.
    Each agent simply optimizes: max_{e} E[U(e, others_effort)]
    """
    
    # Agent A optimizes assuming B uses others_effort
    utilities_a = jax.vmap(
        lambda ea: joint_utility_multiple_strength(
            posterior_sa, posterior_sb, ea, others_effort, model_features=ModelFeatures.NONE
        )[0]  # Get utility_a
    )(efforts)
    effort_a = efforts[np.argmax(utilities_a)]
    
    # Agent B optimizes assuming A uses others_effort  
    utilities_b = jax.vmap(
        lambda eb: joint_utility_multiple_strength(
            posterior_sa, posterior_sb, others_effort, eb, model_features=ModelFeatures.NONE
        )[1]  # Get utility_b
    )(efforts)
    effort_b = efforts[np.argmax(utilities_b)]
    
    # Compute final statistics with these independent efforts
    utility_a, utility_b, success_prob = joint_utility_multiple_strength(
        posterior_sa, posterior_sb, effort_a, effort_b, model_features=ModelFeatures.NONE
    )
    
    return {
        'jointU': utility_a + utility_b,
        'aU': utility_a,
        'bU': utility_b,
        'aE': effort_a,
        'bE': effort_b,
        'P': success_prob
    }
```

## Running the Model for a Scenario

```{python}
class Model():
    def __init__(self, r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b):
        # Get posterior over strengths
        if DEBUG:
            print("\nComputing posterior over agent strengths...")

        posterior_strengths = infer_strengths(r1_outcome_a, r1_outcome_b, r2_outcome_a, r2_outcome_b)

        if DEBUG:
            # DEBUG: Bayesian inference statistics
            print("\n=== BAYESIAN INFERENCE STATISTICS ===")
            posterior_2d = posterior_strengths.reshape(len(strength_values), len(strength_values))
            print(f"Posterior shape: {posterior_2d.shape}")
            print(f"Number of non-negligible posterior samples (compare to WebPPL): {np.sum(posterior_strengths > 1e-10)}")
            print(f"Total probability mass: {np.sum(posterior_2d):.6f}")

            # Compute marginals
            marginal_a = np.sum(posterior_2d, axis=1)
            marginal_b = np.sum(posterior_2d, axis=0)

            # # Compute statistics
            # mean_sa = np.sum(marginal_a * strength_values)
            # mean_sb = np.sum(marginal_b * strength_values)
            # print(f"Agent A strength - mean: {mean_sa:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}")
            # print(f"Agent B strength - mean: {mean_sb:.4f}, min: {strength_values[0]:.4f}, max: {strength_values[-1]:.4f}")

            # # Find high-probability regions
            # high_prob_threshold = np.max(posterior_2d) * 0.1
            # high_prob_indices = np.where(posterior_2d > high_prob_threshold)
            # print(f"High probability region (top 10%): strength_a in [{strength_values[np.min(high_prob_indices[0])]:.2f}, {strength_values[np.max(high_prob_indices[0])]:.2f}]")
            # print(f"                                     strength_b in [{strength_values[np.min(high_prob_indices[1])]:.2f}, {strength_values[np.max(high_prob_indices[1])]:.2f}]")

            # # For F,F;F,F scenario, agents fail at BOTH low_reward=10 and high_reward=20
            # # This means they cannot lift box even with high incentive
            # # Let's check what strengths are actually consistent with failing both rounds
            # print("\n=== FIXME #1: Investigating strength bounds ===")
            # print("For F,F;F,F: agents fail at reward=10 AND reward=20")
            # print(f"Box weight: {weight_box}")
            # print(f"Alpha (effort cost): {alpha}")

            # # Check a few sample strengths to see if they would fail
            # test_strengths = [3.0, 4.0, 4.5, 4.9, 5.0, 5.5, 6.0, 7.0]
            # for s in test_strengths:
            #     outcome_low = outcome(s, low_reward)
            #     outcome_high = outcome(s, high_reward)
            #     opt_e_low = argmax_utility(s, low_reward)
            #     opt_e_high = argmax_utility(s, high_reward)
            #     print(f"  strength={s:.1f}: outcome(R=10)={outcome_low:.0f} (effort={opt_e_low:.2f}), outcome(R=20)={outcome_high:.0f} (effort={opt_e_high:.2f})")

            # Find the actual support of the posterior (non-zero probability mass)
            posterior_support = posterior_2d > 1e-10
            support_indices = np.where(posterior_support)
            if len(support_indices[0]) > 0:
                print(f"Non-negligible posterior support:")
                print(f"  strength_a in [{strength_values[np.min(support_indices[0])]:.2f}, {strength_values[np.max(support_indices[0])]:.2f}], with mean {np.sum(marginal_a * strength_values):.2f}")
                print(f"  strength_b in [{strength_values[np.min(support_indices[1])]:.2f}, {strength_values[np.max(support_indices[1])]:.2f}], with mean {np.sum(marginal_b * strength_values):.2f}")
            else:
                print("\nWARNING: Posterior has no support! Inference may have failed.")

        self.posterior = posterior_strengths.reshape(len(strength_values), len(strength_values))

        # assert that probabilities above 1e-10 all are the same
        unique_probs = np.unique(self.posterior[self.posterior > 1e-10])
        if len(unique_probs) > 1:
            print("\nERROR: Posterior probabilities vary! This WILL affect sampling accuracy.")
            print(f"Unique non-negligible posterior probabilities: {unique_probs}")
        assert len(unique_probs) == 1, "Posterior probabilities vary above threshold!"
        # if probs vary, would need to use a larger number of samples, to approximate the samples correctly, e.g.: 
        # num_samples = int(prob * 1000)
        # posterior_sb.extend([sb] * num_samples)
        # posterior_sa.extend([sa] * num_samples)
        
        # convert from point probabilities to samples; there should be sample counts proportional to the posterior probabilities
        posterior_sa = []
        posterior_sb = []
        for i, sa in enumerate(strength_values):
            for j, sb in enumerate(strength_values):
                prob = self.posterior[i, j]
                if prob > 1e-10:
                    posterior_sa.extend([sa] * 1)
                    posterior_sb.extend([sb] * 1)
        self.posterior_sa = np.array(posterior_sa)
        self.posterior_sb = np.array(posterior_sb)

        if DEBUG:
            print(f"First 5 samples (sa, sb): {np.array(list(zip(list(reversed(self.posterior_sa))[:5], list(reversed(self.posterior_sb))[:5])))}")
            # ^reverse for the sake of matching the order by which webppl enumerates

    def _check_stats(self, stats):
        if DEBUG:
            print("\n=== FINAL RESULTS ===")
            if hasattr(self, 'optimal_init_effort'):
                print(f"Optimal starting effort: {self.optimal_init_effort}")
            print(f"Joint utility at optimum: {stats['jointU']:.6f}")
            print(f"Agent A utility: {stats['aU']:.6f}")
            print(f"Agent B utility: {stats['bU']:.6f}")
            print(f"Agent A equilibrium effort: {stats['aE']:.6f}")
            print(f"Agent B equilibrium effort: {stats['bE']:.6f}")
            print(f"Expected outcome probability: {stats['P']:.6f}")
            print(f"\n(Compare to original WebPPL output)")

        # Sanity check: joint utility should equal sum of individual utilities
        joint_check = stats['aU'] + stats['bU']
        if abs(joint_check - stats['jointU']) > 1e-5:
            print(f"WARNING: Joint utility mismatch! {stats['jointU']:.6f} != {joint_check:.6f}")

    def joint_effort(self, model_features=ModelFeatures.GINI):
        # Find optimal initial effort (by searching over effort space)
        if DEBUG:
            print("\n=== FINDING OPTIMAL INITIAL EFFORT ===")

        self.optimal_init_effort = starting_effort(self.posterior_sa, self.posterior_sb, model_features=model_features)
        if DEBUG:
            print(f"Using initial effort: {self.optimal_init_effort}")

        # Compute expected statistics at equilibrium (utilities, efforts, outcome)
        if DEBUG:
            print("\nComputing expected statistics at equilibrium...")
        stats = expected_joint_utility(self.optimal_init_effort, self.posterior_sa, self.posterior_sb, model_features=model_features)

        self._check_stats(stats)
        return stats

    def compensatory_effort(self):
        stats = independent_effort_optimization(
            others_effort=1.0,  # Assume partner uses maximum effort
            posterior_sa=self.posterior_sa,
            posterior_sb=self.posterior_sb
        )
        self._check_stats(stats)
        return stats
    
    def solitary_effort(self):
        stats = independent_effort_optimization(
            others_effort=0.0,  # Assume partner contributes nothing
            posterior_sa=self.posterior_sa,
            posterior_sb=self.posterior_sb
        )
        self._check_stats(stats)
        return stats
```

```{python}
#| eval: false
Model(0.0, 0.0, 0.0, 0.0).joint_effort()
Model(0.0, 0.0, 0.0, 1.0).solitary_effort()
Model(0.0, 0.0, 0.0, 0.0).compensatory_effort()
```

### Visualization

```{python}
#| eval: false
# Visualize posterior over agent strengths
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Posterior heatmap
im = axes[0].imshow(sample_model.posterior, origin='lower', extent=[1, 10, 1, 10], aspect='auto')
axes[0].set_xlabel('Agent B Strength')
axes[0].set_ylabel('Agent A Strength')
axes[0].set_title('Posterior P(strength_A, strength_B | F,F;F,F)')
plt.colorbar(im, ax=axes[0], label='Probability')

# Marginal distributions
marginal_a = np.sum(sample_model.posterior, axis=1)
marginal_b = np.sum(sample_model.posterior, axis=0)

axes[1].plot(strength_values, marginal_a, label='Agent A', linewidth=2)
axes[1].plot(strength_values, marginal_b, label='Agent B', linewidth=2, linestyle='--')
axes[1].set_xlabel('Strength')
axes[1].set_ylabel('Probability')
axes[1].set_title('Marginal Distributions')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
#| include: false
# Visualize outcome variables like did for webppl

# compare_results <- function(...) {
#   # Accepts data frames as inputs
#   results_list <- list(...)
#   if (length(results_list) == 0) {
#     stop("At least one set of results must be provided")
#   }

#   # Build summaries
#   results_combined <- bind_rows(results_list, .id="method")
#   P_results_summary <- results_combined |> select(method, scenario, P)
#   posteriors_summary <- results_combined |> select(method, scenario, aStrength_posterior, bStrength_posterior)

#   message("Now compare visually")

#   show(P_results_summary)

#   # Make posteriors_summary longer for plotting (cols: method, agent, scenario, strength)
#   posteriors_long <- posteriors_summary |>
#     pivot_longer(cols = c(aStrength_posterior, bStrength_posterior),
#                   names_to = "agent",
#                   values_to = "strength") |>
#     mutate(agent = ifelse(agent == "aStrength_posterior", "a", "b")) |>
#     unnest_longer(strength)

#   # Plot density of posteriors for each scenario
#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_wrap(~scenario, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   message("Now separately for each method (or method and agent) and so that we can ensure that there are no missing posteriors")

#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_grid(method~scenario, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   show(
#     ggplot(posteriors_long, aes(x = strength, color = method, linetype = agent)) +
#       geom_density() +
#       facet_grid(method~scenario+agent, scales = "free_y") +
#       labs(
#         title = "Strength Posteriors Across Scenarios",
#         x = "Strength",
#         y = "Density",
#         color = "Method",
#         linetype = "Agent"
#       ) +
#       theme_minimal() +
#       theme(legend.position = "bottom")
#   )

#   message("Compare all output values across methods")

#   show(results_combined |> select(-aStrength_posterior, -bStrength_posterior, -startingE_table))

#   message("Extract all final_table values into a comprehensive summary")

#   # Plot comparisons for the rest of the variables
#   results_combined_vars_long <- results_combined |>
#     select(-where(is.list)) |>
#     pivot_longer(-c(method, scenario), values_to="value", names_to="variable")
  
#   show(
#     ggplot(results_combined_vars_long, aes(x = scenario, y = value, fill = method)) +
#       geom_col(position = "dodge") +
#       facet_wrap(~variable, scales = "free_y", ncol = 2) +
#       labs(
#         title = "Output Variables Comparison Across Methods and Scenarios",
#         x = "Scenario",
#         y = "Value",
#         fill = "Method"
#       ) +
#       theme_minimal() +
#       theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   )

#   message("Look at startingE details")
#   results_combined_startingE_vars <- results_combined |>
#     select(method, scenario, startingE, startingE_table) |>
#     unnest_wider(startingE_table)
#   show(results_combined_startingE_vars)

#   results_combined_startingE_vars_long <- results_combined |>
#     select(method, scenario, startingE_table) |>
#     unnest_longer(startingE_table, indices_to = "variable", values_to = "value") |>
#     unnest_longer(value)
  
#   show(
#     ggplot(results_combined_startingE_vars_long, aes(x = scenario, y = value, fill = method)) +
#       geom_col(position = "dodge") +
#       facet_wrap(~variable, scales = "free_y", ncol = 2) +
#       labs(
#         title = "startingE_table Variables Comparison Across Methods and Scenarios",
#         x = "Scenario",
#         y = "Value",
#         fill = "Method"
#       ) +
#       theme_minimal() +
#       theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   )
# }

```

## Now get all predictions

```{python}
DEBUG = False

models = {
    "F,F;F,F": Model(0.0, 0.0, 0.0, 0.0),
    "F,F;F,L": Model(0.0, 0.0, 0.0, 1.0),
    "F,F;L,L": Model(0.0, 0.0, 1.0, 1.0),
    "F,L;F,L": Model(0.0, 1.0, 0.0, 1.0),
    "F,L;L,L": Model(0.0, 1.0, 1.0, 1.0),
    "L,L;L,L": Model(1.0, 1.0, 1.0, 1.0)
}

model_fits = dict()
model_fits['joint'] = {
    scenario: model.joint_effort()
    for scenario, model in models.items()
}
model_fits['compensatory'] = {
    scenario: model.compensatory_effort()
    for scenario, model in models.items()
}
model_fits['solitary'] = {
    scenario: model.solitary_effort()
    for scenario, model in models.items()
}

# to replicate competence_effort/Code/supplement/variations_of_joint_model/exp1_simulation.R
model_fits['joint_wo_gini'] = {
    scenario: model.joint_effort(ModelFeatures.NONE)
    for scenario, model in models.items()
}
model_fits['safe_joint_w_gini'] = {
    scenario: model.joint_effort(ModelFeatures.GINI | ModelFeatures.SAFE)
    for scenario, model in models.items()
}
```

```{python}
#| eval: false
import pandas as pd
# Convert model fits to DataFrame for easier manipulation, converting key (e.g. "joint") to a column called "model"
model_fits_df = pd.DataFrame.from_dict(
    {(model, scenario): data 
     for model, scenarios in model_fits.items() 
     for scenario, data in scenarios.items()},
    orient='index'
).reset_index()
model_fits_df.rename(columns={'level_0': 'model', 'level_1': 'scenario'}, inplace=True)

# save to csv
model_fits_df.to_csv('xiang2023-exp1-round3-model_2fits_results.csv', index=False)
```

## Notes on Conversion Strategy

The conversion from WebPPL to memo revealed important distinctions:

### 1. Probabilistic Inference vs. Decision Theory

The original WebPPL code mixes two types of reasoning:
- **Bayesian inference** (rounds 1-2): Infer agent strengths from observed outcomes
- **Game-theoretic optimization** (round 3): Find Nash equilibrium efforts

In the memo conversion:
- **Bayesian inference**: Use `@memo` with `given()`, `observes_that[]`, and `thinks[]`
- **Game-theoretic optimization**: Use pure JAX functions (no probabilistic reasoning, just optimization)

### 2. Key Conversion Decisions

**Mutable arrays (x2a, x2b)**:
- WebPPL: Pushes strength posterior samples to arrays, uses them later
- memo: Query the posterior distribution directly (more principled)

**Nested agent reasoning**:
- WebPPL: Nested JavaScript functions with recursive best-response
- memo/JAX: Iterative best-response in pure JAX (not probabilistic, so no `thinks[]` needed here)

**Expected utilities**:
- WebPPL: Implicitly handles through sampling or enumeration
- memo/JAX: Explicit loop over posterior, computing equilibrium for each strength pair

### 3. Computational Considerations

The original uses either:
- MCMC with 10,000 samples (slow but handles continuous distributions)
- Enumeration with discrete values (exact but potentially more expensive)

### 4. What Could Be Improved

**Potential optimizations (if using pure game-theoretic optimization)**:
- Use coarser grid for exploration, then refine around high-probability regions
- Vectorize the equilibrium finding across strength pairs (currently sequential)
- Use automatic differentiation to find equilibrium (gradient-based instead of grid search)

**Further extension with memo**:
Currently, the game-theoretic reasoning is outside of `@memo`. One could potentially:
- Model agents' beliefs about each other's strengths using nested `thinks[]`
- Model agents' choices as `chooses(e in efforts, wpp=utility(e))`
- But this might be overkill and harder to reason about
- However, this does incorporate uncertainty about the other agent into the model, which might add richness

## How to Compare Outputs

To verify that the WebPPL and memo implementations produce the same results:

These should match (up to numerical precision and discretization choices).

Compare outcome probability (here, `expected_outcome`), strength posteriors, and joint utility results for r3 outcome and for starting effort.

1. **Expected matches:**
   - Helper functions (lift, optE, outcome) should return identical values for test inputs
   - Bayesian inference statistics should show similar:
     - Posterior means (agent strengths)
     - Non-neglible-probability regions
   - Equilibrium finding should show similar:
     - Convergence depth (may vary slightly due to discretization)
     - Equilibrium efforts
   - Final outcome probability should be close

2. **Known differences:**
   - See above
   - Small numerical differences due to different underlying computation engines


## Notes on the alternative models

The joint effort model and the solitary/compensatory models are fundamentally different in their computational structure:

**Joint Effort Model:**

- Uses game-theoretic Nash equilibrium finding
- Agents engage in mutually recursive best-response reasoning
- Includes inequality aversion (Gini coefficient with Î²=24.5)
- Optimizes `init_effort` to maximize joint utility

**Solitary/Compensatory Models:**

- Use independent optimization with **fixed assumption** of partner's effort (0 or 1, resp.)
- Each agent independently maximizes their utility assuming partner uses fixed effort
- **No mutual recursion** - direct calculation
- **No Gini coefficient** - agents don't care about effort matching