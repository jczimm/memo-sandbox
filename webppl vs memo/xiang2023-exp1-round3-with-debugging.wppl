var efforts = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]

// convert to a @jax.jit?
var argMax = function(f, ar){
  return maxWith(f, ar)[0]
};

var alpha = 13.5, beta = 24.5
var weight = mem(function (box) {return 5})
var lowR = 10
var highR = 20

// convert to a @jax.jit
var lift = function(strength,box,effort){
  return (effort*strength >= weight(box))
}

// convert to a @jax.jit
var optE = function(strength,box,reward) {
  return argMax(
    function(effort) {
      if (strength.length > 1)
        {return reward*listMean(map(function(i){return lift(i,box,effort)}, strength)) - alpha*effort}
        else
          {return reward*lift(strength,box,effort) - alpha*effort}
    },
    efforts);
};

// convert to a @jax.jit
var outcome = function(strength,box,reward) {
  if (strength.length > 1)
    { var opt_effort = optE(strength,box,reward)
      return listMean(map(function(i){return lift(i,box,opt_effort)}, strength))}
  else
  {return lift(strength,box,optE(strength,box,reward))}
}

// DEBUG: Test helper functions with sample values
display("=== HELPER FUNCTION TESTS ===")
var test_strength = 5.0
var test_effort = 0.5
var test_lift_result = lift(test_strength, 'box', test_effort)
display("lift(strength=5.0, effort=0.5): " + test_lift_result)

var test_optE_low = optE(test_strength, 'box', lowR)
display("optE(strength=5.0, reward=10): " + test_optE_low)

var test_optE_high = optE(test_strength, 'box', highR)
display("optE(strength=5.0, reward=20): " + test_optE_high)

var test_outcome_low = outcome(test_strength, 'box', lowR)
display("outcome(strength=5.0, reward=10): " + test_outcome_low)

var test_outcome_high = outcome(test_strength, 'box', highR)
display("outcome(strength=5.0, reward=20): " + test_outcome_high)

// (a, b)
var randomBetweenExclExcl = function(a, b, digits) {
  // https://www.desmos.com/calculator/yjczmoif70
  var maxInteger = Math.pow(10, digits+1) - Math.pow(10, digits) - 1
  return a + (b-a) * (randomInteger(maxInteger)+1)/(maxInteger+1)
}

// (a,b]
var randomBetweenExclIncl = function(a, b, digits) {
  // https://www.desmos.com/calculator/yjczmoif70
  var maxInteger = Math.pow(10, digits+1) - Math.pow(10, digits) - 1
  return a + (b-a) * (randomInteger(maxInteger+1)+1)/(maxInteger+1+1)
}

// [a,b)
var randomBetweenInclExcl = function(a, b, digits) {
  // https://www.desmos.com/calculator/yjczmoif70
  var maxInteger = Math.pow(10, digits+1) - Math.pow(10, digits) - 1
  return a + (b-a) * (randomInteger(maxInteger+1))/(maxInteger+1)
}

// [a,b]
var randomBetweenInclIncl = function(a, b, digits) {
  // https://www.desmos.com/calculator/yjczmoif70
  var maxInteger = Math.pow(10, digits+1) - Math.pow(10, digits) - 1
  return a + (b-a) * (randomInteger(maxInteger+1+1))/(maxInteger+1)
}

var params = params_df[0]

// convert to a @memo, and instead of pushing x2a and x2b to a global variable, bring in the computations which use them into the @memo ?
var x2a = [], x2b = [] // the strength posteriors
var make_model = function(discrete) {
  var sampleStrengthPrior = (function(discrete) {
    if (discrete) {
      var randomBetween = ({
        "(1,10)": randomBetweenExclExcl,
        "(1,10]": randomBetweenExclIncl,
        "[1,10)": randomBetweenInclExcl,
        "[1,10]": randomBetweenInclIncl
      })[params.strengthPriorVersion];
      return function() {
        return randomBetween(1, 10, params.strengthPriorPrecision)
      }
    } else {
      return function() {
        return uniform(1, 10)
      }
    }
  })(discrete)
  return function() {
    var sa = sampleStrengthPrior()
    var sb = sampleStrengthPrior()
    condition(outcome(sa,'box',lowR) == params.a_r1result)
    condition(outcome(sb,'box',lowR) == params.b_r1result)
    condition(outcome(sa,'box',highR) == params.a_r2result)
    condition(outcome(sb,'box',highR) == params.b_r2result)
    x2a.push(sa)
    x2b.push(sb)
    return 0
  }
}

if (params.method == "enumerate") {
  var samples2 = Infer({
    method: 'enumerate',
    model: make_model(params.discrete)
  })
} else if (params.method == "MCMC") {
  var samples2 = Infer({
    method: 'MCMC', kernel: 'MH', samples: 10000, burn: 1000,
    model: make_model(params.discrete)
  })
} else {
  display("must specify either enumerate or MCMC as method")
}

// DEBUG: Bayesian inference statistics
display("=== BAYESIAN INFERENCE STATISTICS ===")
display("Number of posterior samples: " + x2a.length)
display("Agent A strength - mean: " + listMean(x2a) + ", min: " + _.min(x2a) + ", max: " + _.max(x2a))
display("Agent B strength - mean: " + listMean(x2b) + ", min: " + _.min(x2b) + ", max: " + _.max(x2b))
display("First 5 samples (sa, sb): " + JSON.stringify(_.zip(_.take(x2a, 5), _.take(x2b, 5))))

// convert to a combination of @jax.jit and @memo ? so I'm not doing two-step nested optimization (like the difference between RSA in webppl and in memo)
var jointUtility = function(init_effort,a_strength,b_strength){
  var r3_reward = highR // round 3 reward
  
  var lift2 = function(strength,strength2,box,effort,effort2){
    return (effort*strength + effort2*strength2) >= weight(box)
  }

  var gini = function(effort, effort2) {return (effort == effort2 ? 0 : Math.abs(effort-effort2)/4/(effort+effort2))}
  // For the Maximum effort model, the Gini coefficient is always 0, so it is fine to keep this term in here for the maximum effort model.
  
  var a = function(depth,reward) {
      var effort2 = b(depth - 1,reward)
      var optEffort = function(strength,strength2,box,reward) {
        return argMax(
          function(effort) {
            if (a_strength.length > 1) {
              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)
            } else {
              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)
            }
          },
          efforts);
      };
    return optEffort(a_strength,b_strength,'box',reward)
  }
  
  var b = function(depth,reward) {
      var effort2 = depth===0 ? init_effort : a(depth,reward)
      var optEffort = function(strength,strength2,box,reward) {
        return argMax(
          function(effort) {
            if (a_strength.length > 1) {
              return reward*listMean(map2(function(i,j){return lift2(i,j,box,effort,effort2)}, strength,strength2)) - alpha*effort - beta*gini(effort,effort2)
            } else {
              return reward*lift2(strength,strength2,box,effort,effort2) - alpha*effort - beta*gini(effort,effort2)
            }
          },
          efforts);
      };
    return optEffort(b_strength,a_strength,'box',reward)
  }
  
  var findDepth = function(x) { // find the depth that is needed to converge
     if (Math.abs(b(x,r3_reward) - b(x+1,r3_reward)) < 0.06) {
       return x;
     } else {
       return -1;
     }
   };

  var ds = [1,2,5,10]; // if converges in 1 round, then depth = 1; if not, then try 2, 5, 10.
  var d = function() {
     if (findDepth(ds[0]) > 0) {
       return ds[0]
     } else if (findDepth(ds[1]) > 0) {
       return ds[1]
     } else if (findDepth(ds[2]) > 0) {
       return ds[2]
     } else if (findDepth(ds[3]) > 0) {
       return ds[3]
     } else {
       display('Effort could not converge in ' + ds[3] + ' iterations. Increase the number of iterations and try again.')
     }
   };

  var depth = d()
  var aE = a(depth+1,r3_reward)
  var bE = b(depth,r3_reward)

  // DEBUG: Equilibrium convergence info (only display for first call)
  if (init_effort == 0.0) {
    display("=== EQUILIBRIUM FINDING (init_effort=0.0, sample strength pair) ===")
    display("Convergence depth: " + depth)
    display("Agent A equilibrium effort: " + aE)
    display("Agent B equilibrium effort: " + bE)
  }
  
  var outcome2 = function(a_strength,b_strength,box) {
    if (a_strength.length > 1) { 
      return listMean(map2(function(i,j){return lift2(i,j,box,aE,bE)}, a_strength,b_strength))
    } else {
      return lift2(a_strength,b_strength,box,aE,bE)
    }
  }
    
  // calculate agents' utility
  if (a_strength.length > 1) {
    var aU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',aE,bE)}, a_strength,b_strength)) - alpha*aE - beta*gini(aE,bE)
    var bU = r3_reward*listMean(map2(function(i,j){return lift2(i,j,'box',bE,aE)}, b_strength,a_strength)) - alpha*bE - beta*gini(bE,aE)
    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};
    return table
  } else {
    var aU = r3_reward*lift2(a_strength,b_strength,'box',aE,bE) - alpha*aE - beta*gini(aE,bE)
    var bU = r3_reward*lift2(b_strength,a_strength,'box',bE,aE) - alpha*bE - beta*gini(bE,aE)
    var table = { aU: aU, bU: bU, aE: aE, bE: bE, jointU: aU+bU, outcome: outcome2(a_strength,b_strength,'box'), a_strength: a_strength, b_strength: b_strength};
    return table
  }
}

// convert to a @jax.jit
// find the intial effort that maximizes the joint utility
var startingEffort = function(a_strength,b_strength) {
  var result_starting_effort = argMax(
    function(init_effort) {
      var tbl = jointUtility(init_effort,a_strength,b_strength)
      return tbl.jointU
    },
    efforts);

  return jointUtility(result_starting_effort,a_strength,b_strength);
};

var startingE_table = startingEffort(x2a,x2b)
var startingE = startingE_table.jointU

// DEBUG: Final results
display("=== FINAL RESULTS ===")
display("Optimal starting effort: " + startingE)
var final_table = jointUtility(startingE,x2a,x2b)
display("Joint utility at optimum: " + final_table.jointU)
display("Agent A utility: " + final_table.aU)
display("Agent B utility: " + final_table.bU)
display("Agent A equilibrium effort: " + final_table.aE)
display("Agent B equilibrium effort: " + final_table.bE)
display("Expected outcome probability: " + final_table.outcome)

var output = {
  P: final_table.outcome,
  aStrength_posterior: x2a,
  bStrength_posterior: x2b,
  final_table: final_table,
  
  startingE: startingE,
  startingE_table: startingE_table
}
output